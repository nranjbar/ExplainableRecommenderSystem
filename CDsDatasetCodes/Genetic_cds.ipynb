{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Link to Drive for CDs dataset: https://drive.google.com/drive/folders/1t2Y24NSpGlT2M29zJUDvvEg2KfP0JqkN?usp=share_link"
      ],
      "metadata": {
        "id": "x5nC9DPnUbdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing..."
      ],
      "metadata": {
        "id": "JST9Mo3-UeAq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnZRW-GvrGOL",
        "outputId": "55e13f33-eca5-4d89-baf5-7882cc2626a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqmspiJd2PHt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tqdm\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import ndcg_score\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DiUsXtF2SIS",
        "outputId": "0bcefebc-27fb-43cb-bfa5-8ec0fbdfe8db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-15 04:26:28--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/CDs_and_Vinyl.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1172666826 (1.1G) [application/octet-stream]\n",
            "Saving to: ‘CDs_and_Vinyl.json.gz’\n",
            "\n",
            "CDs_and_Vinyl.json. 100%[===================>]   1.09G  87.5MB/s    in 13s     \n",
            "\n",
            "2022-06-15 04:26:41 (85.5 MB/s) - ‘CDs_and_Vinyl.json.gz’ saved [1172666826/1172666826]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/CDs_and_Vinyl.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Htc_yec2VLB"
      },
      "outputs": [],
      "source": [
        "!gunzip CDs_and_Vinyl.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn5qp_282dmg",
        "outputId": "4efd1d7a-a60f-4194-f95b-a8c0d211450a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-15 04:26:41--  http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_CDs_and_Vinyl.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 161716387 (154M) [application/octet-stream]\n",
            "Saving to: ‘meta_CDs_and_Vinyl.json.gz’\n",
            "\n",
            "meta_CDs_and_Vinyl. 100%[===================>] 154.22M  79.5MB/s    in 1.9s    \n",
            "\n",
            "2022-06-15 04:26:43 (79.5 MB/s) - ‘meta_CDs_and_Vinyl.json.gz’ saved [161716387/161716387]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/metaFiles2/meta_CDs_and_Vinyl.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uKVAbhA2jQh"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk import FreqDist\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize,pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9GEI0QDVX5a"
      },
      "outputs": [],
      "source": [
        "save_path='/content/drive/Shareddrives/Unlimited Drive | @LicenseMarket/Recommender/cds/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjnAFk-vpVWz"
      },
      "outputs": [],
      "source": [
        "df_review=pd.read_csv(save_path+'df_reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V09pueGpc4b"
      },
      "outputs": [],
      "source": [
        "df_meta=pd.read_csv(save_path+'df_meta.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc-C0agaIArH"
      },
      "outputs": [],
      "source": [
        "df_review.drop(df_review.index[~df_review['asin'].isin(df_meta['asin'])], inplace=True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHbxlwCgObt-"
      },
      "outputs": [],
      "source": [
        "df_bert_sentiment=pd.read_csv(save_path+'dframe_bert_sentiments.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIAtlgZ1ZDg9"
      },
      "outputs": [],
      "source": [
        "df_bert_sentiment.drop(df_bert_sentiment.index[~df_bert_sentiment['item_id'].isin(df_meta['asin'])], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "2mKXo2QmQFjP",
        "outputId": "7adc8e9e-fcb4-4b79-f2f1-4f1220e34103"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cb58ba32-8c23-439e-92cf-1ded20e57426\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>user_id</th>\n",
              "      <th>item_id</th>\n",
              "      <th>feature</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>vect</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>AHO9V8YUQ2G3Q</td>\n",
              "      <td>B0004OPNTA</td>\n",
              "      <td>item good</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.061487213, -0.21710062, 0.457771, 0.309799...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A2MXCDMK91S143</td>\n",
              "      <td>B0004OPNTA</td>\n",
              "      <td>item perfect</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.43000737, 0.010507888, -0.03303118, 0.37484...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A2MXCDMK91S143</td>\n",
              "      <td>B0004OPNTA</td>\n",
              "      <td>item perfect</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.49877173, 0.21823983, 0.30037588, 0.4995622...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>A2FD834RITVVXH</td>\n",
              "      <td>B0004OPNTA</td>\n",
              "      <td>price great</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.14932808, -0.14713731, 0.31191912, 0.29198...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A39EN5XP8LDVX8</td>\n",
              "      <td>B0006TIA8Y</td>\n",
              "      <td>brand new</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.16046865, -0.15162839, 0.1722278, -0.227062...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36772</th>\n",
              "      <td>36772</td>\n",
              "      <td>AMM66EYPMRWL6</td>\n",
              "      <td>B01HCH03HS</td>\n",
              "      <td>protection good</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.2639229, 0.15404528, 0.9183527, -0.1205100...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36773</th>\n",
              "      <td>36773</td>\n",
              "      <td>AUZ1JF6JPUS99</td>\n",
              "      <td>B01HCH03HS</td>\n",
              "      <td>phone protected</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.4061425, -0.007457112, -0.060606517, -0.30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36774</th>\n",
              "      <td>36774</td>\n",
              "      <td>A3OS5Q3V8BCSVP</td>\n",
              "      <td>B01HCH03HS</td>\n",
              "      <td>product great</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.49021837, -0.43521026, 0.2531106, 0.4747344...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36775</th>\n",
              "      <td>36775</td>\n",
              "      <td>AHA390NSTV3VD</td>\n",
              "      <td>B01HCH03HS</td>\n",
              "      <td>case good</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.17711374, -0.5704334, 0.14784569, 0.202380...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36776</th>\n",
              "      <td>36776</td>\n",
              "      <td>AQSXSP82SKYYL</td>\n",
              "      <td>B01HCH03HS</td>\n",
              "      <td>phone pretty</td>\n",
              "      <td>1</td>\n",
              "      <td>[0.037095774, -0.008410223, 0.32270193, -0.275...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36777 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb58ba32-8c23-439e-92cf-1ded20e57426')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb58ba32-8c23-439e-92cf-1ded20e57426 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb58ba32-8c23-439e-92cf-1ded20e57426');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0         user_id     item_id          feature  sentiment  \\\n",
              "0               0   AHO9V8YUQ2G3Q  B0004OPNTA        item good          1   \n",
              "1               1  A2MXCDMK91S143  B0004OPNTA     item perfect          1   \n",
              "2               2  A2MXCDMK91S143  B0004OPNTA     item perfect          1   \n",
              "3               3  A2FD834RITVVXH  B0004OPNTA      price great          1   \n",
              "4               4  A39EN5XP8LDVX8  B0006TIA8Y        brand new          1   \n",
              "...           ...             ...         ...              ...        ...   \n",
              "36772       36772   AMM66EYPMRWL6  B01HCH03HS  protection good          1   \n",
              "36773       36773   AUZ1JF6JPUS99  B01HCH03HS  phone protected          1   \n",
              "36774       36774  A3OS5Q3V8BCSVP  B01HCH03HS    product great          1   \n",
              "36775       36775   AHA390NSTV3VD  B01HCH03HS        case good          1   \n",
              "36776       36776   AQSXSP82SKYYL  B01HCH03HS     phone pretty          1   \n",
              "\n",
              "                                                    vect  \n",
              "0      [-0.061487213, -0.21710062, 0.457771, 0.309799...  \n",
              "1      [0.43000737, 0.010507888, -0.03303118, 0.37484...  \n",
              "2      [0.49877173, 0.21823983, 0.30037588, 0.4995622...  \n",
              "3      [-0.14932808, -0.14713731, 0.31191912, 0.29198...  \n",
              "4      [0.16046865, -0.15162839, 0.1722278, -0.227062...  \n",
              "...                                                  ...  \n",
              "36772  [-0.2639229, 0.15404528, 0.9183527, -0.1205100...  \n",
              "36773  [-0.4061425, -0.007457112, -0.060606517, -0.30...  \n",
              "36774  [0.49021837, -0.43521026, 0.2531106, 0.4747344...  \n",
              "36775  [-0.17711374, -0.5704334, 0.14784569, 0.202380...  \n",
              "36776  [0.037095774, -0.008410223, 0.32270193, -0.275...  \n",
              "\n",
              "[36777 rows x 6 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_bert_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRItXJfIcQuz"
      },
      "outputs": [],
      "source": [
        "sentires_dir='/content/drive/Shareddrives/Unlimited Drive | @LicenseMarket/Recommender/cds/CDs_and_Vinyl'\n",
        "test_length=5\n",
        "sample_ratio=2\n",
        "# val_length=1\n",
        "neg_length=50\n",
        "dataset='cds_and_vinyl'\n",
        "save_path='/content/drive/Shareddrives/Unlimited Drive | @LicenseMarket/Recommender/cds/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwGTbhQJRUG1"
      },
      "outputs": [],
      "source": [
        "def get_user_item_dict(df_bert_sentiment,items_list):\n",
        "  user_dict = {}\n",
        "  item_dict = {}\n",
        "  # removed_users=['A1ULCCHD1QNOS5','A1ZS098EKPVT8F','A1K1WK6I122RX2','A2PAFKGAUSBMIE','AAOYA0DKWED4W','AJS9Q2JYS3DLJ','ADOF1VKGDCBWF','A3G11XDKGXZT9Q','A1R233YLWSRBTC','A3VH9QMH2UTX9D','A15DZOS6KVANQH']    \n",
        "  for index, row in df_bert_sentiment.iterrows():\n",
        "    user=row['user_id']\n",
        "    item=row['item_id']\n",
        "    if item in items_list:\n",
        "      if user not in user_dict:\n",
        "          user_dict[user] = [item]\n",
        "      else:\n",
        "          user_dict[user].append(item)\n",
        "      if item not in item_dict:\n",
        "          item_dict[item] = [user]\n",
        "      else:\n",
        "          item_dict[item].append(user)\n",
        "  return user_dict,item_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "hh8HbcOEjqgN",
        "outputId": "86a35454-9f60-4d83-beb5-9d3a583a1041"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-76579ca9-4469-403c-9c4f-ab68cf5eb964\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>description</th>\n",
              "      <th>title</th>\n",
              "      <th>feature</th>\n",
              "      <th>rank</th>\n",
              "      <th>price</th>\n",
              "      <th>asin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>4082</td>\n",
              "      <td>['Blueant product, not a fake']</td>\n",
              "      <td>SUPERTOOTH LIGHT BLUETOOTH MOBILE PHONE HANDSF...</td>\n",
              "      <td>['brand new']</td>\n",
              "      <td>['&gt;#3,652,634 in Cell Phones &amp; Accessories (Se...</td>\n",
              "      <td>$149.99</td>\n",
              "      <td>B000FQ4GT0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>4470</td>\n",
              "      <td>['Do it Swivel Glide - For installation in hol...</td>\n",
              "      <td>Swivel Glide</td>\n",
              "      <td>['China']</td>\n",
              "      <td>['&gt;#1,892,113 in Tools &amp; Home Improvement (See...</td>\n",
              "      <td>$6.99</td>\n",
              "      <td>B000HE4TPG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>5453</td>\n",
              "      <td>['Lucky Line Leather Belt Hook Key Ring - Made...</td>\n",
              "      <td>Lucky Line 45301 Leather Belt Hook</td>\n",
              "      <td>['China']</td>\n",
              "      <td>['&gt;#1,885,591 in Cell Phones &amp; Accessories (Se...</td>\n",
              "      <td>$6.71</td>\n",
              "      <td>B000LNU0NI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>9733</td>\n",
              "      <td>['Mot W385 Standard 940 mAh lithium ion Battery']</td>\n",
              "      <td>Motorola K1m W220 W385 Z6m Z6tv BT51 Battery</td>\n",
              "      <td>['Batteries']</td>\n",
              "      <td>['&gt;#101,575 in Cell Phones &amp; Accessories (See ...</td>\n",
              "      <td>$4.47</td>\n",
              "      <td>B001680REO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>10463</td>\n",
              "      <td>['LG VX8550 Chocolate Standard Battery Black']</td>\n",
              "      <td>LG VX8550 Chocolate Std Battery Black</td>\n",
              "      <td>['Batteries']</td>\n",
              "      <td>['&gt;#415,269 in Cell Phones &amp; Accessories (See ...</td>\n",
              "      <td>$5.48</td>\n",
              "      <td>B0019F6WXE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104134</th>\n",
              "      <td>585106</td>\n",
              "      <td>['perfectly fit, easy to install, covering the...</td>\n",
              "      <td>Galaxy S7 Active Case, DuroCase Hybrid Dual La...</td>\n",
              "      <td>['Discovery']</td>\n",
              "      <td>['&gt;#1,526,176 in Cell Phones &amp; Accessories (Se...</td>\n",
              "      <td>$11.29</td>\n",
              "      <td>B01H0YUGU2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104157</th>\n",
              "      <td>585147</td>\n",
              "      <td>['perfectly fit, easy to install, covering the...</td>\n",
              "      <td>LG Stylo 2 Plus Case / LG Stylus 2 Plus Case, ...</td>\n",
              "      <td>['Discovery']</td>\n",
              "      <td>['&gt;#347,482 in Cell Phones &amp; Accessories (See ...</td>\n",
              "      <td>$11.29</td>\n",
              "      <td>B01H12AL12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104173</th>\n",
              "      <td>585200</td>\n",
              "      <td>['perfectly fit, easy to install, covering the...</td>\n",
              "      <td>LG Stylo 2 Plus Case / LG Stylus 2 Plus Case, ...</td>\n",
              "      <td>['Discovery']</td>\n",
              "      <td>['&gt;#2,052,064 in Cell Phones &amp; Accessories (Se...</td>\n",
              "      <td>$11.29</td>\n",
              "      <td>B01H149Z80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104929</th>\n",
              "      <td>588361</td>\n",
              "      <td>['perfectly fit, easy to install, covering the...</td>\n",
              "      <td>DKmagic 6PCS Mini Earphone SD Card Macarons Ba...</td>\n",
              "      <td>['Discovery']</td>\n",
              "      <td>['&gt;#1,220,879 in Cell Phones &amp; Accessories (Se...</td>\n",
              "      <td>$2.96</td>\n",
              "      <td>B01HCSDRHA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105089</th>\n",
              "      <td>589106</td>\n",
              "      <td>['7 Plus Black']</td>\n",
              "      <td>Krusell Orsa FolioCase iPhone 8/7 Plus Black, ...</td>\n",
              "      <td>['60766']</td>\n",
              "      <td>['&gt;#4,551,229 in Cell Phones &amp; Accessories (Se...</td>\n",
              "      <td>$39.90</td>\n",
              "      <td>B01HFFRWY4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>259 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76579ca9-4469-403c-9c4f-ab68cf5eb964')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76579ca9-4469-403c-9c4f-ab68cf5eb964 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76579ca9-4469-403c-9c4f-ab68cf5eb964');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        Unnamed: 0                                        description  \\\n",
              "342           4082                    ['Blueant product, not a fake']   \n",
              "376           4470  ['Do it Swivel Glide - For installation in hol...   \n",
              "483           5453  ['Lucky Line Leather Belt Hook Key Ring - Made...   \n",
              "889           9733  ['Mot W385 Standard 940 mAh lithium ion Battery']   \n",
              "958          10463     ['LG VX8550 Chocolate Standard Battery Black']   \n",
              "...            ...                                                ...   \n",
              "104134      585106  ['perfectly fit, easy to install, covering the...   \n",
              "104157      585147  ['perfectly fit, easy to install, covering the...   \n",
              "104173      585200  ['perfectly fit, easy to install, covering the...   \n",
              "104929      588361  ['perfectly fit, easy to install, covering the...   \n",
              "105089      589106                                   ['7 Plus Black']   \n",
              "\n",
              "                                                    title        feature  \\\n",
              "342     SUPERTOOTH LIGHT BLUETOOTH MOBILE PHONE HANDSF...  ['brand new']   \n",
              "376                                          Swivel Glide      ['China']   \n",
              "483                    Lucky Line 45301 Leather Belt Hook      ['China']   \n",
              "889          Motorola K1m W220 W385 Z6m Z6tv BT51 Battery  ['Batteries']   \n",
              "958                 LG VX8550 Chocolate Std Battery Black  ['Batteries']   \n",
              "...                                                   ...            ...   \n",
              "104134  Galaxy S7 Active Case, DuroCase Hybrid Dual La...  ['Discovery']   \n",
              "104157  LG Stylo 2 Plus Case / LG Stylus 2 Plus Case, ...  ['Discovery']   \n",
              "104173  LG Stylo 2 Plus Case / LG Stylus 2 Plus Case, ...  ['Discovery']   \n",
              "104929  DKmagic 6PCS Mini Earphone SD Card Macarons Ba...  ['Discovery']   \n",
              "105089  Krusell Orsa FolioCase iPhone 8/7 Plus Black, ...      ['60766']   \n",
              "\n",
              "                                                     rank    price        asin  \n",
              "342     ['>#3,652,634 in Cell Phones & Accessories (Se...  $149.99  B000FQ4GT0  \n",
              "376     ['>#1,892,113 in Tools & Home Improvement (See...    $6.99  B000HE4TPG  \n",
              "483     ['>#1,885,591 in Cell Phones & Accessories (Se...    $6.71  B000LNU0NI  \n",
              "889     ['>#101,575 in Cell Phones & Accessories (See ...    $4.47  B001680REO  \n",
              "958     ['>#415,269 in Cell Phones & Accessories (See ...    $5.48  B0019F6WXE  \n",
              "...                                                   ...      ...         ...  \n",
              "104134  ['>#1,526,176 in Cell Phones & Accessories (Se...   $11.29  B01H0YUGU2  \n",
              "104157  ['>#347,482 in Cell Phones & Accessories (See ...   $11.29  B01H12AL12  \n",
              "104173  ['>#2,052,064 in Cell Phones & Accessories (Se...   $11.29  B01H149Z80  \n",
              "104929  ['>#1,220,879 in Cell Phones & Accessories (Se...    $2.96  B01HCSDRHA  \n",
              "105089  ['>#4,551,229 in Cell Phones & Accessories (Se...   $39.90  B01HFFRWY4  \n",
              "\n",
              "[259 rows x 7 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_meta.loc[df_meta['feature'].str.len()<15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMC87xZi-M9N"
      },
      "outputs": [],
      "source": [
        "def get_average_vect_train(df,not_in_columns):\n",
        "  lists=df.loc[:, ~df.columns.isin([not_in_columns])].values\n",
        "  words=df[not_in_columns].values\n",
        "  # print(words)\n",
        "  vects=[sub_list[0] for sub_list in lists]\n",
        "  average=np.average(np.array(vects),axis=0)\n",
        "  # print(average)\n",
        "  return average,list(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6v8XMseBgND"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woUUSm-mdH1A"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import final\n",
        "def get_item_matrix(inv_item_name_dict,item_name_dict,user_name_dict,df_bert_sentiment,df_meta,items_list):\n",
        "  print('----- get items matrix -------')\n",
        "  # normalized_rank=get_normalized_rank(df_meta)\n",
        "  item_quality_matrix = np.zeros((len(inv_item_name_dict), 1536))\n",
        "  print((len(inv_item_name_dict)))\n",
        "  i=0\n",
        "  features={}\n",
        "  for  item in inv_item_name_dict.keys():\n",
        "    # user_id= inv_user_name_dict[user]\n",
        "    features[item]=[]\n",
        "    if i%200==0:\n",
        "      print(i)\n",
        "    i+=1\n",
        "    # if i <4455:\n",
        "    #   continue\n",
        "    item_id= inv_item_name_dict[item]\n",
        "    final_vector=[]\n",
        "    if os.path.exists(save_path+'descriptions_bert/'+'df_bert_desc_{}.json'.format(item_id)) :\n",
        "      df_vect_desc= pd.read_json(save_path+'descriptions_bert/'+'df_bert_desc_{}.json'.format(item_id))\n",
        "      average_vect_desc,words_desc=get_average_vect_train(df_vect_desc,'description_words')\n",
        "\n",
        "      df_vect_title= pd.read_json(save_path+'titles_bert/'+'df_bert_title_{}.json'.format(item_id))\n",
        "      average_vect_title,words_title=get_average_vect_train(df_vect_title,'title_words')\n",
        "\n",
        "      # df_vect_feature= pd.read_json(save_path+'features_bert/'+'df_bert_feature_{}.json'.format(item_id))\n",
        "      # average_vect_feature,words_feature=get_average_vect_train(df_vect_feature,'feature_words')\n",
        "\n",
        "      # average_bert_sentiment,words_senti=get_average_bert_sentiment_train(item_id,df_bert_sentiment,user_name_dict,item_name_dict,'item_id',items_list)\n",
        "      \n",
        "      # rank=normalized_rank[item_id]\n",
        "      features[item].append(words_desc)\n",
        "      features[item].append(words_title)\n",
        "      # features[item].append(words_feature)\n",
        "      # features[item].append(words_senti)\n",
        "      # features[item].append([rank])\n",
        "      final_vector=list(average_vect_desc)\n",
        "      final_vector+=list(average_vect_title)\n",
        "      # final_vector+=list(average_vect_feature)\n",
        "      # final_vector+=list(average_bert_sentiment)\n",
        "      # final_vector+=[rank]\n",
        "      \n",
        "      if len(final_vector)>1536:\n",
        "          print(len(list(average_vect_desc)))\n",
        "          print(len(list(average_vect_title)))\n",
        "          # print(len(list(average_vect_feature)))\n",
        "          # print(len(list(average_bert_sentiment)))\n",
        "          print(item_id)\n",
        "      item_quality_matrix[item]=final_vector\n",
        "      # print(item_quality_matrix)\n",
        "\n",
        "  item_quality_matrix = np.array(item_quality_matrix, dtype='float32')\n",
        "  return item_quality_matrix,features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRpOh_KG94WX"
      },
      "outputs": [],
      "source": [
        "def get_user_matrix(df_meta,item_matrix,items_features,inv_user_name_dict,item_name_dict,user_name_dict,df_bert_sentiment,items_list):\n",
        "  # normalized_rank=get_normalized_rank(df_meta)\n",
        "  user_quality_matrix = np.zeros((len(inv_user_name_dict), 1536))\n",
        "  i=0\n",
        "  print('----- get users matrix -------')\n",
        "  print((len(inv_user_name_dict)))\n",
        "  user_features={}\n",
        "  for  user in inv_user_name_dict.keys():\n",
        "    if i%1000==0:\n",
        "      print(i)\n",
        "    i+=1\n",
        "    user_id= inv_user_name_dict[user]\n",
        "    # average_bert_sentiment,words_senti=get_average_bert_sentiment_train(user_id,df_bert_sentiment,user_name_dict,item_name_dict,'user_id',items_list)\n",
        "    # item_id= inv_item_name_dict[item]\n",
        "    final_vector=[]\n",
        "    items_interacted=df_bert_sentiment[df_bert_sentiment['user_id']==user_id]['item_id'].values\n",
        "    user_features[user]=[]\n",
        "    for item_id in items_interacted:\n",
        "      if item_id in items_list:\n",
        "        item=item_name_dict[item_id]\n",
        "        if os.path.exists(save_path+'descriptions_bert/'+'df_bert_desc_{}.json'.format(item_id)):\n",
        "          final_vector.append(item_matrix[item])\n",
        "          user_features[user].append(items_features[item])\n",
        "    \n",
        "    final_average=np.average(final_vector,axis=0)\n",
        "    # final_average[2304:3840]=average_bert_sentiment\n",
        "    \n",
        "    # if np.isnan(np.sum(final_average)):\n",
        "    #   print(\"YEEEEEEEEESSS\")\n",
        "    if ~np.isnan(np.sum(final_average)):\n",
        "      user_quality_matrix[user]=final_average\n",
        "    else:\n",
        "      print(user_id)\n",
        "  user_quality_matrix = np.array(user_quality_matrix, dtype='float32')\n",
        "  return user_quality_matrix,user_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfm9U58AAOnk"
      },
      "outputs": [],
      "source": [
        "def sample_training_pairs(user, training_items, item_set, sample_ratio=10):\n",
        "    positive_items = set(training_items)\n",
        "    negative_items = set()\n",
        "    for item in item_set:\n",
        "        if item not in positive_items:\n",
        "            negative_items.add(item)\n",
        "    neg_length = len(positive_items) * sample_ratio\n",
        "    negative_items = np.random.choice(np.array(list(negative_items)), neg_length, replace=False)\n",
        "    train_pairs = []\n",
        "    for p_item in positive_items:\n",
        "        train_pairs.append([user, p_item, 1])\n",
        "    for n_item in negative_items:\n",
        "        train_pairs.append([user, n_item, 0])\n",
        "    return train_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrl9nTx0iGFW"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "def get_items_list():\n",
        "  items_list=[]\n",
        "  names=glob.glob(save_path+\"titles_bert/*.json\")\n",
        "  for name in names:\n",
        "    items_list.append(name.split('/')[-1].split('_')[-1][0:-5])\n",
        "  return items_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJFlUjlGqyYp"
      },
      "outputs": [],
      "source": [
        "final_items = []\n",
        "final_users=[]\n",
        "training_pairs = np.loadtxt(save_path+'training_data.txt',dtype=str)\n",
        "for pair in training_pairs:\n",
        "  final_items.append(pair[1])\n",
        "  final_users.append(pair[0])\n",
        "with open(save_path+'test_data.pickle', 'rb') as f:\n",
        "  test_pairs= pickle.load(f)\n",
        "  for user_id in test_pairs.keys():\n",
        "    final_users.append(user_id)\n",
        "    items_ids=test_pairs[user_id][0]\n",
        "    final_items+=items_ids\n",
        "final_items=list(set(final_items))\n",
        "final_users=list(set(final_users))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE2kP5MDra4K",
        "outputId": "46e62b55-ebc0-4bd9-bb0b-7d9209703aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12087\n",
            "6934\n"
          ]
        }
      ],
      "source": [
        "print(len(final_items))\n",
        "print(len(final_users))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRV2woMaQP2z"
      },
      "outputs": [],
      "source": [
        "from re import S\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "import pickle\n",
        "# from torch._C import R\n",
        "import tqdm\n",
        "from torch.random import seed\n",
        "\n",
        "\n",
        "class AmazonDataset():\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.user_name_dict = {}  # rename users to integer names\n",
        "        self.item_name_dict = {}\n",
        "        self.feature_name_dict = {}\n",
        "\n",
        "        self.features = {}  # feature list\n",
        "        self.users = []\n",
        "        self.items = []\n",
        "\n",
        "        # the interacted items for each user, sorted with date {user:[i1, i2, i3, ...], user:[i1, i2, i3, ...]}\n",
        "        self.user_hist_inter_dict = {}\n",
        "        # the interacted users for each item\n",
        "        self.item_hist_inter_dict = {}  \n",
        "\n",
        "        self.user_num = None\n",
        "        self.item_num = None\n",
        "        self.feature_num = 1536# number of features\n",
        "\n",
        "        self.user_feature_matrix = None  # user aspect attention matrix\n",
        "        self.item_feature_matrix = None  # item aspect quality matrix\n",
        "\n",
        "        self.training_data = None\n",
        "        self.test_data = None\n",
        "        self.pre_processing()\n",
        "        self.get_user_item_feature_matrix()\n",
        "        self.sample_training()  # sample training data, for traning BPR loss\n",
        "        self.sample_test()  # sample test data\n",
        "\n",
        "    def pre_processing(self,):\n",
        "        self.items_list=get_items_list()\n",
        "        user_dict, item_dict = get_user_item_dict(df_bert_sentiment,self.items_list)  # not sorted with time\n",
        "        print(len(item_dict))\n",
        "        user_item_date_dict = {}   # {(user, item): date, (user, item): date ...}  # used to remove duplicate\n",
        "        # removed_users=['A1ULCCHD1QNOS5','A1ZS098EKPVT8F','A1K1WK6I122RX2','A2PAFKGAUSBMIE','AAOYA0DKWED4W','AJS9Q2JYS3DLJ','ADOF1VKGDCBWF','A3G11XDKGXZT9Q','A1R233YLWSRBTC','A3VH9QMH2UTX9D','A15DZOS6KVANQH']\n",
        "        for i, row in df_review.iterrows():\n",
        "            user = row['reviewerID']\n",
        "            # if user not in removed_users:\n",
        "            item = row['asin']\n",
        "            date = row['unixReviewTime']\n",
        "            if item in self.items_list:\n",
        "              if user in user_dict and item in user_dict[user] and (user, item) not in user_item_date_dict:\n",
        "                  user_item_date_dict[(user, item)] = date\n",
        "      \n",
        "        # rename users, items, and features to integer names\n",
        "        user_name_dict = {}\n",
        "        item_name_dict = {}\n",
        "        # feature_name_dict = {}\n",
        "        # features = get_feature_list(df_bert_sentiment,df_bert_desc,)\n",
        "        \n",
        "        count = 0\n",
        "        for user in user_dict:\n",
        "            if user not in user_name_dict:\n",
        "                user_name_dict[user] = count\n",
        "                count += 1\n",
        "        count = 0\n",
        "        for item in item_dict:\n",
        "            if item not in item_name_dict:\n",
        "                item_name_dict[item] = count\n",
        "                count += 1\n",
        "        self.inv_user_name_dict = {v: k for k, v in user_name_dict.items()}\n",
        "        self.inv_item_name_dict = {v: k for k, v in item_name_dict.items()}\n",
        "        \n",
        "        # for i in range(len(sentiment_data)):\n",
        "        #     sentiment_data[i][0] = user_name_dict[sentiment_data[i][0]]\n",
        "        #     sentiment_data[i][1] = item_name_dict[sentiment_data[i][1]]\n",
        "        #     for j in range(len(sentiment_data[i]) - 2):\n",
        "        #         sentiment_data[i][j+2][0] = feature_name_dict[sentiment_data[i][j + 2][0]]\n",
        "\n",
        "        renamed_user_item_date_dict = {}\n",
        "        for key, value in user_item_date_dict.items():\n",
        "            renamed_user_item_date_dict[user_name_dict[key[0]], item_name_dict[key[1]]] = value\n",
        "\n",
        "        # sort with date\n",
        "        renamed_user_item_date_dict  = dict(sorted(renamed_user_item_date_dict .items(), key=lambda item: item[1]))\n",
        "\n",
        "        user_hist_inter_dict = {}  # {\"u1\": [i1, i2, i3, ...], \"u2\": [i1, i2, i3, ...]}, sort with time\n",
        "        item_hist_inter_dict = {}\n",
        "        # ranked_user_item_dict = {}  # {\"u1\": [i1, i2, i3, ...], \"u2\": [i1, i2, i3, ...]}\n",
        "        for key, value in renamed_user_item_date_dict.items():\n",
        "            user = key[0]\n",
        "            item = key[1]\n",
        "            if user not in user_hist_inter_dict:\n",
        "                user_hist_inter_dict[user] = [item]\n",
        "            else:\n",
        "                user_hist_inter_dict[user].append(item)\n",
        "            if item not in item_hist_inter_dict:\n",
        "                item_hist_inter_dict[item] = [user]\n",
        "            else:\n",
        "                item_hist_inter_dict[item].append(user)\n",
        "\n",
        "        user_hist_inter_dict = dict(sorted(user_hist_inter_dict.items()))\n",
        "        item_hist_inter_dict = dict(sorted(item_hist_inter_dict.items()))\n",
        "\n",
        "        users = list(user_hist_inter_dict.keys())\n",
        "        items = list(item_hist_inter_dict.keys())\n",
        "\n",
        "        self.user_name_dict = user_name_dict\n",
        "        self.item_name_dict = item_name_dict\n",
        "        self.user_hist_inter_dict = user_hist_inter_dict\n",
        "        self.item_hist_inter_dict = item_hist_inter_dict\n",
        "        self.users = users\n",
        "        self.items = items\n",
        "        self.user_num = len(users)\n",
        "        self.item_num = len(items)\n",
        "        return True\n",
        "    \n",
        "    def get_user_item_feature_matrix(self,):\n",
        "        # exclude test data from the sentiment data to construct matrix\n",
        "        train_u_i_set = set()\n",
        "        for user, items in self.user_hist_inter_dict.items():\n",
        "            items = items[:-test_length]\n",
        "            for item in items:\n",
        "                train_u_i_set.add((user, item))\n",
        "\n",
        "        self.item_feature_matrix,self.item_features = get_item_matrix(self.inv_item_name_dict,self.item_name_dict,self.user_name_dict,df_bert_sentiment,df_meta,self.items_list)\n",
        "        self.user_feature_matrix,self.user_features = get_user_matrix(df_meta,self.item_feature_matrix,self.item_features,self.inv_user_name_dict,self.item_name_dict,self.user_name_dict,df_bert_sentiment,self.items_list)\n",
        "        \n",
        "        return True\n",
        "    def sample_training(self):\n",
        "        print('======================= sample training data =======================')\n",
        "        # print(self.user_feature_matrix.shape, self.item_feature_matrix.shape)\n",
        "        training_data = []\n",
        "        training_pairs = np.loadtxt(save_path+'training_data.txt',dtype=str)\n",
        "        for pair in training_pairs:\n",
        "          training_data.append([self.user_name_dict[pair[0]],self.item_name_dict[pair[1]],int(pair[2])])\n",
        "        print('# training samples :', len(training_data))\n",
        "        self.training_data = np.array(training_data)\n",
        "        return True\n",
        "    \n",
        "    def sample_test(self):\n",
        "        print('======================= sample test data =======================')\n",
        "        user_item_label_list = []  # [[u, [item1, item2, ...], [l1, l2, ...]], ...]\n",
        "        with open(save_path+'test_data.pickle', 'rb') as f:\n",
        "            test_pairs= pickle.load(f)\n",
        "        for user_id in test_pairs.keys():\n",
        "          user=self.user_name_dict[user_id]\n",
        "          items_ids=test_pairs[user_id][0]\n",
        "          labels=test_pairs[user_id][1]\n",
        "          items=np.array([self.item_name_dict[item] for item in items_ids])\n",
        "          labels=np.array([float(label) for label in labels])\n",
        "          user_item_label_list.append([user,items,labels])\n",
        "        print('# test samples :', len(user_item_label_list))\n",
        "        self.test_data = np.array(user_item_label_list)\n",
        "        return True\n",
        "    # def sample_training(self):\n",
        "    #     print('======================= sample training data =======================')\n",
        "    #     # print(self.user_feature_matrix.shape, self.item_feature_matrix.shape)\n",
        "    #     training_data = []\n",
        "    #     item_set = set(self.items)\n",
        "    #     for user, items in self.user_hist_inter_dict.items():\n",
        "    #         if len(items)>9:\n",
        "    #           items = items[:-(test_length)]\n",
        "    #           training_pairs = sample_training_pairs(\n",
        "    #               user, \n",
        "    #               items, \n",
        "    #               item_set, \n",
        "    #               sample_ratio)\n",
        "    #           for pair in training_pairs:\n",
        "    #               training_data.append(pair)\n",
        "    #     print('# training samples :', len(training_data))\n",
        "    #     self.training_data = np.array(training_data)\n",
        "    #     return True\n",
        "    \n",
        "    # def sample_test(self):\n",
        "    #     print('======================= sample test data =======================')\n",
        "    #     user_item_label_list = []  # [[u, [item1, item2, ...], [l1, l2, ...]], ...]\n",
        "    #     for user, items in self.user_hist_inter_dict.items():\n",
        "    #       if len(items)>9:\n",
        "    #         items = items[-(test_length):]\n",
        "    #         user_item_label_list.append([user, items, np.ones(len(items))])  # add the test items\n",
        "    #         neg_length=len(items)*10\n",
        "    #         negative_items = [item for item in self.items if \n",
        "    #             item not in self.user_hist_inter_dict[user]]  # the not interacted items\n",
        "    #         negative_items = np.random.choice(np.array(negative_items), neg_length, replace=False)\n",
        "    #         user_item_label_list[-1][1] = np.concatenate((user_item_label_list[-1][1], negative_items), axis=0)\n",
        "    #         user_item_label_list[-1][2] = np.concatenate((user_item_label_list[-1][2], np.zeros(neg_length)), axis=0)\n",
        "    #     print('# test samples :', len(user_item_label_list))\n",
        "    #     self.test_data = np.array(user_item_label_list)\n",
        "    #     return True\n",
        "\n",
        "    def save(self, save_path):\n",
        "        return True\n",
        "    \n",
        "    def load(self):\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJU1IgY8LF07"
      },
      "outputs": [],
      "source": [
        "def amazon_preprocessing():\n",
        "    rec_dataset = AmazonDataset()\n",
        "    return rec_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ECM0Lw3NFbC"
      },
      "outputs": [],
      "source": [
        "def dataset_init():\n",
        "\tif dataset == \"yelp\":\n",
        "\t\trec_dataset = yelp_preprocessing()\n",
        "\telif dataset == \"cell_phones\" or \"kindle_store\" or \"electronic\" or \"cds_and_vinyl\":\n",
        "\t\trec_dataset = amazon_preprocessing()\n",
        "\treturn rec_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAb2lKMWb1Zo"
      },
      "outputs": [],
      "source": [
        "dataset='cds_and_vinyl'\n",
        "gpu=True\n",
        "cuda='0'\n",
        "weight_decay=0.00001\n",
        "lr=0.01\n",
        "epochs=100\n",
        "batch_size=64\n",
        "rec_k=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4Zo-5mANolR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "class UserItemInterDataset(Dataset):\n",
        "    def __init__(self, data, user_feature_matrix, item_feature_matrix):\n",
        "        self.data = data\n",
        "        self.user_feature_matrix = user_feature_matrix\n",
        "        self.item_feature_matrix = item_feature_matrix\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user = self.data[index][0]\n",
        "        item = self.data[index][1]\n",
        "        label = self.data[index][2]\n",
        "        user_feature = self.user_feature_matrix[user]\n",
        "        item_feature = self.item_feature_matrix[item]\n",
        "        return user_feature, item_feature, label\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Black-box model..."
      ],
      "metadata": {
        "id": "ZbmHCBLTUqLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lma7BKCoNySG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import ndcg_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep5Pi9VbNtvi"
      },
      "outputs": [],
      "source": [
        "def compute_ndcg(test_data, user_feature_matrix, item_feature_matrix, k, model, device):\n",
        "    model.eval()\n",
        "    ndcgs = []\n",
        "    with torch.no_grad():\n",
        "        for row in test_data:\n",
        "            user = row[0]\n",
        "            items = row[1]\n",
        "            gt_labels = row[2]\n",
        "            user_features = np.array([user_feature_matrix[user] for i in range(len(items))])\n",
        "            item_features = np.array([item_feature_matrix[item] for item in items])\n",
        "            scores = model(torch.from_numpy(user_features).to(device),\n",
        "                                    torch.from_numpy(item_features).to(device)).squeeze()\n",
        "            scores = np.array(scores.to('cpu'))\n",
        "            ndcg = ndcg_score([gt_labels], [scores], k=k)\n",
        "            ndcgs.append(ndcg)\n",
        "    ave_ndcg = np.mean(ndcgs)\n",
        "    return ave_ndcg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnvfmuGXM-7F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import tqdm\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-szQN49N-l0"
      },
      "outputs": [],
      "source": [
        "from numpy import core\n",
        "\n",
        "class BaseRecModel(torch.nn.Module):\n",
        "    def __init__(self, feature_length):\n",
        "        super(BaseRecModel, self).__init__()\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(feature_length * 2, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, user_feature, item_feature):\n",
        "        fusion = torch.cat((user_feature, item_feature), 1)\n",
        "        out = self.fc(fusion)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05LMQ0K63i30",
        "outputId": "d8345ed4-26e2-402b-c8d0-2b7a4ab351b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if gpu:\n",
        "  device = torch.device('cuda:%s' % cuda)\n",
        "else:\n",
        "  device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VSW0s52y3vR",
        "outputId": "b655afc5-59ad-4af4-cac4-70d4e7082d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1945\n",
            "----- get items matrix -------\n",
            "1945\n",
            "0\n",
            "200\n",
            "400\n",
            "600\n",
            "800\n",
            "1000\n",
            "1200\n",
            "1400\n",
            "1600\n",
            "1800\n",
            "----- get users matrix -------\n",
            "6794\n",
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "======================= sample training data =======================\n",
            "# training samples : 2871\n",
            "======================= sample test data =======================\n",
            "# test samples : 93\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "rec_dataset = dataset_init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unSEhZk-qTdK",
        "outputId": "a887dc48-9927-4a26-ba74-e2cd7f015521"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1945, 2304)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rec_dataset.item_feature_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taAj1CnnK50V"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(save_path, dataset + \"_dataset_obj.pickle\"), 'rb') as inp:\n",
        "  rec_dataset = pickle.load(inp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2G66iinM9Os",
        "outputId": "4a8d133d-e411-4a42-f8c1-883449a4584e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA 0\n",
            "init ndcg: 0.07731034104130662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/240 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0:  training loss:  0.6559843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 1/240 [00:00<02:20,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0:  training loss:  0.6559843 NDCG:  0.08904614145749981\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|          | 2/240 [00:00<01:34,  2.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1:  training loss:  0.6366049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  1%|▏         | 3/240 [00:01<01:16,  3.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2:  training loss:  0.6350551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 4/240 [00:01<01:09,  3.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3:  training loss:  0.6342837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▏         | 5/240 [00:01<01:03,  3.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4:  training loss:  0.6332564\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  2%|▎         | 6/240 [00:01<00:59,  3.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5:  training loss:  0.63218343\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 7/240 [00:02<00:56,  4.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 6:  training loss:  0.63148654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  3%|▎         | 8/240 [00:02<00:55,  4.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7:  training loss:  0.63044715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 9/240 [00:02<00:56,  4.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 8:  training loss:  0.62933797\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  4%|▍         | 10/240 [00:02<00:55,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 9:  training loss:  0.6282087\n",
            "epoch 10:  training loss:  0.62691575\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▍         | 11/240 [00:03<01:15,  3.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10:  training loss:  0.62691575 NDCG:  0.22428561723041499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 12/240 [00:03<01:07,  3.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 11:  training loss:  0.625341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  5%|▌         | 13/240 [00:03<01:02,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 12:  training loss:  0.62412286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▌         | 14/240 [00:03<00:58,  3.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 13:  training loss:  0.62280476\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  6%|▋         | 15/240 [00:04<00:54,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 14:  training loss:  0.62134904\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 16/240 [00:04<00:54,  4.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 15:  training loss:  0.6195056\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  7%|▋         | 17/240 [00:04<00:52,  4.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 16:  training loss:  0.61726743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 18/240 [00:04<00:51,  4.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 17:  training loss:  0.61640984\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 19/240 [00:05<00:52,  4.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 18:  training loss:  0.6146805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  8%|▊         | 20/240 [00:05<00:52,  4.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 19:  training loss:  0.6118428\n",
            "epoch 20:  training loss:  0.6098647\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 21/240 [00:05<01:08,  3.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 20:  training loss:  0.6098647 NDCG:  0.2278696250066917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  9%|▉         | 22/240 [00:06<01:02,  3.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 21:  training loss:  0.60771394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|▉         | 23/240 [00:06<00:58,  3.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 22:  training loss:  0.6051721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 24/240 [00:06<00:56,  3.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 23:  training loss:  0.6031347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 10%|█         | 25/240 [00:06<00:53,  3.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 24:  training loss:  0.5993265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 11%|█         | 26/240 [00:06<00:51,  4.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 25:  training loss:  0.59763217\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 11%|█▏        | 27/240 [00:07<00:51,  4.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 26:  training loss:  0.59504855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 28/240 [00:07<00:50,  4.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 27:  training loss:  0.59245473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▏        | 29/240 [00:07<00:49,  4.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 28:  training loss:  0.5896407\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 12%|█▎        | 30/240 [00:07<00:48,  4.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 29:  training loss:  0.5874632\n",
            "epoch 30:  training loss:  0.5842192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 31/240 [00:08<01:03,  3.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 30:  training loss:  0.5842192 NDCG:  0.2370903680016749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 13%|█▎        | 32/240 [00:08<01:00,  3.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 31:  training loss:  0.58108675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 33/240 [00:08<00:55,  3.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 32:  training loss:  0.57764804\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 14%|█▍        | 34/240 [00:09<00:52,  3.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 33:  training loss:  0.5767197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▍        | 35/240 [00:09<00:50,  4.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 34:  training loss:  0.5722899\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 36/240 [00:09<00:49,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 35:  training loss:  0.5701174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 15%|█▌        | 37/240 [00:09<00:48,  4.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 36:  training loss:  0.56783366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▌        | 38/240 [00:09<00:47,  4.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 37:  training loss:  0.5644155\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 16%|█▋        | 39/240 [00:10<00:48,  4.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 38:  training loss:  0.5643587\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 40/240 [00:10<00:48,  4.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 39:  training loss:  0.56045276\n",
            "epoch 40:  training loss:  0.5556122\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 17%|█▋        | 41/240 [00:10<01:02,  3.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 40:  training loss:  0.5556122 NDCG:  0.24337287767232893\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 42/240 [00:11<00:57,  3.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 41:  training loss:  0.5570251\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 43/240 [00:11<00:54,  3.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 42:  training loss:  0.5508098\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 18%|█▊        | 44/240 [00:11<00:51,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 43:  training loss:  0.54837817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|█▉        | 45/240 [00:11<00:48,  3.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 44:  training loss:  0.54394525\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 19%|█▉        | 46/240 [00:12<00:46,  4.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 45:  training loss:  0.54389817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 48/240 [00:12<00:42,  4.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 46:  training loss:  0.5430806\n",
            "epoch 47:  training loss:  0.54080117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 20%|██        | 49/240 [00:12<00:41,  4.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 48:  training loss:  0.53772545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 21%|██        | 50/240 [00:12<00:42,  4.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 49:  training loss:  0.5330241\n",
            "epoch 50:  training loss:  0.5298495\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 21%|██▏       | 51/240 [00:13<00:56,  3.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 50:  training loss:  0.5298495 NDCG:  0.2618063692540938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 52/240 [00:13<00:51,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 51:  training loss:  0.52634436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▏       | 53/240 [00:13<00:49,  3.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 52:  training loss:  0.5224576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 22%|██▎       | 54/240 [00:14<00:50,  3.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 53:  training loss:  0.5221452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 55/240 [00:14<00:49,  3.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 54:  training loss:  0.52260864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 23%|██▎       | 56/240 [00:14<00:52,  3.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 55:  training loss:  0.519084\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 57/240 [00:15<00:57,  3.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 56:  training loss:  0.51691777\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 24%|██▍       | 58/240 [00:15<01:03,  2.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 57:  training loss:  0.51588005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▍       | 59/240 [00:15<00:59,  3.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 58:  training loss:  0.5096834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 60/240 [00:16<00:59,  3.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 59:  training loss:  0.50604415\n",
            "epoch 60:  training loss:  0.5046773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 25%|██▌       | 61/240 [00:17<01:28,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 60:  training loss:  0.5046773 NDCG:  0.2819376457699049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 26%|██▌       | 62/240 [00:17<01:24,  2.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 61:  training loss:  0.50321645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 26%|██▋       | 63/240 [00:17<01:14,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 62:  training loss:  0.5090624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 65/240 [00:18<00:56,  3.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 63:  training loss:  0.49168006\n",
            "epoch 64:  training loss:  0.48918712\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 67/240 [00:18<00:40,  4.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 65:  training loss:  0.48831847\n",
            "epoch 66:  training loss:  0.48680016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 69/240 [00:18<00:32,  5.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 67:  training loss:  0.49218366\n",
            "epoch 68:  training loss:  0.48096052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 29%|██▉       | 70/240 [00:18<00:30,  5.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 69:  training loss:  0.4840072\n",
            "epoch 70:  training loss:  0.47810465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 30%|██▉       | 71/240 [00:19<00:37,  4.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 70:  training loss:  0.47810465 NDCG:  0.29091009335652457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 73/240 [00:19<00:32,  5.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 71:  training loss:  0.479787\n",
            "epoch 72:  training loss:  0.48126686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███       | 74/240 [00:19<00:34,  4.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 73:  training loss:  0.4612952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 31%|███▏      | 75/240 [00:20<00:34,  4.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 74:  training loss:  0.46222398\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 76/240 [00:20<00:39,  4.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 75:  training loss:  0.4599989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▏      | 77/240 [00:20<00:46,  3.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 76:  training loss:  0.4612971\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 32%|███▎      | 78/240 [00:21<00:47,  3.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 77:  training loss:  0.4691077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 79/240 [00:21<00:51,  3.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 78:  training loss:  0.45463863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 33%|███▎      | 80/240 [00:21<00:57,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 79:  training loss:  0.45895654\n",
            "epoch 80:  training loss:  0.44757903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 81/240 [00:22<01:21,  1.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 80:  training loss:  0.44757903 NDCG:  0.3087299035323336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 34%|███▍      | 82/240 [00:23<01:15,  2.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 81:  training loss:  0.44914037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▍      | 83/240 [00:23<01:11,  2.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 82:  training loss:  0.46183586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 84/240 [00:23<01:01,  2.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 83:  training loss:  0.43950114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 35%|███▌      | 85/240 [00:24<00:54,  2.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 84:  training loss:  0.43316528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 36%|███▌      | 86/240 [00:24<00:48,  3.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 85:  training loss:  0.4346923\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 36%|███▋      | 87/240 [00:24<00:43,  3.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 86:  training loss:  0.452036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 88/240 [00:24<00:40,  3.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 87:  training loss:  0.4353205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 37%|███▋      | 89/240 [00:25<00:38,  3.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 88:  training loss:  0.42705333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 90/240 [00:25<00:36,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 89:  training loss:  0.43626645\n",
            "epoch 90:  training loss:  0.41913956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 91/240 [00:25<00:45,  3.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 90:  training loss:  0.41913956 NDCG:  0.30095865252518306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 38%|███▊      | 92/240 [00:25<00:42,  3.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 91:  training loss:  0.43031722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 39%|███▉      | 93/240 [00:26<00:42,  3.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 92:  training loss:  0.43032938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 39%|███▉      | 94/240 [00:26<00:40,  3.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 93:  training loss:  0.41695106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|███▉      | 95/240 [00:26<00:38,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 94:  training loss:  0.41301563\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 96/240 [00:26<00:36,  3.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 95:  training loss:  0.41155073\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 40%|████      | 97/240 [00:27<00:34,  4.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 96:  training loss:  0.41692406\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 41%|████      | 98/240 [00:27<00:33,  4.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 97:  training loss:  0.3903146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 41%|████▏     | 99/240 [00:27<00:32,  4.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 98:  training loss:  0.42191985\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 100/240 [00:27<00:32,  4.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 99:  training loss:  0.40901834\n",
            "epoch 100:  training loss:  0.40936852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 42%|████▏     | 101/240 [00:28<00:42,  3.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 100:  training loss:  0.40936852 NDCG:  0.3107498755477088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 42%|████▎     | 102/240 [00:28<00:40,  3.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 101:  training loss:  0.42588884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 103/240 [00:28<00:37,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 102:  training loss:  0.39845535\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 43%|████▎     | 104/240 [00:29<00:35,  3.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 103:  training loss:  0.40244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 44%|████▍     | 105/240 [00:29<00:33,  4.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 104:  training loss:  0.39932865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 107/240 [00:29<00:29,  4.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 105:  training loss:  0.41523197\n",
            "epoch 106:  training loss:  0.40553096\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 108/240 [00:29<00:26,  4.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 107:  training loss:  0.38465422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 45%|████▌     | 109/240 [00:30<00:28,  4.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 108:  training loss:  0.38000238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 110/240 [00:30<00:29,  4.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 109:  training loss:  0.37290955\n",
            "epoch 110:  training loss:  0.3930898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 112/240 [00:30<00:33,  3.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 110:  training loss:  0.3930898 NDCG:  0.32187189928494353\n",
            "epoch 111:  training loss:  0.3827937\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 47%|████▋     | 113/240 [00:31<00:29,  4.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 112:  training loss:  0.38525683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 114/240 [00:31<00:29,  4.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 113:  training loss:  0.37280816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 115/240 [00:31<00:28,  4.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 114:  training loss:  0.3925651\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 48%|████▊     | 116/240 [00:31<00:28,  4.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 115:  training loss:  0.3632952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 49%|████▉     | 117/240 [00:32<00:28,  4.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 116:  training loss:  0.37438062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 49%|████▉     | 118/240 [00:32<00:28,  4.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 117:  training loss:  0.3759567\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|████▉     | 119/240 [00:32<00:27,  4.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 118:  training loss:  0.34566367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 120/240 [00:32<00:27,  4.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 119:  training loss:  0.39201164\n",
            "epoch 120:  training loss:  0.36662903\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 121/240 [00:33<00:35,  3.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 120:  training loss:  0.36662903 NDCG:  0.329753277586806\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████▏    | 123/240 [00:33<00:29,  3.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 121:  training loss:  0.37571254\n",
            "epoch 122:  training loss:  0.386705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 125/240 [00:33<00:24,  4.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 123:  training loss:  0.35439774\n",
            "epoch 124:  training loss:  0.35738134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 52%|█████▎    | 126/240 [00:34<00:22,  5.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 125:  training loss:  0.34376857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 127/240 [00:34<00:23,  4.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 126:  training loss:  0.35447735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 53%|█████▎    | 128/240 [00:34<00:23,  4.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 127:  training loss:  0.3612941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 129/240 [00:34<00:25,  4.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 128:  training loss:  0.34847474\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 130/240 [00:35<00:25,  4.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 129:  training loss:  0.36162597\n",
            "epoch 130:  training loss:  0.35005978\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 55%|█████▍    | 131/240 [00:35<00:35,  3.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 130:  training loss:  0.35005978 NDCG:  0.326089360975312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 55%|█████▌    | 132/240 [00:35<00:33,  3.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 131:  training loss:  0.3462741\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 55%|█████▌    | 133/240 [00:36<00:32,  3.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 132:  training loss:  0.3363633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 56%|█████▌    | 134/240 [00:36<00:31,  3.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 133:  training loss:  0.33202776\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 56%|█████▋    | 135/240 [00:36<00:29,  3.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 134:  training loss:  0.3305079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 57%|█████▋    | 136/240 [00:36<00:27,  3.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 135:  training loss:  0.3450936\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 57%|█████▋    | 137/240 [00:37<00:26,  3.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 136:  training loss:  0.34005508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 57%|█████▊    | 138/240 [00:37<00:25,  3.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 137:  training loss:  0.34549943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 139/240 [00:37<00:24,  4.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 138:  training loss:  0.3345901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 58%|█████▊    | 140/240 [00:37<00:23,  4.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 139:  training loss:  0.32661918\n",
            "epoch 140:  training loss:  0.3378381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 142/240 [00:38<00:27,  3.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 140:  training loss:  0.3378381 NDCG:  0.30114012221455055\n",
            "epoch 141:  training loss:  0.32687974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 144/240 [00:38<00:19,  4.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 142:  training loss:  0.30695412\n",
            "epoch 143:  training loss:  0.30499697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 146/240 [00:39<00:17,  5.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 144:  training loss:  0.31725237\n",
            "epoch 145:  training loss:  0.33050227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 148/240 [00:39<00:15,  5.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 146:  training loss:  0.32733306\n",
            "epoch 147:  training loss:  0.30308357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▎   | 150/240 [00:39<00:14,  6.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 148:  training loss:  0.33652154\n",
            "epoch 149:  training loss:  0.33111772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 63%|██████▎   | 151/240 [00:40<00:18,  4.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 150:  training loss:  0.29522848\n",
            "epoch 150:  training loss:  0.29522848 NDCG:  0.3405321344534416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 153/240 [00:40<00:15,  5.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 151:  training loss:  0.3047018\n",
            "epoch 152:  training loss:  0.31692863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▍   | 155/240 [00:40<00:13,  6.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 153:  training loss:  0.31318566\n",
            "epoch 154:  training loss:  0.27509367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 157/240 [00:41<00:12,  6.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 155:  training loss:  0.30458575\n",
            "epoch 156:  training loss:  0.292929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▋   | 159/240 [00:41<00:12,  6.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 157:  training loss:  0.28962952\n",
            "epoch 158:  training loss:  0.2819192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 67%|██████▋   | 160/240 [00:41<00:11,  6.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 159:  training loss:  0.31146365\n",
            "epoch 160:  training loss:  0.29956818\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 162/240 [00:41<00:14,  5.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 160:  training loss:  0.29956818 NDCG:  0.34453536610308794\n",
            "epoch 161:  training loss:  0.31077382\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 164/240 [00:42<00:12,  6.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 162:  training loss:  0.2830786\n",
            "epoch 163:  training loss:  0.2616557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 166/240 [00:42<00:11,  6.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 164:  training loss:  0.28758764\n",
            "epoch 165:  training loss:  0.28547114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 168/240 [00:42<00:11,  6.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 166:  training loss:  0.3074325\n",
            "epoch 167:  training loss:  0.30125028\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 170/240 [00:43<00:10,  6.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 168:  training loss:  0.28790602\n",
            "epoch 169:  training loss:  0.27654502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 71%|███████▏  | 171/240 [00:43<00:13,  5.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 170:  training loss:  0.27750453\n",
            "epoch 170:  training loss:  0.27750453 NDCG:  0.3374773801100938\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 173/240 [00:43<00:11,  5.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 171:  training loss:  0.2722199\n",
            "epoch 172:  training loss:  0.24927747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 175/240 [00:43<00:10,  6.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 173:  training loss:  0.29379332\n",
            "epoch 174:  training loss:  0.2833095\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 177/240 [00:44<00:09,  6.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 175:  training loss:  0.2932887\n",
            "epoch 176:  training loss:  0.2697788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▍  | 179/240 [00:44<00:09,  6.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 177:  training loss:  0.24739315\n",
            "epoch 178:  training loss:  0.2594655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 75%|███████▌  | 180/240 [00:44<00:08,  6.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 179:  training loss:  0.25828162\n",
            "epoch 180:  training loss:  0.29005054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 182/240 [00:45<00:10,  5.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 180:  training loss:  0.29005054 NDCG:  0.3440670164999059\n",
            "epoch 181:  training loss:  0.26054817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 184/240 [00:45<00:09,  6.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 182:  training loss:  0.25255573\n",
            "epoch 183:  training loss:  0.264631\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 186/240 [00:45<00:08,  6.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 184:  training loss:  0.25162128\n",
            "epoch 185:  training loss:  0.26256305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 188/240 [00:46<00:07,  6.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 186:  training loss:  0.2738304\n",
            "epoch 187:  training loss:  0.2591705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 190/240 [00:46<00:07,  6.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 188:  training loss:  0.22291541\n",
            "epoch 189:  training loss:  0.23511198\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|███████▉  | 191/240 [00:46<00:09,  5.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 190:  training loss:  0.26912412\n",
            "epoch 190:  training loss:  0.26912412 NDCG:  0.34708713344325204\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 192/240 [00:46<00:08,  5.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 191:  training loss:  0.2548068\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 80%|████████  | 193/240 [00:47<00:09,  4.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 192:  training loss:  0.24220791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████▏ | 195/240 [00:47<00:08,  5.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 193:  training loss:  0.27543673\n",
            "epoch 194:  training loss:  0.25131673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 196/240 [00:47<00:08,  5.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 195:  training loss:  0.25404567\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 82%|████████▏ | 197/240 [00:47<00:09,  4.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 196:  training loss:  0.23448215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 82%|████████▎ | 198/240 [00:48<00:12,  3.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 197:  training loss:  0.21454388\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 199/240 [00:48<00:13,  3.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 198:  training loss:  0.25039837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 83%|████████▎ | 200/240 [00:48<00:11,  3.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 199:  training loss:  0.27580163\n",
            "epoch 200:  training loss:  0.23305058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 201/240 [00:49<00:13,  2.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 200:  training loss:  0.23305058 NDCG:  0.33677719821215873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 84%|████████▍ | 202/240 [00:49<00:13,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 201:  training loss:  0.24144618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 85%|████████▍ | 203/240 [00:50<00:13,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 202:  training loss:  0.20424683\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 85%|████████▌ | 204/240 [00:50<00:11,  3.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 203:  training loss:  0.23504208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 85%|████████▌ | 205/240 [00:50<00:10,  3.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 204:  training loss:  0.20851834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 86%|████████▌ | 206/240 [00:50<00:09,  3.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 205:  training loss:  0.24117525\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 86%|████████▋ | 207/240 [00:51<00:08,  3.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 206:  training loss:  0.25066337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 208/240 [00:51<00:08,  3.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 207:  training loss:  0.21725246\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 87%|████████▋ | 209/240 [00:51<00:07,  4.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 208:  training loss:  0.23174074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 210/240 [00:51<00:07,  4.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 209:  training loss:  0.2118329\n",
            "epoch 210:  training loss:  0.21854992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 211/240 [00:52<00:08,  3.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 210:  training loss:  0.21854992 NDCG:  0.3329187287950759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 88%|████████▊ | 212/240 [00:52<00:07,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 211:  training loss:  0.20048466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 89%|████████▉ | 213/240 [00:52<00:07,  3.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 212:  training loss:  0.20314778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 89%|████████▉ | 214/240 [00:52<00:06,  3.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 213:  training loss:  0.19081813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 216/240 [00:53<00:05,  4.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 214:  training loss:  0.19477747\n",
            "epoch 215:  training loss:  0.24233654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 218/240 [00:53<00:04,  5.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 216:  training loss:  0.225229\n",
            "epoch 217:  training loss:  0.2202788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 220/240 [00:53<00:03,  5.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 218:  training loss:  0.19391556\n",
            "epoch 219:  training loss:  0.20334499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 92%|█████████▏| 221/240 [00:54<00:04,  4.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 220:  training loss:  0.23643221\n",
            "epoch 220:  training loss:  0.23643221 NDCG:  0.34787766766561323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 223/240 [00:54<00:03,  5.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 221:  training loss:  0.20165884\n",
            "epoch 222:  training loss:  0.19661492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 225/240 [00:54<00:02,  5.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 223:  training loss:  0.17188874\n",
            "epoch 224:  training loss:  0.17336611\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▍| 227/240 [00:55<00:02,  6.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 225:  training loss:  0.22687244\n",
            "epoch 226:  training loss:  0.23375416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 229/240 [00:55<00:01,  6.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 227:  training loss:  0.1877829\n",
            "epoch 228:  training loss:  0.1736302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▌| 230/240 [00:55<00:01,  6.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 229:  training loss:  0.19657086\n",
            "epoch 230:  training loss:  0.15822601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 96%|█████████▋| 231/240 [00:56<00:02,  4.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 230:  training loss:  0.15822601 NDCG:  0.3428547115880152\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 233/240 [00:56<00:01,  4.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 231:  training loss:  0.2205112\n",
            "epoch 232:  training loss:  0.2126118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 235/240 [00:56<00:01,  4.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 233:  training loss:  0.17146191\n",
            "epoch 234:  training loss:  0.24441206\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 237/240 [00:57<00:00,  5.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 235:  training loss:  0.18343328\n",
            "epoch 236:  training loss:  0.16697739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 239/240 [00:57<00:00,  6.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 237:  training loss:  0.16849056\n",
            "epoch 238:  training loss:  0.21604227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 240/240 [00:57<00:00,  4.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 239:  training loss:  0.24283147\n"
          ]
        }
      ],
      "source": [
        "def train_base_recommendation():\n",
        "    if gpu:\n",
        "        device = torch.device('cuda:%s' % cuda)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
        "    with open(os.path.join(save_path,dataset + \"_dataset_obj.pickle\"), 'wb') as outp:\n",
        "        pickle.dump(rec_dataset, outp, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    train_loader = DataLoader(dataset=UserItemInterDataset(rec_dataset.training_data, \n",
        "                                rec_dataset.user_feature_matrix, \n",
        "                                rec_dataset.item_feature_matrix),\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "    model = BaseRecModel(rec_dataset.feature_num).to(device)\n",
        "    loss_fn = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    out_path = os.path.join(\"./logs\", dataset + \"_logs\")\n",
        "    Path(out_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ndcg = compute_ndcg(rec_dataset.test_data, \n",
        "            rec_dataset.user_feature_matrix, \n",
        "            rec_dataset.item_feature_matrix, \n",
        "            rec_k, \n",
        "            model, \n",
        "            device)\n",
        "    print('init ndcg:', ndcg)\n",
        "    for epoch in tqdm.trange(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        losses = []\n",
        "        for user_behaviour_feature, item_aspect_feature, label in train_loader:\n",
        "            user_behaviour_feature = user_behaviour_feature.to(device)\n",
        "            item_aspect_feature = item_aspect_feature.to(device)\n",
        "            label = label.float().to(device)\n",
        "            out = model(user_behaviour_feature, item_aspect_feature).squeeze()\n",
        "            loss = loss_fn(out, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            losses.append(loss.to('cpu').detach().numpy())\n",
        "            ave_train = np.mean(np.array(losses))\n",
        "        print('epoch %d: ' % epoch, 'training loss: ', ave_train)\n",
        "        # compute necg\n",
        "        if epoch % 10 == 0:\n",
        "            ndcg = compute_ndcg(rec_dataset.test_data, \n",
        "            rec_dataset.user_feature_matrix, \n",
        "            rec_dataset.item_feature_matrix, \n",
        "            rec_k, \n",
        "            model, \n",
        "            device)\n",
        "            print('epoch %d: ' % epoch, 'training loss: ', ave_train, 'NDCG: ', ndcg)\n",
        "    torch.save(model.state_dict(), os.path.join(save_path, \"model.model\"))\n",
        "    return rec_dataset\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    if gpu:\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] =cuda\n",
        "        print(\"Using CUDA\",cuda)\n",
        "    else:\n",
        "        print(\"Using CPU\")\n",
        "    rec_dataset=train_base_recommendation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ucRyXRT4mzb"
      },
      "outputs": [],
      "source": [
        "dataset='cds_and_vinyl'\n",
        "base_model_path=\"/content/drive/Shareddrives/Unlimited Drive | @LicenseMarket/Recommender/cds/\"\n",
        "gpu=True\n",
        "cuda='0'\n",
        "data_obj_path=\"/content/drive/Shareddrives/Unlimited Drive | @LicenseMarket/Recommender/cds/\"\n",
        "rec_k=5\n",
        "lam=100\n",
        "gam=0.7\n",
        "alp=0.2\n",
        "user_mask=False\n",
        "lr=0.01\n",
        "step=500\n",
        "mask_thresh=0.1\n",
        "test_num=-1\n",
        "# save_path=\"./explanation_objs/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snwXLWLePEmC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC_P_74iOR-T"
      },
      "source": [
        "# User Perspective Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j03L6H5PA4bK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o93FnpHX60cX",
        "outputId": "2eae548d-00f4-48ba-d91e-07c48bbb07a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def lemmatization(text):\n",
        "    result=[]\n",
        "    wordnet = WordNetLemmatizer()\n",
        "    for token,tag in pos_tag(text):\n",
        "        pos=tag[0].lower()\n",
        "        if pos not in ['a', 'r', 'n', 'v']:\n",
        "            pos='n'\n",
        "        # if pos in ['n','a']:   \n",
        "        #   result.append(wordnet.lemmatize(token,pos))\n",
        "    return result\n",
        "def remove_stopwords(text):\n",
        "    en_stopwords = stopwords.words('english')\n",
        "    en_stopwords+=['may','could','that','without','iii','with','and','This','That','Those','These','the','The','brbr','so','it','such']\n",
        "    result = []\n",
        "    for token in text:\n",
        "        if token not in en_stopwords:\n",
        "            result.append(token)\n",
        "            \n",
        "    return result\n",
        "def remove_punct(text):\n",
        "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "    lst=tokenizer.tokenize(' '.join(text))\n",
        "    return lst\n",
        "\n",
        "def remove_tag(text):\n",
        "    text=' '.join(text)\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def preprocess(text):\n",
        "  chars=['&','%','#','@','^','>','<','\\n','\\\\','\\t',';','\"','/']\n",
        "  stwords=stopwords.words('english')\n",
        "  for ch in chars:\n",
        "    text=text.replace(ch,' ')\n",
        "  text=\" \".join(text.split())\n",
        "  # text=text.lower()\n",
        "  text_tokenized=word_tokenize(text)\n",
        "  cleaned_text= remove_stopwords(text_tokenized)\n",
        "  cleaned_text= remove_punct(cleaned_text)\n",
        "  # cleaned_text=lemmatization(cleaned_text)\n",
        "  cleaned_text=remove_tag(cleaned_text)\n",
        "  cleaned_text=remove_urls(cleaned_text)\n",
        "  cleaned_text=''.join([i for i in cleaned_text ])\n",
        "  cleaned_text=[word for word in cleaned_text.split(' ') if len(word)>1]\n",
        "  # print(cleaned_text)\n",
        "  return ' '.join(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtlEHpkN-lCe"
      },
      "outputs": [],
      "source": [
        "def preprocess_text_first(text):\n",
        "  list_to_replace=['br','mso','gte','xml','false','!','-','\\'','\\\"','[',']','/','\\\\n','\\\\','span','a-size-base','a-color-secondary','input type','header name','value','=','<a href= javascript:void(0) class= ','{','}','class=','header','<a href= javascript:void(0)','<','>','href',')','(',';','quot','&',':','javascript']\n",
        "  for char in list_to_replace:\n",
        "    text=text.replace(char,' ')\n",
        "  for i in range(15):\n",
        "    text=text.replace('  ',' ')\n",
        "  new_text=''\n",
        "  for word in text.split(' '):\n",
        "    if len(word)>1 and len(word)<35:\n",
        "      new_text+=word+' '\n",
        "  return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQRrvFS4cifS",
        "outputId": "ca0e7bca-8c2e-467f-e1bd-9ed56cb066ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "5000\n",
            "10000\n",
            "15000\n",
            "20000\n",
            "25000\n",
            "30000\n",
            "35000\n",
            "40000\n",
            "45000\n",
            "50000\n",
            "55000\n",
            "60000\n",
            "65000\n",
            "70000\n",
            "75000\n",
            "80000\n",
            "85000\n",
            "90000\n",
            "95000\n",
            "100000\n",
            "105000\n",
            "110000\n",
            "115000\n",
            "120000\n",
            "125000\n",
            "130000\n",
            "135000\n",
            "140000\n",
            "145000\n",
            "150000\n",
            "155000\n",
            "160000\n",
            "165000\n",
            "170000\n",
            "175000\n",
            "180000\n",
            "185000\n",
            "190000\n",
            "195000\n",
            "200000\n",
            "205000\n",
            "210000\n",
            "215000\n",
            "220000\n",
            "225000\n",
            "230000\n",
            "235000\n",
            "240000\n",
            "245000\n",
            "250000\n",
            "255000\n",
            "260000\n",
            "265000\n"
          ]
        }
      ],
      "source": [
        "items_list=[]\n",
        "users_list=[]\n",
        "review_features={}\n",
        "f=open(save_path+'CDs_and_Vinyl')\n",
        "lines=f.readlines()\n",
        "i=0\n",
        "for line in lines:\n",
        "  if i%5000==0:\n",
        "    print(i)\n",
        "  i+=1\n",
        "  user_id = line.split('@')[0]\n",
        "  item_id = line.split('@')[1]\n",
        "  # if item_id in df_meta['asin'].values:\n",
        "  users_list.append(user_id)\n",
        "  items_list.append(item_id)\n",
        "  l = len(user_id) + len(item_id)\n",
        "  fosr_data = line[l+3:]\n",
        "  for seg in fosr_data.split('||'):\n",
        "    if (user_id,item_id) not in review_features.keys():\n",
        "      review_features[(user_id,item_id)]=[]\n",
        "    fos = seg.split(':')[0].strip('|')\n",
        "    if len(fos.split('|')) > 1:\n",
        "          feature = fos.split('|')[0]\n",
        "          opinion = fos.split('|')[1]\n",
        "          sentiment = fos.split('|')[2]\n",
        "          sentence= seg.split(':')[1].lower()\n",
        "          if sentiment=='+1':\n",
        "            senti=1\n",
        "          else:\n",
        "            senti=-1\n",
        "          review_features[(user_id,item_id)].append([feature,opinion,senti,sentence])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqg-cDlQFqIx",
        "outputId": "996ad229-4f92-462a-ac71-159a3f698837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "5000\n",
            "10000\n",
            "15000\n",
            "20000\n",
            "25000\n",
            "30000\n",
            "35000\n",
            "40000\n",
            "45000\n",
            "50000\n",
            "55000\n",
            "60000\n",
            "65000\n",
            "70000\n",
            "75000\n",
            "80000\n",
            "85000\n",
            "90000\n",
            "95000\n",
            "100000\n",
            "105000\n",
            "110000\n",
            "115000\n",
            "120000\n",
            "125000\n",
            "130000\n",
            "135000\n",
            "140000\n",
            "145000\n",
            "150000\n",
            "155000\n",
            "160000\n",
            "165000\n",
            "170000\n",
            "175000\n",
            "180000\n",
            "185000\n",
            "190000\n",
            "195000\n",
            "200000\n",
            "205000\n",
            "210000\n",
            "215000\n",
            "220000\n",
            "225000\n",
            "230000\n",
            "235000\n",
            "240000\n",
            "245000\n",
            "250000\n",
            "255000\n"
          ]
        }
      ],
      "source": [
        "user_test_perspective={}\n",
        "i=0\n",
        "for (user_id , item_id) in review_features.keys():\n",
        "  review_feature=review_features[(user_id,item_id)]\n",
        "  if i%5000==0:\n",
        "    print(i)\n",
        "  i+=1\n",
        "  for features in review_feature:\n",
        "    sentence=features[3]\n",
        "    sentence=preprocess_text_first(sentence)\n",
        "    sentence=preprocess(sentence).lower()\n",
        "    if (user_id , item_id) not in user_test_perspective.keys():\n",
        "      final_vect=[]\n",
        "    final_vect+=sentence.split(' ')\n",
        "    for word in sentence.split(' '):\n",
        "      # tokens=list(set(df_words[df_words['word']==word]['tokenized'].values))\n",
        "      # for token in tokens:\n",
        "        final_vect+=word.split(' ')\n",
        "    final_vect=list(set(final_vect))\n",
        "    new_final_vect=[]\n",
        "    for word in final_vect:\n",
        "      if len(word)>1:\n",
        "        new_final_vect.append(word)\n",
        "    user_test_perspective[(user_id , item_id)]=new_final_vect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W254L5obCgm6"
      },
      "outputs": [],
      "source": [
        "user_test_perspective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cuVEpuvBde8T",
        "outputId": "f87a9d13-36eb-4bf7-afed-9aeabdd894c1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'B00FL6ADNC'"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rec_dataset.inv_item_name_dict[622]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdHtY0uTOppf"
      },
      "source": [
        "# Train Explaination Generator Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6o-et3EvqQ9"
      },
      "outputs": [],
      "source": [
        "def get_features(item_id):\n",
        "  item=rec_dataset.item_name_dict[item_id]\n",
        "  item_features=rec_dataset.item_features[item]\n",
        "  len_features=0\n",
        "  features=[]\n",
        "  th1=len(item_features[0])\n",
        "  # th2=len(item_features[0])+len(item_features[1])\n",
        "  for i in range(2):\n",
        "    len_features+=len(item_features[i])\n",
        "    features+=item_features[i]\n",
        "  return features,th1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6_hIQu5sUrW"
      },
      "outputs": [],
      "source": [
        "def get_average_vect_test(weights,tensor_vect):\n",
        "  weights=torch.FloatTensor(weights).to(device)\n",
        "  # print()\n",
        "  weights=torch.transpose(weights.repeat(tensor_vect.shape[1],1),0,1)\n",
        "  average=torch.mean(weights*tensor_vect,axis=0).to(device)\n",
        "  return average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjd28mVPv-k3"
      },
      "outputs": [],
      "source": [
        "def get_new_item_vector(item_id,th1,new_weights,tensor_vect_desc,tensor_vect_title):\n",
        "    # item_id= rec_dataset.inv_item_name_dict[item]\n",
        "    final_vector=[]\n",
        "    average_vect_desc=get_average_vect_test(new_weights[0:th1],tensor_vect_desc)\n",
        "    average_vect_title=get_average_vect_test(new_weights[th1:],tensor_vect_title)\n",
        "\n",
        "    final_vector=torch.cat((average_vect_desc,average_vect_title))\n",
        "      # final_vect=np.array(final_vector, dtype='float32')\n",
        "    return final_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7U71c2bg8TD"
      },
      "outputs": [],
      "source": [
        "def get_random_chromosome(features):\n",
        "  # print(features)\n",
        "  ch = np.random.choice([0, 1], size=len(features),p=[0.1,0.9])\n",
        "  # for i in range(10):\n",
        "  #   rand_num=np.random.randint(0,len(ch))\n",
        "  #   ch[rand_num]=0\n",
        "  # ch=np.ones(len(features))\n",
        "  return ch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZMygTmhirAs"
      },
      "outputs": [],
      "source": [
        "def get_first_population(num_population,features):\n",
        "  pops=[]\n",
        "  for i in range(num_population):\n",
        "    ch=get_random_chromosome(features)\n",
        "    pops.append(ch)\n",
        "  return pops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whbHS48QBuuY"
      },
      "outputs": [],
      "source": [
        "def select_chrs(dict_chooses,num_population,population):\n",
        "  new_population=[]\n",
        "  while len(new_population)<num_population:\n",
        "    rand_num=np.random.rand()\n",
        "    for key in dict_chooses.keys():\n",
        "      if rand_num>=key[0] and rand_num<=key[1]:\n",
        "        new_population.append(population[dict_chooses[key]])\n",
        "  return new_population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqmtIHmjEZus"
      },
      "outputs": [],
      "source": [
        "alpha=1\n",
        "lam=10\n",
        "beta=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Sdgstvkj3Ne"
      },
      "outputs": [],
      "source": [
        "def calc_fitnesses(margin_score,population,th1,user_id,item_id,base_model,tensor_vect_desc,tensor_vect_title):\n",
        "  best_fit=-10000.0\n",
        "  best_ans=[]\n",
        "  dict_fitnesses={}\n",
        "  # print(out_last)\n",
        "  count=0\n",
        "  # print(len(population))\n",
        "  user=rec_dataset.user_name_dict[user_id]\n",
        "  user_vect=torch.from_numpy(rec_dataset.user_feature_matrix[user]).to(device)\n",
        "  count_best=0\n",
        "  for chr in population:\n",
        "    item_feature_star=get_new_item_vector(item_id,th1,chr,tensor_vect_desc,tensor_vect_title)\n",
        "    score = base_model(user_vect.unsqueeze(0), item_feature_star.unsqueeze(0))\n",
        "    count_features_rate=np.count_nonzero(chr)/len(chr)\n",
        "    beta=10\n",
        "    if score.cpu()-margin_score.cpu()>0:\n",
        "      count_features_rate=(len(chr)-np.count_nonzero(chr))/len(chr)\n",
        "      beta=0.5\n",
        "      \n",
        "    # else:\n",
        "    #   alpha=100.0\n",
        "    #   beta=1.0\n",
        "    # print(margin_score)\n",
        "    # print(score)\n",
        "    fitness=1.0/(lam*(((alpha+score.cpu()-margin_score.cpu()))))+(beta*((count_features_rate)))\n",
        "    # if out_new<0.5:\n",
        "    #   for rate in [0.8,0.7,0.6,0.5,0.4,0.3]:\n",
        "    #     if (np.count_nonzero(chr)/len(chr))<rate:\n",
        "    #       fitness-=(1.0-rate)\n",
        "    #       break\n",
        "    if fitness>best_fit:\n",
        "      best_fit=fitness\n",
        "      best_ans=chr.copy()\n",
        "      out_best_last=margin_score\n",
        "      out_best_new=score\n",
        "      count_best=len(chr)-np.count_nonzero(chr)\n",
        "    # print(fitness,np.log((out_last-out_new)+1.0),((np.count_nonzero(chr)/len(chr))))\n",
        "    dict_fitnesses[count]=fitness.item()\n",
        "    count+=1\n",
        "  # print(len(dict_fitnesses))\n",
        "  # print('score: ',out_best_new)\n",
        "  # print(len(best_ans)-np.count_nonzero(best_ans),len(best_ans))\n",
        "  # print('fitness: ',best_fit)\n",
        "  # print('----------')\n",
        "  return dict_fitnesses,best_fit,best_ans,out_best_new,count_best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsNN2v6YD4d1"
      },
      "outputs": [],
      "source": [
        "def cross_over(th1,new_pop1):\n",
        "  new_pop2=[]\n",
        "  # print(len(new_pop1))\n",
        "  # print()\n",
        "  for i in range(0,len(new_pop1)-1,2):\n",
        "    chr1=new_pop1[i]\n",
        "    chr2=new_pop1[i+1]\n",
        "    rand_num=np.random.rand()\n",
        "    if rand_num>0.01:\n",
        "      new_chr1=list(chr1[0:th1])\n",
        "      new_chr1+=list(chr2[th1:])\n",
        "      # new_chr1+=list(chr1[th2:])\n",
        "\n",
        "      new_chr2=list(chr2[0:th1])\n",
        "      new_chr2+=list(chr1[th1:])\n",
        "      # new_chr2+=list(chr2[th2:])\n",
        "\n",
        "      new_pop2.append(new_chr1)\n",
        "      new_pop2.append(new_chr2)\n",
        "    else:\n",
        "      new_pop2.append(chr1)\n",
        "      new_pop2.append(chr2)\n",
        "  return new_pop2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqyqGX8GInNC"
      },
      "outputs": [],
      "source": [
        "def mutate(new_pop):\n",
        "  new_pop1=new_pop.copy()\n",
        "  # print(len(new_pop1))\n",
        "  # print(len(new_pop))\n",
        "  rand_count=np.random.randint(0,0.5*int(len(new_pop1)))\n",
        "  for i in range(rand_count):\n",
        "    rand_ind=np.random.randint(0,len(new_pop1))\n",
        "    chr1=new_pop1[rand_ind]\n",
        "    rand_count2=np.random.randint(0,0.1*int(len(new_pop1)))\n",
        "    for j in range(rand_count2):\n",
        "      rand_ind=np.random.randint(0,len(chr1))\n",
        "      chr1[rand_ind]=1.0-chr1[rand_ind]\n",
        "  return new_pop1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_P8bHkKiQq0"
      },
      "outputs": [],
      "source": [
        "num_population=200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzO5JyluPyG-"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_perspective(\n",
        "        rec_dict,\n",
        "        u_i_exp_dict,\n",
        "        base_model,\n",
        "        user_feature_matrix,\n",
        "        item_feature_matrix,\n",
        "        rec_k,\n",
        "        device):\n",
        "    \"\"\"\n",
        "    compute PN, PS and F_NS score for the explanations\n",
        "    :param rec_dict: {u1: [i1, i2, i3, ...] , u2: [i1, i2, i3, ...]}\n",
        "    :param u_i_exp_dict: {(u, i): [f1, f2, ...], ...}\n",
        "    :param base_model: the trained base recommendation model\n",
        "    :param user_feature_matrix: |u| x |p| matrix, the attention on each feature p for each user u\n",
        "    :param item_feature_matrix: |i| x |p| matrix, the quality on each feature p for each item i\n",
        "    :param rec_k: the length of the recommendation list, only generated explanations for the items on the list\n",
        "    :param device: the device of the model\n",
        "    :return: the mean of the PN, PS and FNS scores\n",
        "    \"\"\"\n",
        "    pn_count = 0\n",
        "    ps_count = 0\n",
        "    dict_items_cf_feature={}\n",
        "    for u_i, fs in u_i_exp_dict.items():\n",
        "        user = u_i[0]\n",
        "        target_item = u_i[1]\n",
        "        features = set(fs)\n",
        "        items = rec_dict[user]\n",
        "        target_index = items.index(target_item)\n",
        "        # print(len(items))\n",
        "        # print(features)\n",
        "        # compute PN\n",
        "        cf_items_features1 = []\n",
        "        cf_items_features2 = []\n",
        "        for item in items:\n",
        "            # print(item)\n",
        "            # item_ori_feature = np.array(item_feature_matrix[item])\n",
        "            item_id=rec_dataset.inv_item_name_dict[item]\n",
        "            all_features,th1=get_features(item_id)\n",
        "            # print(all_features)\n",
        "            item_feature_name=rec_dataset.item_features[item]\n",
        "            weights1=[0.0 if fea in features else 1.0 for fea in all_features]\n",
        "            weights2=[1.0 if fea in features else 0.0 for fea in all_features]\n",
        "            # weights=torch.FloatTensor(weights).to(device)\n",
        "            if item in dict_items_cf_feature.keys():\n",
        "              tensor_vect_desc,tensor_vect_title=dict_items_cf_feature[item]\n",
        "            else:\n",
        "              tensor_vect_desc,tensor_vect_title=get_tensor_vects(item_id)\n",
        "              dict_items_cf_feature[item]=(tensor_vect_desc,tensor_vect_title)\n",
        "            \n",
        "            item_cf_feature1=get_new_item_vector(item_id,th1,weights1,tensor_vect_desc,tensor_vect_title).detach().to('cpu').numpy()\n",
        "            item_cf_feature2=get_new_item_vector(item_id,th1,weights2,tensor_vect_desc,tensor_vect_title).detach().to('cpu').numpy()\n",
        "            cf_items_features1.append(item_cf_feature1)\n",
        "            cf_items_features2.append(item_cf_feature2)\n",
        "            # print(item_cf_feature)\n",
        "            # print(torch.from_numpy(np.array(cf_items_features,dtype='float32')).to(device))\n",
        "            # print(np.shape(item_cf_feature))\n",
        "            # print(np.shape(cf_items_features))\n",
        "        # print(np.shape(cf_items_features))\n",
        "        cf_ranking_scores1 = base_model(torch.from_numpy(np.array([user_feature_matrix[user]\n",
        "                                                                      for i in range(len(cf_items_features1))])\n",
        "                                                            ).to(device),\n",
        "                                           torch.from_numpy(np.array(cf_items_features1,dtype='float32')).to(device)).squeeze()\n",
        "        cf_score_list1 = cf_ranking_scores1.to('cpu').detach().numpy()\n",
        "        sorted_index1 = np.argsort(cf_score_list1)[::-1]\n",
        "        cf_rank1 = np.argwhere(sorted_index1 == target_index)[0, 0]  # the updated ranking of the current item\n",
        "        if cf_rank1 > rec_k - 1:\n",
        "            pn_count += 1\n",
        "        # compute NS\n",
        "        cf_ranking_scores2 = base_model(torch.from_numpy(np.array([user_feature_matrix[user]\n",
        "                                                                      for i in range(len(cf_items_features2))])\n",
        "                                                            ).to(device),\n",
        "                                           torch.from_numpy(np.array(cf_items_features2,dtype='float32')).to(device)).squeeze()\n",
        "        cf_score_list2 = cf_ranking_scores2.to('cpu').detach().numpy()\n",
        "        sorted_index2 = np.argsort(cf_score_list2)[::-1]\n",
        "        cf_rank2 = np.argwhere(sorted_index2 == target_index)[0, 0]  # the updated ranking of the current item\n",
        "        if cf_rank2 < rec_k:\n",
        "            ps_count += 1\n",
        "    if len(u_i_exp_dict) != 0:\n",
        "        pn = pn_count / len(u_i_exp_dict)\n",
        "        ps = ps_count / len(u_i_exp_dict)\n",
        "        if (pn + ps) != 0:\n",
        "            fns = (2 * pn * ps) / (pn + ps)\n",
        "        else:\n",
        "            fns = 0\n",
        "    else:\n",
        "        pn = 0\n",
        "        ps = 0\n",
        "        fns = 0\n",
        "    return pn, ps, fns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGywTyrWPsrh"
      },
      "outputs": [],
      "source": [
        "def evaluate_user_perspective(user_perspective_data, u_i_expl_dict):\n",
        "    pres = []\n",
        "    recs = []\n",
        "    f1s = []\n",
        "    for u_i, gt_features in user_perspective_data.items():\n",
        "        if u_i in u_i_expl_dict:\n",
        "            TP = 0\n",
        "            pre_features = u_i_expl_dict[u_i]\n",
        "            # print('f: ', gt_features, pre_features)\n",
        "            for feature in pre_features:\n",
        "                if feature in gt_features:\n",
        "                    TP += 1\n",
        "            # print(gt_features)\n",
        "            # print(pre_features)\n",
        "            pre = TP / len(pre_features)\n",
        "            rec = TP / len(gt_features)\n",
        "            if (pre + rec) != 0:\n",
        "                f1 = (2 * pre * rec) / (pre + rec)\n",
        "            else:\n",
        "                f1 = 0\n",
        "            pres.append(pre)\n",
        "            recs.append(rec)\n",
        "            f1s.append(f1)\n",
        "    ave_pre = np.mean(pres)\n",
        "    ave_rec = np.mean(recs)\n",
        "    ave_f1 = np.mean(f1s)\n",
        "    return ave_pre, ave_rec, ave_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlQOyDlT-rj-"
      },
      "outputs": [],
      "source": [
        "class ExpOptimizationModel(torch.nn.Module):\n",
        "    def __init__(self, base_model, rec_dataset, device):\n",
        "        super(ExpOptimizationModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.rec_dataset = rec_dataset\n",
        "        self.device = device\n",
        "        self.u_i_exp_dict = {}  # {(user, item): [f1, f2, f3 ...], ...}\n",
        "        self.user_feature_matrix = torch.from_numpy(self.rec_dataset.user_feature_matrix).to(self.device)\n",
        "        self.item_feature_matrix = torch.from_numpy(self.rec_dataset.item_feature_matrix).to(self.device)\n",
        "        self.rec_dict, self.user_perspective_test_data = self.generate_rec_dict()\n",
        "\n",
        "    def generate_rec_dict(self):\n",
        "        rec_dict = {}\n",
        "        correct_rec_dict = {}  # used for user-side evaluation\n",
        "        for row in self.rec_dataset.test_data:\n",
        "            user = row[0]\n",
        "            items = row[1]\n",
        "            labels = row[2]\n",
        "            correct_rec_dict[user] = []\n",
        "            user_features = self.user_feature_matrix[user].repeat(len(items), 1)\n",
        "            scores = self.base_model(user_features,\n",
        "                        self.item_feature_matrix[items]).squeeze()\n",
        "            scores = np.array(scores.to('cpu'))\n",
        "            sort_index = sorted(range(len(scores)), key=lambda k: scores[k], reverse=True)\n",
        "            sorted_items = [items[i] for i in sort_index]\n",
        "            rec_dict[user] = sorted_items\n",
        "            for i in range(rec_k):  # find the correct items and add to the user side test data\n",
        "                if labels[sort_index[i]] == 1:\n",
        "                    correct_rec_dict[user].append(items[sort_index[i]])\n",
        "\n",
        "        user_perspective_test_data = {}  # {(u, i):f, (u, i): f]}\n",
        "        # for user, items in correct_rec_dict.items():\n",
        "        #     for item in items:\n",
        "        #         user_id=rec_dataset.inv_user_name_dict[user]\n",
        "        #         item_id=rec_dataset.inv_item_name_dict[item]\n",
        "        #         feature = user_test_perspective[(user_id, item_id)]\n",
        "        #         user_perspective_test_data[(user, item)] = feature\n",
        "        return rec_dict, user_perspective_test_data\n",
        "    def user_side_evaluation(self):\n",
        "        ave_pre, ave_rec, ave_f1 = evaluate_user_perspective(self.user_perspective_test_data, self.u_i_exp_dict)\n",
        "        print('user\\'s perspective:')\n",
        "        print('ave pre: ', ave_pre, '  ave rec: ', ave_rec, '  ave f1: ', ave_f1)\n",
        "    \n",
        "    def model_side_evaluation(self):\n",
        "        ave_pn, ave_ps, ave_fns = evaluate_model_perspective(\n",
        "            self.rec_dict,\n",
        "            self.u_i_exp_dict,\n",
        "            self.base_model,\n",
        "            self.rec_dataset.user_feature_matrix,\n",
        "            self.rec_dataset.item_feature_matrix,\n",
        "            rec_k,\n",
        "            self.device)\n",
        "        print('model\\'s perspective:')\n",
        "        print('ave PN: ', ave_pn, '  ave PS: ', ave_ps, '  ave F_{NS}: ', ave_fns)  \n",
        "    def generate_explanation(self):\n",
        "        # u_i_exps_dict = {}  # {(user, item): [f1, f2, f3 ...], ...}\n",
        "        exp_nums = []\n",
        "        exp_complexities = []\n",
        "        self.no_exp_count = 0\n",
        "        # test_num=10\n",
        "        if test_num == -1:\n",
        "            test_num1 = len(list(self.rec_dict.items()))\n",
        "        else:\n",
        "            test_num1 = test_num\n",
        "        count=0\n",
        "        for user, items in tqdm.tqdm(list(self.rec_dict.items())[:20]):\n",
        "            user_id=self.rec_dataset.inv_user_name_dict[user]\n",
        "            count+=1\n",
        "            # if count<200:\n",
        "            #   continue\n",
        "            # if count==3:\n",
        "            #   break\n",
        "            items = self.rec_dict[user]\n",
        "            margin_item = items[rec_k]\n",
        "            margin_score = self.base_model(self.user_feature_matrix[user].unsqueeze(0), \n",
        "                            self.item_feature_matrix[margin_item].unsqueeze(0)).squeeze()\n",
        "            # print('margin_score: ',margin_score)\n",
        "            for item in items[: rec_k]:\n",
        "                item_id=self.rec_dataset.inv_item_name_dict[item]\n",
        "                tensor_vect_desc,tensor_vect_title=get_tensor_vects(item_id)\n",
        "                explanation_features_words, exp_num=self.get_explanation(item_id,margin_score,user_id,tensor_vect_desc,tensor_vect_title)\n",
        "                \n",
        "                if explanation_features_words is None:\n",
        "                    # print('no explanation for user %d and item %d' % (user, item))\n",
        "                    self.no_exp_count += 1\n",
        "                else:\n",
        "                    self.u_i_exp_dict[(user, item)] = explanation_features_words\n",
        "                    # print(explanation_features_words)\n",
        "                    exp_nums.append(exp_num)\n",
        "\n",
        "              \n",
        "        print('ave num: ', np.mean(exp_nums), 'ave complexity: ', np.mean(exp_complexities) , 'no_exp_count: ', self.no_exp_count)\n",
        "        return True\n",
        "    def get_explanation(self,item_id,margin_score,user_id,tensor_vect_desc,tensor_vect_title):\n",
        "      best_answer=[]\n",
        "      best_fit_all=-10000.0\n",
        "      features,th1=get_features(item_id)\n",
        "      # print(features)\n",
        "      population=get_first_population(num_population,features)\n",
        "      # print(len(population))\n",
        "      # print(population)\n",
        "      exit=False\n",
        "      iter=0\n",
        "      while not exit:\n",
        "        fitnesses,best_fit,best_ans,best_score,count_zero= calc_fitnesses(margin_score,population,th1,user_id,item_id,self.base_model,tensor_vect_desc,tensor_vect_title)\n",
        "        # print(population[0])\n",
        "        # print(fitnesses)\n",
        "        # if iter%1==0:\n",
        "        #   print('iter: ', iter, 'best_fit: ',best_fit , 'best_score: ', best_score , 'count_fea: ', count_zero )\n",
        "        iter+=1\n",
        "        if best_fit>best_fit_all:\n",
        "          best_fit_all=best_fit\n",
        "          best_answer=best_ans.copy()\n",
        "        dict_probs={}\n",
        "        for key in fitnesses.keys():\n",
        "          # print(fitnesses[key])\n",
        "          # print(np.sum(list(fitnesses.values())))\n",
        "          dict_probs[key]=fitnesses[key]/np.sum(list(fitnesses.values()))\n",
        "        dict_chooses={}\n",
        "        last_prob=0\n",
        "        sum_prob=0\n",
        "        for key in dict_probs.keys():\n",
        "          prob=dict_probs[key]\n",
        "          dict_chooses[(sum_prob,prob+sum_prob)]=key\n",
        "          sum_prob+=prob\n",
        "        # print(dict_chooses)\n",
        "        new_pop1=select_chrs(dict_chooses,num_population,population)\n",
        "        # print(len(new_pop1))\n",
        "        new_pop2=cross_over(th1,new_pop1)\n",
        "        # print(len(new_pop2))\n",
        "        rand_num=np.random.rand()\n",
        "        if rand_num>0.9:\n",
        "          new_pop3=mutate(new_pop2)\n",
        "          # print(len(new_pop3))\n",
        "        else:\n",
        "          new_pop3=new_pop2.copy()\n",
        "        population=new_pop3.copy()\n",
        "        if best_fit_all.cpu()>=1.0 and iter==10:\n",
        "          exit=True \n",
        "        elif iter==50:\n",
        "          exit=True\n",
        "      if best_fit_all.cpu()>=1.0:\n",
        "        best_features=[features[i] for i in range(len(features)) if best_answer[i]<1]\n",
        "        # print(best_features)\n",
        "        return best_features,len(best_features)\n",
        "      else:\n",
        "        # print('no explanation')\n",
        "        return None,None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX0E2BXY4EBH"
      },
      "outputs": [],
      "source": [
        "def get_tensor_vect_df(df,not_in_columns):\n",
        "  df1=df.copy()\n",
        "  df1['main_word']=np.where(df1['replaced_word'] == df1[not_in_columns],1,0)\n",
        "  df1=df1[df1['main_word']==1]\n",
        "  if df1.empty:\n",
        "    df1=df.copy()\n",
        "    df1['main_word']=np.where(df1['logit']==5.0,1,0)\n",
        "    df1=df1[df1['main_word']==1]\n",
        "  if df1.empty:\n",
        "    df1=df.copy()\n",
        "    df1['main_word']=np.where(df1['logit']==12.0,1,0)\n",
        "    df1=df1[df1['main_word']==1]\n",
        "  lists=df1.loc[:, ~df1.columns.isin([not_in_columns,'replaced_word','logit','main_word'])].values\n",
        "  words=df1[not_in_columns].values\n",
        "  # print(words)\n",
        "  vects=[sub_list[0] for sub_list in lists]\n",
        "  tensor_vect=torch.FloatTensor(vects).to(device)\n",
        "  return tensor_vect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyzyAFFI28Va"
      },
      "outputs": [],
      "source": [
        "def get_tensor_vects(item_id):\n",
        "  tensor_vect_desc=None\n",
        "  tensor_vect_title=None\n",
        "  # print(item_id)\n",
        "  # if os.path.exists(save_path+'descriptions_bert/'+'df_bert_desc_{}.json'.format(item_id)) :\n",
        "  df_vect_desc= pd.read_json(save_path+'descriptions_bert/'+'{}.json'.format(item_id))\n",
        "  # df_vect_desc=df_vect[0]\n",
        "  tensor_vect_desc=get_tensor_vect_df(df_vect_desc,'description_words')\n",
        "\n",
        "  df_vect_title= pd.read_json(save_path+'titles_bert/'+'{}.json'.format(item_id))\n",
        "  # df_vect_title=df_vect[1]\n",
        "  tensor_vect_title=get_tensor_vect_df(df_vect_title,'title_words')\n",
        "  return tensor_vect_desc,tensor_vect_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugapwlxk1MdA"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(data_obj_path, dataset + \"_dataset_obj.pickle\"), 'rb') as inp:\n",
        "        rec_dataset = pickle.load(inp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N492pXTYa208",
        "outputId": "0299e8f2-7eb6-45c2-f42c-bf4f168800c1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6931/6931 [17:57:51<00:00,  9.33s/it]\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  7.328972238629651 ave complexity:  nan no_exp_count:  795\n",
            "user's perspective:\n",
            "ave pre:  0.12181757349902651   ave rec:  0.034752825736500854   ave f1:  0.03847731861005478\n",
            "model's perspective:\n",
            "ave PN:  0.9578854105138807   ave PS:  0.9971647962197283   ave F_{NS}:  0.9771305175561397\n"
          ]
        }
      ],
      "source": [
        "def generate_explanation():\n",
        "    if gpu:\n",
        "        device = torch.device('cuda:%s' %cuda)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    print(device)\n",
        "    # import dataset\n",
        "    with open(os.path.join(data_obj_path, dataset + \"_dataset_obj.pickle\"), 'rb') as inp:\n",
        "        rec_dataset = pickle.load(inp)\n",
        "    base_model = BaseRecModel(rec_dataset.feature_num).to(device)\n",
        "    base_model.load_state_dict(torch.load(os.path.join(base_model_path,\"model.model\"),map_location=torch.device(device)))\n",
        "    base_model.eval()\n",
        "    #  fix the rec model\n",
        "    for param in base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Create optimization model\n",
        "    opt_model = ExpOptimizationModel(\n",
        "        base_model=base_model,\n",
        "        rec_dataset=rec_dataset,\n",
        "        device = device,\n",
        "        \n",
        "    )\n",
        "\n",
        "    opt_model.generate_explanation()\n",
        "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
        "    with open(os.path.join(save_path, dataset + \"_explanation_obj_genetic.pickle\"), 'wb') as outp:\n",
        "        pickle.dump(opt_model, outp, pickle.HIGHEST_PROTOCOL)\n",
        "    # with open(os.path.join(save_path, dataset + \"_explanation_obj.pickle\"), 'rb') as opt:\n",
        "    #     opt_model = pickle.load(opt)\n",
        "    opt_model.user_side_evaluation()\n",
        "    opt_model.model_side_evaluation()\n",
        "    # print(opt_model.u_i_exp_dict)\n",
        "    # Path(save_path).mkdir(parents=True, exist_ok=True)\n",
        "    # with open(os.path.join(save_path, dataset + \"_explanation_obj.pickle\"), 'wb') as outp:\n",
        "    #     pickle.dump(opt_model, outp, pickle.HIGHEST_PROTOCOL)\n",
        "    return opt_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt_model=generate_explanation()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# calculate Stability..."
      ],
      "metadata": {
        "id": "oLGTBgJwU0cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_explanation_check_stability():\n",
        "    if gpu:\n",
        "        device = torch.device('cuda:%s' %cuda)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    print(device)\n",
        "    # import dataset\n",
        "    with open(os.path.join(data_obj_path, dataset + \"_dataset_obj_2.pickle\"), 'rb') as inp:\n",
        "        rec_dataset = pickle.load(inp)\n",
        "    \n",
        "    base_model = BaseRecModel(rec_dataset.feature_num).to(device)\n",
        "    base_model.load_state_dict(torch.load(os.path.join(base_model_path,\"model2.model\"),map_location=torch.device(device)))\n",
        "    base_model.eval()\n",
        "    #  fix the rec model\n",
        "    for param in base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Create optimization model\n",
        "    features_found=[]\n",
        "    for i in range(10):\n",
        "      opt_model = ExpOptimizationModel(\n",
        "        base_model=base_model,\n",
        "        rec_dataset=rec_dataset,\n",
        "        device = device,)\n",
        "      opt_model.generate_explanation()\n",
        "      features_found.append(opt_model.u_i_exp_dict)\n",
        "      \n",
        "    \n",
        "    return features_found\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    features_found=generate_explanation_check_stability()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw3vT7lYtMMH",
        "outputId": "d3752db6-bfc8-47a0-8aa5-616a98c08d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [05:47<00:00, 17.38s/it]\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  7.123711340206185 ave complexity:  nan no_exp_count:  3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:56<00:00,  8.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  6.428571428571429 ave complexity:  nan no_exp_count:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:36<00:00,  7.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  6.707070707070707 ave complexity:  nan no_exp_count:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:52<00:00,  8.64s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  5.3125 ave complexity:  nan no_exp_count:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:56<00:00,  8.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  5.770833333333333 ave complexity:  nan no_exp_count:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:37<00:00,  7.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  7.171717171717172 ave complexity:  nan no_exp_count:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:47<00:00,  8.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  6.083333333333333 ave complexity:  nan no_exp_count:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:50<00:00,  8.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  6.26530612244898 ave complexity:  nan no_exp_count:  2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:56<00:00,  8.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  5.90625 ave complexity:  nan no_exp_count:  4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [02:37<00:00,  7.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ave num:  7.393939393939394 ave complexity:  nan no_exp_count:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(data_obj_path, dataset + \"_dataset_obj_2.pickle\"), 'rb') as inp:\n",
        "     rec_dataset = pickle.load(inp)"
      ],
      "metadata": {
        "id": "q1ERuD_80-ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_features={}\n",
        "for iter_feas in features_found:\n",
        "  for u_i in iter_feas.keys():\n",
        "    if u_i in dict_features.keys():\n",
        "      dict_features[u_i].append(iter_feas[u_i])\n",
        "    else:\n",
        "      dict_features[u_i]=[]\n",
        "      dict_features[u_i].append(iter_feas[u_i])"
      ],
      "metadata": {
        "id": "nHcOCida1oXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_features"
      ],
      "metadata": {
        "id": "3J-zJC-yNKVh",
        "outputId": "c5340f2f-42c5-4828-a07e-d52d44da0559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 11246): [['holiday', 'wishes'],\n",
              "  ['holiday', 'wishes'],\n",
              "  ['holiday', 'wishes'],\n",
              "  ['holiday', 'wishes'],\n",
              "  ['holiday', 'wishes'],\n",
              "  ['holiday', 'wishes'],\n",
              "  ['holiday', 'wishes'],\n",
              "  ['wishes', 'holiday', 'wishes'],\n",
              "  ['holiday', 'wishes'],\n",
              "  ['holiday', 'wishes']],\n",
              " (0,\n",
              "  9121): [['cappella',\n",
              "   'christmas',\n",
              "   'lsquo',\n",
              "   'appropriately',\n",
              "   'holiday',\n",
              "   'holiday',\n",
              "   'alongside',\n",
              "   'holiday',\n",
              "   'coming',\n",
              "   'chaser',\n",
              "   'snow',\n",
              "   'christmas',\n",
              "   'cheers'], ['album',\n",
              "   'new',\n",
              "   'world',\n",
              "   'season',\n",
              "   'studio',\n",
              "   'sticking',\n",
              "   'classic',\n",
              "   'list',\n",
              "   'donde',\n",
              "   'gentlemen',\n",
              "   'bells',\n",
              "   'christmas',\n",
              "   'cheers'], ['guys',\n",
              "   'cities',\n",
              "   'holiday',\n",
              "   'today',\n",
              "   'christmas',\n",
              "   'cheers',\n",
              "   'comedic',\n",
              "   'time',\n",
              "   'another',\n",
              "   'christmastime',\n",
              "   'snow',\n",
              "   'rest',\n",
              "   'christmas',\n",
              "   'cheers'], ['2007',\n",
              "   'together',\n",
              "   'christmas',\n",
              "   'lsquo',\n",
              "   '110',\n",
              "   'formula',\n",
              "   'cheer',\n",
              "   'track',\n",
              "   'claus',\n",
              "   'nosed',\n",
              "   'mean',\n",
              "   'christmas',\n",
              "   'cheers'], ['2007',\n",
              "   'cities',\n",
              "   'days',\n",
              "   'indiana',\n",
              "   'copies',\n",
              "   'music',\n",
              "   'cheers',\n",
              "   'holiday',\n",
              "   'release',\n",
              "   '24th',\n",
              "   'holiday',\n",
              "   'town',\n",
              "   'christmas',\n",
              "   'cheers'], ['110',\n",
              "   'sensation',\n",
              "   'friends',\n",
              "   'abc',\n",
              "   'holiday',\n",
              "   'recording',\n",
              "   'holiday',\n",
              "   'guys',\n",
              "   'nationwide',\n",
              "   'yet',\n",
              "   'get',\n",
              "   'snow',\n",
              "   'christmas',\n",
              "   'cheers'], ['members',\n",
              "   'youtube',\n",
              "   'album',\n",
              "   'sell',\n",
              "   'lsquo',\n",
              "   'straight',\n",
              "   'collection',\n",
              "   'harmonies',\n",
              "   'barry',\n",
              "   'yet',\n",
              "   'list',\n",
              "   'holy',\n",
              "   'christmas',\n",
              "   'cheers'], ['1998',\n",
              "   'former',\n",
              "   'asked',\n",
              "   'show',\n",
              "   'sticking',\n",
              "   'christmas',\n",
              "   'nationwide',\n",
              "   'performing',\n",
              "   'barry',\n",
              "   'hollywood',\n",
              "   'october',\n",
              "   'back',\n",
              "   'jingle',\n",
              "   'christmas',\n",
              "   'christmas',\n",
              "   'cheers'], ['holiday',\n",
              "   'sell',\n",
              "   'media',\n",
              "   'sensation',\n",
              "   'chaser',\n",
              "   'alongside',\n",
              "   'get',\n",
              "   'manilow',\n",
              "   'let',\n",
              "   'santa',\n",
              "   'merry',\n",
              "   'christmas',\n",
              "   'cheers'], ['days',\n",
              "   '1998',\n",
              "   'lsquo',\n",
              "   'media',\n",
              "   'straight',\n",
              "   'classic',\n",
              "   'songs',\n",
              "   'release',\n",
              "   'snow',\n",
              "   'santa',\n",
              "   'red',\n",
              "   'night',\n",
              "   'days',\n",
              "   'christmas',\n",
              "   'cheers']],\n",
              " (0, 11558): [['global', 'woman', 'blu', 'irish', 'cultural', 'destiny'],\n",
              "  ['look', 'destiny'],\n",
              "  ['high', 'repertoire', 'destiny'],\n",
              "  ['tradition', 'bagpipers', 'experience', 'traditional', 'destiny'],\n",
              "  ['album', 'tradition', 'including', 'destiny'],\n",
              "  ['bring', 'destiny'],\n",
              "  ['traditional', 'destiny'],\n",
              "  ['brand', 'destiny'],\n",
              "  ['repertoire', 'including', 'destiny'],\n",
              "  ['brilliant', 'contemporary', 'style', 'destiny']],\n",
              " (0, 8955): [['digital', 'coming', 'spirits'],\n",
              "  ['human', 'spirits'],\n",
              "  ['ago', 'spirits'],\n",
              "  ['spirits'],\n",
              "  ['almost', 'spirits'],\n",
              "  ['sense', 'spirits'],\n",
              "  ['much', 'spirits'],\n",
              "  ['chaser', 'unadulterated', 'spirits'],\n",
              "  ['video', 'spirits'],\n",
              "  ['sound', 'spirits']],\n",
              " (0, 7365): [['result', 'diva'],\n",
              "  ['performance', 'diva'],\n",
              "  ['original', 'career', 'diva'],\n",
              "  ['boulevard', 'remastered'],\n",
              "  ['tour', 'glenn'],\n",
              "  ['career', 'silent'],\n",
              "  ['lloyd', 'glenn'],\n",
              "  ['lloyd', 'glenn'],\n",
              "  ['lloyd', 'diva'],\n",
              "  ['glenn', 'career']],\n",
              " (3, 2780): [['print', 'rare', 'greatest'],\n",
              "  ['greatest', 'hits'],\n",
              "  ['greatest', 'hits'],\n",
              "  ['greatest', 'hits'],\n",
              "  ['extremely', 'rare', 'greatest'],\n",
              "  ['greatest', 'hits'],\n",
              "  ['greatest', 'hits'],\n",
              "  ['greatest', 'hits'],\n",
              "  ['greatest', 'hits'],\n",
              "  ['thailand', 'greatest', 'hits']],\n",
              " (3, 0): [['bar', 'christmas', 'eve', 'christmas'],\n",
              "  ['prog', 'electric', 'driving', 'christmas'],\n",
              "  ['masterful', 'children', 'driving', 'christmas'],\n",
              "  ['orchestra', 'high', 'christmas'],\n",
              "  ['high', 'eve'],\n",
              "  ['snow', 'children', 'choir', 'christmas'],\n",
              "  ['plus', 'plenty', 'bombastic', 'christmas'],\n",
              "  ['angel', 'typical', 'concept', 'christmas'],\n",
              "  ['stories', 'sentimental', 'christmas'],\n",
              "  ['eve', 'keyboards', 'christmas', 'solos', 'christmas']],\n",
              " (3, 614): [['bruce', 'nowadays', 'imagery', 'bad', 'oddly', 'test', 'new'],\n",
              "  ['new', 'spaghetti', 'oddly', 'new'],\n",
              "  ['jersey', 'jovi', 'humorous', 'new'],\n",
              "  ['bruce', 'stick', 'stadium', 'still', 'new'],\n",
              "  ['wind', 'like', 'medicine', 'song', 'new'],\n",
              "  ['hooky', 'grew', 'wind', 'lay', 'new'],\n",
              "  ['oddly', 'jovi', 'sale', 'new'],\n",
              "  ['jovi', 'dated', 'new'],\n",
              "  ['sin', 'songs', 'genevieve', 'new'],\n",
              "  ['full', 'catchy', 'evident', 'new']],\n",
              " (3, 4165): [['serpentine', 'rockers', 'greatest', 'hits'],\n",
              "  ['fire', 'positivity', 'greatest', 'hits'],\n",
              "  ['song', 'chart', 'syncopated', 'greatest', 'hits'],\n",
              "  ['pop', 'rockers', 'jam', 'greatest', 'hits'],\n",
              "  ['hits',\n",
              "   'get',\n",
              "   'star',\n",
              "   'got',\n",
              "   'sound',\n",
              "   'ever',\n",
              "   'disc',\n",
              "   'greatest',\n",
              "   'funk',\n",
              "   'serpentine',\n",
              "   'getaway',\n",
              "   'remastering',\n",
              "   'exponents',\n",
              "   'shining',\n",
              "   'reminder',\n",
              "   'special',\n",
              "   'hits'],\n",
              "  ['shining', 'major', 'exponents', 'greatest', 'hits'],\n",
              "  ['song', 'collection', 'star', 'greatest', 'hits'],\n",
              "  ['life', 'greatest', 'hits'],\n",
              "  ['remastering', 'shining', 'rickey', 'greatest', 'hits'],\n",
              "  ['september', 'love', 'greatest', 'hits']],\n",
              " (3, 9824): [['collection'],\n",
              "  ['sting'],\n",
              "  ['definitive'],\n",
              "  ['unreleased'],\n",
              "  ['remastered'],\n",
              "  ['definitive'],\n",
              "  ['music'],\n",
              "  ['definitive'],\n",
              "  ['music'],\n",
              "  ['definitive']],\n",
              " (4, 2177): [['dream', 'dream', 'summertime'],\n",
              "  ['dream', 'dream', 'summertime'],\n",
              "  ['dream', 'dream', 'summertime'],\n",
              "  ['dream', 'dream', 'summertime'],\n",
              "  ['gordon', 'dream', 'summertime', 'summertime'],\n",
              "  ['dream', 'dream', 'summertime'],\n",
              "  ['dream', 'dream', 'summertime'],\n",
              "  ['dream', 'dream', 'summertime'],\n",
              "  ['dream', 'dream', 'summertime'],\n",
              "  ['dream', 'dream', 'summertime']],\n",
              " (4,\n",
              "  7310): [['fourth',\n",
              "   'franchise',\n",
              "   'features',\n",
              "   'jonny',\n",
              "   'steve',\n",
              "   'movie',\n",
              "   'far',\n",
              "   'theme',\n",
              "   'mode',\n",
              "   'voldemort',\n",
              "   'blacker',\n",
              "   'overall',\n",
              "   'acquits',\n",
              "   'cup',\n",
              "   'ominous',\n",
              "   'cup',\n",
              "   'frantic',\n",
              "   'sisters',\n",
              "   'hogwarts',\n",
              "   'led',\n",
              "   'frontman',\n",
              "   'somehow',\n",
              "   'across',\n",
              "   'like',\n",
              "   'everybody',\n",
              "   'elisabeth',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'goblet'], ['musicians',\n",
              "   'also',\n",
              "   'first',\n",
              "   'away',\n",
              "   'acquits',\n",
              "   'ends',\n",
              "   'songs',\n",
              "   'led',\n",
              "   'pulp',\n",
              "   'idol',\n",
              "   'songs',\n",
              "   'night',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'goblet'], ['office',\n",
              "   'nominated',\n",
              "   'phil',\n",
              "   'buckle',\n",
              "   'cocker',\n",
              "   'theme',\n",
              "   'concludes',\n",
              "   'two',\n",
              "   'obvious',\n",
              "   'taste',\n",
              "   'vincentelli',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'goblet'], ['fire',\n",
              "   'doyle',\n",
              "   'performed',\n",
              "   'greenwood',\n",
              "   'news',\n",
              "   'harry',\n",
              "   'feels',\n",
              "   'far',\n",
              "   'black',\n",
              "   'horns',\n",
              "   'martial',\n",
              "   'war',\n",
              "   'quidditch',\n",
              "   'note',\n",
              "   'includes',\n",
              "   'across',\n",
              "   'white',\n",
              "   'still',\n",
              "   'tracks',\n",
              "   'everybody',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'goblet'], ['successful',\n",
              "   'goblet',\n",
              "   'composer',\n",
              "   'three',\n",
              "   'also',\n",
              "   'musical',\n",
              "   'away',\n",
              "   'voldemort',\n",
              "   'light',\n",
              "   'jarringly',\n",
              "   'grungy',\n",
              "   'better',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'goblet'], ['soundtrack',\n",
              "   'potter',\n",
              "   'nearly',\n",
              "   'goblet',\n",
              "   'academy',\n",
              "   'written',\n",
              "   'performed',\n",
              "   'jason',\n",
              "   'also',\n",
              "   'appearing',\n",
              "   'big',\n",
              "   'john',\n",
              "   'williams',\n",
              "   'patrick',\n",
              "   'feels',\n",
              "   'far',\n",
              "   'away',\n",
              "   'main',\n",
              "   'voldemort',\n",
              "   'doyle',\n",
              "   'acquits',\n",
              "   'world',\n",
              "   'cup',\n",
              "   'starts',\n",
              "   'mostly',\n",
              "   'world',\n",
              "   'ends',\n",
              "   'war',\n",
              "   'aforementioned',\n",
              "   'weird',\n",
              "   'project',\n",
              "   'relaxed',\n",
              "   'cocker',\n",
              "   'fast',\n",
              "   'somehow',\n",
              "   'two',\n",
              "   'night',\n",
              "   'less',\n",
              "   'contrast',\n",
              "   'elisabeth',\n",
              "   'vincentelli',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'fire'], ['potter',\n",
              "   'fire',\n",
              "   'soundtrack',\n",
              "   'potter',\n",
              "   'franchise',\n",
              "   'goblet',\n",
              "   'academy',\n",
              "   'composer',\n",
              "   'performed',\n",
              "   'musicians',\n",
              "   'musical',\n",
              "   'john',\n",
              "   'feels',\n",
              "   'away',\n",
              "   'pops',\n",
              "   'crashes',\n",
              "   'finish',\n",
              "   'mode',\n",
              "   'tumultuous',\n",
              "   'well',\n",
              "   'acquits',\n",
              "   'chants',\n",
              "   'jig',\n",
              "   'different',\n",
              "   'note',\n",
              "   'songs',\n",
              "   'weird',\n",
              "   'sisters',\n",
              "   'performs',\n",
              "   'ball',\n",
              "   'jarringly',\n",
              "   'led',\n",
              "   'across',\n",
              "   'like',\n",
              "   'idol',\n",
              "   'dancing',\n",
              "   'less',\n",
              "   'much',\n",
              "   'still',\n",
              "   'score',\n",
              "   'potter',\n",
              "   'fire',\n",
              "   'goblet'], ['fourth',\n",
              "   'potter',\n",
              "   'franchise',\n",
              "   'score',\n",
              "   'composer',\n",
              "   'jarvis',\n",
              "   'also',\n",
              "   'movie',\n",
              "   'cocker',\n",
              "   'installments',\n",
              "   'never',\n",
              "   'main',\n",
              "   'theme',\n",
              "   'illustrates',\n",
              "   'wizard',\n",
              "   'williams',\n",
              "   'mode',\n",
              "   'mostly',\n",
              "   'ends',\n",
              "   'martial',\n",
              "   'war',\n",
              "   'aforementioned',\n",
              "   'quidditch',\n",
              "   'jig',\n",
              "   'concludes',\n",
              "   'note',\n",
              "   'sisters',\n",
              "   'group',\n",
              "   'jarvis',\n",
              "   'includes',\n",
              "   'cocker',\n",
              "   'fast',\n",
              "   'somehow',\n",
              "   'across',\n",
              "   'two',\n",
              "   'less',\n",
              "   'tracks',\n",
              "   'instrumental',\n",
              "   'precedes',\n",
              "   'harry',\n",
              "   'potter'], ['successful',\n",
              "   'box',\n",
              "   'news',\n",
              "   'musical',\n",
              "   'shrieking',\n",
              "   'quidditch',\n",
              "   'like',\n",
              "   'billy',\n",
              "   'still',\n",
              "   'vincentelli',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'goblet']],\n",
              " (4, 2425): [['nine'],\n",
              "  ['nine'],\n",
              "  ['nine'],\n",
              "  ['nine'],\n",
              "  ['nine'],\n",
              "  ['nine'],\n",
              "  ['full', 'nine'],\n",
              "  ['nine'],\n",
              "  ['nine'],\n",
              "  ['starr', 'nine']],\n",
              " (5, 1219): [['alice', 'cooper', 'hey'],\n",
              "  ['alice', 'cooper', 'hey'],\n",
              "  ['cooper', 'hey', 'hey', 'stoopid'],\n",
              "  ['alice', 'cooper', 'hey'],\n",
              "  ['alice', 'cooper', 'hey'],\n",
              "  ['alice', 'cooper', 'hey'],\n",
              "  ['alice', 'cooper', 'hey'],\n",
              "  ['alice', 'cooper', 'stoopid'],\n",
              "  ['cooper', 'hey', 'hey', 'stoopid'],\n",
              "  ['alice', 'cooper', 'hey']],\n",
              " (5,\n",
              "  0): [['eve',\n",
              "   'whatever',\n",
              "   'solos',\n",
              "   'interspersed',\n",
              "   'driving',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['stories',\n",
              "   'traditionals',\n",
              "   'eve',\n",
              "   'record',\n",
              "   'orchestra',\n",
              "   'jason',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['snow',\n",
              "   'rock',\n",
              "   'ornament',\n",
              "   'electric',\n",
              "   'solos',\n",
              "   'children',\n",
              "   'lively',\n",
              "   'orchestra',\n",
              "   'holiday',\n",
              "   'prince',\n",
              "   'interspersed',\n",
              "   'driving',\n",
              "   'angelic',\n",
              "   'albums',\n",
              "   'jason',\n",
              "   'christmas',\n",
              "   'eve'], ['ornament',\n",
              "   'filled',\n",
              "   'children',\n",
              "   'verlinde',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['came',\n",
              "   'perhaps',\n",
              "   'disc',\n",
              "   'prog',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'compared',\n",
              "   'siberian',\n",
              "   'holiday',\n",
              "   'instrumentals',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['album',\n",
              "   'christmas',\n",
              "   'holiday',\n",
              "   'masterful',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['album',\n",
              "   'christmas',\n",
              "   'play',\n",
              "   'came',\n",
              "   'first',\n",
              "   'trans',\n",
              "   'siberian',\n",
              "   'orchestra',\n",
              "   'stories',\n",
              "   'rock',\n",
              "   'opera',\n",
              "   'christmas',\n",
              "   'musicianship',\n",
              "   'filled',\n",
              "   'electric',\n",
              "   'plenty',\n",
              "   'synthesized',\n",
              "   'choir',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'orchestra',\n",
              "   'holiday',\n",
              "   'christmas',\n",
              "   'attic',\n",
              "   'instrumentals',\n",
              "   'occasionally',\n",
              "   'christmas',\n",
              "   'stories'], ['album',\n",
              "   'trans',\n",
              "   'orchestra',\n",
              "   'stories',\n",
              "   'rock',\n",
              "   'new',\n",
              "   'musicianship',\n",
              "   'ornament',\n",
              "   'filled',\n",
              "   'guitar',\n",
              "   'drumming',\n",
              "   'one',\n",
              "   'disc',\n",
              "   'angelic',\n",
              "   'occasionally',\n",
              "   'concept',\n",
              "   'holiday',\n",
              "   'christmas',\n",
              "   'eve'], ['concept',\n",
              "   'way',\n",
              "   'angel',\n",
              "   'siberian',\n",
              "   'holiday',\n",
              "   'rock',\n",
              "   'age',\n",
              "   'ornament',\n",
              "   'prog',\n",
              "   'electric',\n",
              "   'record',\n",
              "   'trans',\n",
              "   'siberian',\n",
              "   'vocal',\n",
              "   'numbers',\n",
              "   'bombastic',\n",
              "   'verlinde',\n",
              "   'christmas',\n",
              "   'eve'], ['angel',\n",
              "   'bar',\n",
              "   'opera',\n",
              "   'age',\n",
              "   'christmas',\n",
              "   'filled',\n",
              "   'electric',\n",
              "   'drumming',\n",
              "   'siberian',\n",
              "   'disc',\n",
              "   'vocal',\n",
              "   'numbers',\n",
              "   'instrumentals',\n",
              "   'christmas',\n",
              "   'eve']],\n",
              " (5, 10126): [['brian', 'god', 'radio'],\n",
              "  ['studio', 'studio', 'god'],\n",
              "  ['first', 'bruce', 'god', 'radio'],\n",
              "  ['studio', 'pop', 'studio', 'jardine', 'god'],\n",
              "  ['album', 'god', 'radio'],\n",
              "  ['album', 'god', 'radio'],\n",
              "  ['studio', 'brian', 'god'],\n",
              "  ['studio', 'god', 'radio'],\n",
              "  ['wilson', 'god', 'radio'],\n",
              "  ['wilson', 'god', 'radio']],\n",
              " (5, 9004): [['democracy', 'shm'],\n",
              "  ['democracy', 'shm'],\n",
              "  ['democracy', 'shm'],\n",
              "  ['pressing', 'democracy', 'shm'],\n",
              "  ['democracy', 'shm'],\n",
              "  ['democracy', 'shm'],\n",
              "  ['democracy', 'shm'],\n",
              "  ['democracy', 'shm'],\n",
              "  ['democracy', 'shm'],\n",
              "  ['high', 'chinese', 'shm']],\n",
              " (5,\n",
              "  7243): [['long',\n",
              "   'platinum',\n",
              "   'sailor',\n",
              "   'never',\n",
              "   '2005',\n",
              "   'producer',\n",
              "   'rufus',\n",
              "   'elizondo',\n",
              "   'pawn',\n",
              "   'hip',\n",
              "   'view',\n",
              "   'window',\n",
              "   'machine',\n",
              "   'machine'], ['brilliant',\n",
              "   'loyal',\n",
              "   'scenes',\n",
              "   'label',\n",
              "   'ladder',\n",
              "   'playful',\n",
              "   'missed',\n",
              "   'mckay',\n",
              "   'ticked',\n",
              "   'machine',\n",
              "   'view',\n",
              "   'ever',\n",
              "   'machine'], ['machine',\n",
              "   'tracks',\n",
              "   'third',\n",
              "   'cent',\n",
              "   'garland',\n",
              "   'chorus',\n",
              "   'clear',\n",
              "   'slight',\n",
              "   'comparison',\n",
              "   'little',\n",
              "   'feel',\n",
              "   'tip',\n",
              "   'clear',\n",
              "   'break',\n",
              "   'machine'], ['irony',\n",
              "   'elizondo',\n",
              "   'world',\n",
              "   'mckay',\n",
              "   'closer',\n",
              "   'assumption',\n",
              "   'dipped',\n",
              "   'since',\n",
              "   'fans',\n",
              "   'glass',\n",
              "   'fogging',\n",
              "   'gorce',\n",
              "   'machine'], ['songs',\n",
              "   'passionate',\n",
              "   'third',\n",
              "   'number',\n",
              "   'notes',\n",
              "   'expectations',\n",
              "   'machine'], ['multi',\n",
              "   'side',\n",
              "   'performances',\n",
              "   'apple',\n",
              "   'produced',\n",
              "   'rufus',\n",
              "   'apple',\n",
              "   'every',\n",
              "   'track'], ['long',\n",
              "   'fiona',\n",
              "   'performances',\n",
              "   'judy',\n",
              "   'beats',\n",
              "   'time',\n",
              "   'spotlight',\n",
              "   'moves',\n",
              "   'correct',\n",
              "   'filler',\n",
              "   'track',\n",
              "   'sky',\n",
              "   'machine'], ['fiona',\n",
              "   'apple',\n",
              "   'sailor',\n",
              "   'released',\n",
              "   'brainy',\n",
              "   'young',\n",
              "   'moves',\n",
              "   'tip',\n",
              "   'hip',\n",
              "   'hop',\n",
              "   'apple',\n",
              "   'extraordinary'], ['lyrics',\n",
              "   'legions',\n",
              "   'artist',\n",
              "   'title',\n",
              "   'jon',\n",
              "   'producer',\n",
              "   'mike',\n",
              "   'like',\n",
              "   'kicks',\n",
              "   'cleverness',\n",
              "   'might',\n",
              "   'track',\n",
              "   'apple',\n",
              "   'machine'], ['multi',\n",
              "   'intensely',\n",
              "   'disc',\n",
              "   'behind',\n",
              "   'depth',\n",
              "   'wainwright',\n",
              "   'around',\n",
              "   'years',\n",
              "   'lament',\n",
              "   'glass',\n",
              "   'expectations',\n",
              "   'machine']],\n",
              " (6,\n",
              "  0): [['album',\n",
              "   'kindness',\n",
              "   'christmas',\n",
              "   'opera',\n",
              "   'filled',\n",
              "   'electric',\n",
              "   'synthesized',\n",
              "   'vocal',\n",
              "   'christmas',\n",
              "   'eve'], ['eve',\n",
              "   'lively',\n",
              "   'holiday',\n",
              "   'disc',\n",
              "   'prince',\n",
              "   'angelic',\n",
              "   'high',\n",
              "   'christmas',\n",
              "   'eve'], ['play',\n",
              "   'bar',\n",
              "   'age',\n",
              "   'christmas',\n",
              "   'numbers',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['age',\n",
              "   'masterful',\n",
              "   'children',\n",
              "   'eve',\n",
              "   'solos',\n",
              "   'peace',\n",
              "   'holiday',\n",
              "   'christmas',\n",
              "   'eve'], ['christmas',\n",
              "   'old',\n",
              "   'rock',\n",
              "   'album',\n",
              "   'guitar',\n",
              "   'drumming',\n",
              "   'numbers',\n",
              "   'christmas',\n",
              "   'eve'], ['trans',\n",
              "   'eve',\n",
              "   'stories',\n",
              "   'holiday',\n",
              "   'traditionals',\n",
              "   'orchestra',\n",
              "   'peace',\n",
              "   'jason',\n",
              "   'christmas',\n",
              "   'eve'], ['old',\n",
              "   'age',\n",
              "   'ornament',\n",
              "   'electric',\n",
              "   'plenty',\n",
              "   'holiday',\n",
              "   'albums',\n",
              "   'christmas',\n",
              "   'eve'], ['kindness',\n",
              "   'typical',\n",
              "   'record',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['angel', 'trans', 'stories', 'eve', 'christmas', 'angelic', 'holiday', 'christmas', 'eve'], ['new',\n",
              "   'guitar',\n",
              "   'keyboards',\n",
              "   'holiday',\n",
              "   'disc',\n",
              "   'prince',\n",
              "   'christmas',\n",
              "   'eve']],\n",
              " (6, 1609): [['classics'],\n",
              "  ['classics'],\n",
              "  ['classics'],\n",
              "  ['classics'],\n",
              "  ['classics'],\n",
              "  ['classics'],\n",
              "  ['classics'],\n",
              "  ['classics'],\n",
              "  ['classics'],\n",
              "  ['classics']],\n",
              " (6, 1294): [['shoes'],\n",
              "  ['shoes'],\n",
              "  ['shoes'],\n",
              "  ['shoes'],\n",
              "  ['shoes'],\n",
              "  ['shoes'],\n",
              "  ['shoes'],\n",
              "  ['shoes'],\n",
              "  ['shoes'],\n",
              "  ['shoes']],\n",
              " (6,\n",
              "  5008): [['gracefully',\n",
              "   'curiosity',\n",
              "   'bent',\n",
              "   'executive',\n",
              "   'helicopter',\n",
              "   'human'], ['last',\n",
              "   'one',\n",
              "   'collection',\n",
              "   'love',\n",
              "   'guest',\n",
              "   'human'], ['version', 'rock', '1998', 'boys', 'reflective', 'songs', 'array', 'handful', 'human'], ['great',\n",
              "   'boys',\n",
              "   'believe',\n",
              "   'human'], ['accused',\n",
              "   'rooster',\n",
              "   'dickins',\n",
              "   'writers',\n",
              "   'macy',\n",
              "   'handful',\n",
              "   'human'], ['matured', 'spacious', 'reflective', 'come', 'human'], ['rod',\n",
              "   'rather',\n",
              "   'guest',\n",
              "   'playful',\n",
              "   'helicopter',\n",
              "   'knopfler',\n",
              "   'human'], ['rod',\n",
              "   'rod',\n",
              "   'rather',\n",
              "   'rob',\n",
              "   'spacious',\n",
              "   'mature',\n",
              "   'human'], ['boys', 'album', 'rob', 'keeps', 'stewart', 'including', 'human'], ['rod',\n",
              "   'rod',\n",
              "   'singer',\n",
              "   'curiosity',\n",
              "   'surprising',\n",
              "   'stewart',\n",
              "   'slate',\n",
              "   'slash',\n",
              "   'human']],\n",
              " (6,\n",
              "  7431): [['group',\n",
              "   'engage',\n",
              "   'frivolous',\n",
              "   'catastrophe',\n",
              "   'sky',\n",
              "   'near',\n",
              "   'day',\n",
              "   'wide',\n",
              "   'along',\n",
              "   'andrew',\n",
              "   'open',\n",
              "   'life'], ['sebastian',\n",
              "   'album',\n",
              "   'pursuit',\n",
              "   'carriage',\n",
              "   'clock',\n",
              "   'since',\n",
              "   'step',\n",
              "   'rex',\n",
              "   'story',\n",
              "   'strap',\n",
              "   'life'], ['since',\n",
              "   'cast',\n",
              "   'baby',\n",
              "   'song',\n",
              "   'bright',\n",
              "   'rhythms',\n",
              "   'cup',\n",
              "   'like',\n",
              "   'life',\n",
              "   'pursuit'], ['diverse',\n",
              "   'cornelius',\n",
              "   'late',\n",
              "   'pathos',\n",
              "   'beat',\n",
              "   'retain',\n",
              "   'lift',\n",
              "   'still',\n",
              "   'funk',\n",
              "   'funny',\n",
              "   'life',\n",
              "   'pursuit'], ['reminiscent',\n",
              "   'cast',\n",
              "   'frivolous',\n",
              "   'get',\n",
              "   'set',\n",
              "   'beat',\n",
              "   'boy',\n",
              "   'flawlessly',\n",
              "   'pacing',\n",
              "   'sing',\n",
              "   'pursuit'], ['folky',\n",
              "   'carriage',\n",
              "   'gotten',\n",
              "   'influences',\n",
              "   'step',\n",
              "   'familiar',\n",
              "   'almost',\n",
              "   'arab',\n",
              "   'life'], ['cast',\n",
              "   'retain',\n",
              "   'indulged',\n",
              "   'greenbaum',\n",
              "   'flawlessly',\n",
              "   'funk',\n",
              "   'tell',\n",
              "   'another',\n",
              "   'sunny',\n",
              "   'belle',\n",
              "   'like',\n",
              "   'belle',\n",
              "   'life',\n",
              "   'pursuit'], ['group',\n",
              "   'sophisticated',\n",
              "   'catastrophe',\n",
              "   'waitress',\n",
              "   'catastrophe',\n",
              "   'rex',\n",
              "   'pacing',\n",
              "   'boy',\n",
              "   'life'], ['cornelius',\n",
              "   'manfred',\n",
              "   'work',\n",
              "   'pop',\n",
              "   'work',\n",
              "   'sebastian',\n",
              "   'boot',\n",
              "   'boy',\n",
              "   'indulged',\n",
              "   'greenbaum',\n",
              "   'decorous',\n",
              "   'wounds',\n",
              "   'life',\n",
              "   'pursuit'], ['work',\n",
              "   'free',\n",
              "   'players',\n",
              "   'clock',\n",
              "   'dying',\n",
              "   'since',\n",
              "   'step',\n",
              "   'retain',\n",
              "   'charms',\n",
              "   'reaches',\n",
              "   'decorous',\n",
              "   'belle',\n",
              "   'sebastian',\n",
              "   'life']],\n",
              " (7, 9801): [['album', 'events'],\n",
              "  ['album', 'events'],\n",
              "  ['album', 'events'],\n",
              "  ['album', 'events'],\n",
              "  ['album', 'events'],\n",
              "  ['album', 'events'],\n",
              "  ['album', 'events'],\n",
              "  ['album', 'events'],\n",
              "  ['album', 'events'],\n",
              "  ['album', 'events']],\n",
              " (7, 9519): [['years', 'commanding', 'dream', 'static'],\n",
              "  ['well', 'metal', 'years', 'static'],\n",
              "  ['new', 'static', 'impulse'],\n",
              "  ['effort', 'many', 'static'],\n",
              "  ['years', 'static'],\n",
              "  ['impulse', 'prog', 'gods', 'static'],\n",
              "  ['sure', 'last', 'prog', 'static', 'impulse'],\n",
              "  ['one', 'question', 'anticipating', 'static'],\n",
              "  ['first', 'static'],\n",
              "  ['first', 'question', 'latest', 'static']],\n",
              " (7, 994): [['wall'],\n",
              "  ['wall'],\n",
              "  ['wall'],\n",
              "  ['wall'],\n",
              "  ['wall'],\n",
              "  ['wall'],\n",
              "  ['wall'],\n",
              "  ['wall'],\n",
              "  ['wall'],\n",
              "  ['wall']],\n",
              " (7, 9740): [['dedicated'],\n",
              "  ['dedicated'],\n",
              "  ['forward', 'dedicated'],\n",
              "  ['forward', 'dedicated'],\n",
              "  ['dedicated'],\n",
              "  ['hard', 'dedicated'],\n",
              "  ['record', 'dedicated'],\n",
              "  ['dedicated'],\n",
              "  ['dedicated'],\n",
              "  ['sacrifice', 'dedicated']],\n",
              " (7, 9223): [['melting', 'album', 'mountain'],\n",
              "  ['opening', 'album', 'intro', 'recognizable', 'stockholm', 'mountain'],\n",
              "  ['intro', 'stunning', 'steve', 'mountain'],\n",
              "  ['album', 'foot', 'mountain'],\n",
              "  ['career', 'lyrics', 'ljunggren', 'mountain'],\n",
              "  ['influenced', 'flows', 'sunny', 'cements', 'mountain'],\n",
              "  ['announce', 'album', 'stockholm', 'mountain'],\n",
              "  ['beats', 'recorded', 'steve', 'mountain'],\n",
              "  ['arcade', 'crest', 'album', 'alongside', 'mountain'],\n",
              "  ['quickly', 'return', 'beats', 'mountain']],\n",
              " (8,\n",
              "  9121): [['guys',\n",
              "   'album',\n",
              "   'appropriately',\n",
              "   'sensation',\n",
              "   'fox',\n",
              "   'cnn',\n",
              "   'city',\n",
              "   'kings',\n",
              "   'one',\n",
              "   'claus',\n",
              "   'jingle',\n",
              "   'christmas',\n",
              "   'cheers'], ['ordinary',\n",
              "   'back',\n",
              "   'records',\n",
              "   'including',\n",
              "   'music',\n",
              "   'christmas',\n",
              "   'classic',\n",
              "   'alongside',\n",
              "   'holy',\n",
              "   'town',\n",
              "   'christmas',\n",
              "   'cheers'], ['former',\n",
              "   '000',\n",
              "   'straight',\n",
              "   'chaser',\n",
              "   'recording',\n",
              "   'twist',\n",
              "   'barry',\n",
              "   'orient',\n",
              "   'reindeer',\n",
              "   'santa',\n",
              "   'christmas',\n",
              "   'cheers'], ['straight',\n",
              "   'changed',\n",
              "   'season',\n",
              "   'spirits',\n",
              "   'sell',\n",
              "   'show',\n",
              "   'headline',\n",
              "   'christmas',\n",
              "   'sensation',\n",
              "   'lsquo',\n",
              "   'tour',\n",
              "   'list',\n",
              "   'christmas',\n",
              "   'cheers'], ['straight',\n",
              "   '2008',\n",
              "   '1998',\n",
              "   'went',\n",
              "   'record',\n",
              "   'lsquo',\n",
              "   'lsquo',\n",
              "   'studio',\n",
              "   'recording',\n",
              "   'christmas',\n",
              "   'including',\n",
              "   'comedic',\n",
              "   'twist',\n",
              "   'also',\n",
              "   'kings',\n",
              "   'let',\n",
              "   'days',\n",
              "   'christmas',\n",
              "   'cheers'], ['youtube',\n",
              "   'indiana',\n",
              "   'asked',\n",
              "   'went',\n",
              "   'sensation',\n",
              "   'times',\n",
              "   'christmas',\n",
              "   'october',\n",
              "   'also',\n",
              "   'get',\n",
              "   'three',\n",
              "   'rest',\n",
              "   'christmas',\n",
              "   'cheers'], ['december',\n",
              "   'days',\n",
              "   'filmed',\n",
              "   'viral',\n",
              "   'university',\n",
              "   'spirits',\n",
              "   'copies',\n",
              "   'media',\n",
              "   'sensation',\n",
              "   'throughout',\n",
              "   'holiday',\n",
              "   'featured',\n",
              "   'amp',\n",
              "   'show',\n",
              "   'washington',\n",
              "   'tnt',\n",
              "   'holiday',\n",
              "   'christmas',\n",
              "   'including',\n",
              "   'original',\n",
              "   'music',\n",
              "   'formula',\n",
              "   'smooth',\n",
              "   'comedic',\n",
              "   'new',\n",
              "   'guys',\n",
              "   'city',\n",
              "   'alongside',\n",
              "   'bowl',\n",
              "   'get',\n",
              "   'round',\n",
              "   'list',\n",
              "   'three',\n",
              "   'let',\n",
              "   'claus',\n",
              "   'esta',\n",
              "   'rudolph',\n",
              "   'one',\n",
              "   'spiked',\n",
              "   'christmas',\n",
              "   'cheers'], ['indiana',\n",
              "   'together',\n",
              "   'went',\n",
              "   'world',\n",
              "   'news',\n",
              "   'recording',\n",
              "   'formula',\n",
              "   'cheers',\n",
              "   'comedic',\n",
              "   'track',\n",
              "   'home',\n",
              "   'jingle',\n",
              "   'christmas',\n",
              "   'cheers'], ['christmas',\n",
              "   'former',\n",
              "   'titled',\n",
              "   'show',\n",
              "   'cnn',\n",
              "   'holiday',\n",
              "   'december',\n",
              "   'cheer',\n",
              "   'nog',\n",
              "   'christmas',\n",
              "   'cheers'], ['together',\n",
              "   'christmas',\n",
              "   'holiday',\n",
              "   'straight',\n",
              "   'collection',\n",
              "   'classic',\n",
              "   'twist',\n",
              "   'santa',\n",
              "   'holy',\n",
              "   'gentlemen',\n",
              "   'jingle',\n",
              "   'christmas',\n",
              "   'cheers']],\n",
              " (8,\n",
              "  941): [['words',\n",
              "   'offerings',\n",
              "   'night',\n",
              "   'traveled',\n",
              "   'well',\n",
              "   'christmas',\n",
              "   'album'], ['christmas',\n",
              "   'christmas',\n",
              "   'keller',\n",
              "   'christmas',\n",
              "   'album'], ['side', 'streisand', 'christmas', 'album'], ['streisand',\n",
              "   'bowl',\n",
              "   'easy',\n",
              "   'christmas',\n",
              "   'album'], ['album',\n",
              "   'pushing',\n",
              "   'bebop',\n",
              "   'little',\n",
              "   'christmas',\n",
              "   'album'], ['barbra', 'christmas', 'album'], ['things',\n",
              "   'christmas',\n",
              "   'song',\n",
              "   'keller',\n",
              "   'christmas',\n",
              "   'album'], ['singer', 'bowl', 'singular', 'christmas', 'album'], ['keenly',\n",
              "   'christmas',\n",
              "   'album'], ['christmas', 'range', 'christmas', 'album']],\n",
              " (8, 8955): [['real', 'chaser', 'holiday'],\n",
              "  ['ago', 'holiday'],\n",
              "  ['deal', 'holiday'],\n",
              "  ['cappella', 'reemerged', 'digital', 'holiday'],\n",
              "  ['holiday'],\n",
              "  ['pro', 'holiday'],\n",
              "  ['originally', 'christmas', 'holiday'],\n",
              "  ['views', 'holiday'],\n",
              "  ['coming', 'holiday'],\n",
              "  ['together', 'holiday']],\n",
              " (8, 5515): [['lilies'],\n",
              "  ['lilies'],\n",
              "  ['lilies'],\n",
              "  ['lilies'],\n",
              "  ['lilies'],\n",
              "  ['lilies'],\n",
              "  ['lilies'],\n",
              "  ['lilies'],\n",
              "  ['lilies'],\n",
              "  ['lilies']],\n",
              " (8, 3062): [['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits']],\n",
              " (9,\n",
              "  0): [['tales',\n",
              "   'christmas',\n",
              "   'play',\n",
              "   'old',\n",
              "   'siberian',\n",
              "   'stories',\n",
              "   'rock',\n",
              "   'filled',\n",
              "   'solos',\n",
              "   'synthesized',\n",
              "   'christmas',\n",
              "   'compared',\n",
              "   'siberian',\n",
              "   'disc',\n",
              "   'prince',\n",
              "   'albums',\n",
              "   'christmas',\n",
              "   'eve'], ['angel',\n",
              "   'maybe',\n",
              "   'christmas',\n",
              "   'lively',\n",
              "   'drumming',\n",
              "   'record',\n",
              "   'driving',\n",
              "   'occasionally',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['concept',\n",
              "   'tales',\n",
              "   'angel',\n",
              "   'city',\n",
              "   'bar',\n",
              "   'christmas',\n",
              "   'new',\n",
              "   'masterful',\n",
              "   'ornament',\n",
              "   'solos',\n",
              "   'keyboards',\n",
              "   'lively',\n",
              "   'christmas',\n",
              "   'trans',\n",
              "   'siberian',\n",
              "   'christmas',\n",
              "   'instrumentals',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['first',\n",
              "   'christmas',\n",
              "   'intertwined',\n",
              "   'musicianship',\n",
              "   'numbers',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['play', 'typical', 'christmas', 'eve', 'stories'], ['plus',\n",
              "   'came',\n",
              "   'holiday',\n",
              "   'holiday',\n",
              "   'filled',\n",
              "   'electric',\n",
              "   'children',\n",
              "   'eve',\n",
              "   'one',\n",
              "   'trans',\n",
              "   'holiday',\n",
              "   'vocal',\n",
              "   'prince',\n",
              "   'interspersed',\n",
              "   'instrumentals',\n",
              "   'sentimental',\n",
              "   'bombastic',\n",
              "   'christmas',\n",
              "   'eve'], ['bar',\n",
              "   'holiday',\n",
              "   'holiday',\n",
              "   'age',\n",
              "   'prog',\n",
              "   'choir',\n",
              "   'solos',\n",
              "   'prince',\n",
              "   'jason',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['trans',\n",
              "   'rock',\n",
              "   'one',\n",
              "   'solos',\n",
              "   'angelic',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories'], ['way',\n",
              "   'plus',\n",
              "   'snow',\n",
              "   'siberian',\n",
              "   'holiday',\n",
              "   'disc',\n",
              "   'solos',\n",
              "   'plenty',\n",
              "   'keyboards',\n",
              "   'choir',\n",
              "   'lively',\n",
              "   'eve',\n",
              "   'compared',\n",
              "   'siberian',\n",
              "   'vocal',\n",
              "   'angelic',\n",
              "   'christmas',\n",
              "   'eve'], ['tales',\n",
              "   'city',\n",
              "   'siberian',\n",
              "   'rock',\n",
              "   'opera',\n",
              "   'perhaps',\n",
              "   'new',\n",
              "   'typical',\n",
              "   'solos',\n",
              "   'children',\n",
              "   'eve',\n",
              "   'one',\n",
              "   'holiday',\n",
              "   'peace',\n",
              "   'driving',\n",
              "   'instrumentals',\n",
              "   'christmas',\n",
              "   'eve',\n",
              "   'stories']],\n",
              " (9, 5389): [['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire'],\n",
              "  ['christmas', 'extraordinaire']],\n",
              " (9, 3990): [['attic', 'carols', 'christmas'],\n",
              "  ['orchestra', 'trilogy', 'pop', 'less', 'christmas'],\n",
              "  ['sarajevo', 'soul', 'lloyd', 'christmas', 'hymns', 'christmas'],\n",
              "  ['type',\n",
              "   'mate',\n",
              "   'one',\n",
              "   'pop',\n",
              "   'acoustic',\n",
              "   'numbers',\n",
              "   'singers',\n",
              "   'professional',\n",
              "   'christmas'],\n",
              "  ['christmas',\n",
              "   'orchestra',\n",
              "   'andrew',\n",
              "   'trans',\n",
              "   'behind',\n",
              "   'christmas',\n",
              "   'grand',\n",
              "   'ambitious',\n",
              "   'keller',\n",
              "   'christmas'],\n",
              "  ['orchestra',\n",
              "   'planned',\n",
              "   'attic',\n",
              "   'date',\n",
              "   'themes',\n",
              "   'leave',\n",
              "   'away',\n",
              "   'christmas'],\n",
              "  ['behind', 'hymns', 'melodrama', 'christmas'],\n",
              "  ['attic', 'wool', 'neill', 'often', 'nonetheless', 'christmas'],\n",
              "  ['music', 'trans', 'sent', 'orchestral', 'something', 'christmas'],\n",
              "  ['blend',\n",
              "   'christmas',\n",
              "   'mate',\n",
              "   'siberian',\n",
              "   'sent',\n",
              "   'heavy',\n",
              "   'christmas',\n",
              "   'attic']],\n",
              " (9, 1431): [['christmas'],\n",
              "  ['christmas'],\n",
              "  ['christmas'],\n",
              "  ['christmas'],\n",
              "  ['christmas'],\n",
              "  ['christmas'],\n",
              "  ['christmas'],\n",
              "  ['christmas'],\n",
              "  ['christmas'],\n",
              "  ['christmas']],\n",
              " (9, 80): [['fresh'],\n",
              "  ['fresh'],\n",
              "  ['fresh'],\n",
              "  ['fresh'],\n",
              "  ['fresh'],\n",
              "  ['fresh'],\n",
              "  ['fresh'],\n",
              "  ['fresh'],\n",
              "  ['fresh'],\n",
              "  ['fresh']],\n",
              " (10, 9393): [['songs', 'amp', 'two'],\n",
              "  ['feels', 'deschanel', 'amp', 'two'],\n",
              "  ['fascinating', 'arrangements', 'amp', 'two'],\n",
              "  ['grown', 'even', 'amp', 'two'],\n",
              "  ['matt', 'ward', 'amp', 'two'],\n",
              "  ['songs', 'amp', 'two'],\n",
              "  ['old', 'amp', 'two'],\n",
              "  ['old', 'harmonies', 'deschanel', 'volume', 'two'],\n",
              "  ['layered', 'zooey', 'amp', 'two'],\n",
              "  ['stay', 'zooey', 'amp', 'two']],\n",
              " (10,\n",
              "  3985): [['music',\n",
              "   'soundtrack',\n",
              "   'symphony',\n",
              "   'orchestra',\n",
              "   'lot',\n",
              "   'maire',\n",
              "   'back',\n",
              "   'titanic'], ['celine',\n",
              "   'piano',\n",
              "   'disappointed',\n",
              "   'timeless',\n",
              "   'awkward',\n",
              "   'third',\n",
              "   'back',\n",
              "   'titanic'], ['james',\n",
              "   'titanic',\n",
              "   'music',\n",
              "   'epilogue',\n",
              "   'move',\n",
              "   'compositions',\n",
              "   'party',\n",
              "   'back',\n",
              "   'titanic'], ['music',\n",
              "   'fans',\n",
              "   'disc',\n",
              "   'epilogue',\n",
              "   'played',\n",
              "   'chamber',\n",
              "   'jason',\n",
              "   'back',\n",
              "   'titanic'], ['rescues',\n",
              "   'great',\n",
              "   'despite',\n",
              "   'awkward',\n",
              "   'played',\n",
              "   'lot',\n",
              "   'josephine',\n",
              "   'back',\n",
              "   'titanic'], ['film',\n",
              "   'fans',\n",
              "   'newly',\n",
              "   'timeless',\n",
              "   'josephine',\n",
              "   'movie',\n",
              "   'breathy',\n",
              "   'back',\n",
              "   'titanic'], ['collection',\n",
              "   'performance',\n",
              "   'titanic',\n",
              "   'version',\n",
              "   'plus',\n",
              "   'first',\n",
              "   'composed',\n",
              "   'move',\n",
              "   'choirsters',\n",
              "   'back',\n",
              "   'titanic'], ['album',\n",
              "   'premiere',\n",
              "   'moving',\n",
              "   'melodies',\n",
              "   'cambridge',\n",
              "   'work',\n",
              "   'back',\n",
              "   'titanic'], ['sunk',\n",
              "   'collection',\n",
              "   'includes',\n",
              "   'dialogue',\n",
              "   'world',\n",
              "   'premiere',\n",
              "   'background',\n",
              "   'composed',\n",
              "   'movie',\n",
              "   'awkward',\n",
              "   'diversity',\n",
              "   'storm',\n",
              "   'dialogue',\n",
              "   'back',\n",
              "   'titanic'], ['irish',\n",
              "   'found',\n",
              "   'newly',\n",
              "   'composed',\n",
              "   'irish',\n",
              "   'flying',\n",
              "   'dialogue',\n",
              "   'songs',\n",
              "   'verlinde',\n",
              "   'back',\n",
              "   'titanic']],\n",
              " (10, 5741): [['stylized', 'dramatic', 'songs'],\n",
              "  ['certainly', 'uhelszki', 'songs'],\n",
              "  ['day', 'conjures', 'singer', 'songs'],\n",
              "  ['organ', 'maroon', 'adam', 'songs'],\n",
              "  ['organ', 'harmonies', 'conjures', 'songs'],\n",
              "  ['best', 'despite', 'songs'],\n",
              "  ['never', 'harmonies', 'jaan', 'songs'],\n",
              "  ['new', 'latter', 'vocals', 'sound', 'songs'],\n",
              "  ['close', 'songs', 'levin', 'songs'],\n",
              "  ['like', 'engaging', 'uhelszki', 'songs']],\n",
              " (10, 2871): [['wind'],\n",
              "  ['wind'],\n",
              "  ['wind'],\n",
              "  ['wind'],\n",
              "  ['wind'],\n",
              "  ['wind'],\n",
              "  ['wind'],\n",
              "  ['wind'],\n",
              "  ['wind'],\n",
              "  ['wind']],\n",
              " (10,\n",
              "  9473): [['20th',\n",
              "   'money',\n",
              "   '2010',\n",
              "   'catalogue',\n",
              "   'album',\n",
              "   'megaforce',\n",
              "   'feature',\n",
              "   'performance',\n",
              "   'croweology'], ['year',\n",
              "   'maker',\n",
              "   'release',\n",
              "   'album',\n",
              "   'croweology',\n",
              "   'paul',\n",
              "   'black',\n",
              "   'hour',\n",
              "   'half',\n",
              "   'croweology'], ['time',\n",
              "   'catalogue',\n",
              "   'years',\n",
              "   'label',\n",
              "   'electric',\n",
              "   'set',\n",
              "   '2010',\n",
              "   'croweology'], ['august',\n",
              "   'sunset',\n",
              "   'released',\n",
              "   'black',\n",
              "   'electric',\n",
              "   'consist',\n",
              "   'acoustic',\n",
              "   'croweology'], ['acoustic',\n",
              "   'august',\n",
              "   'crowes',\n",
              "   'feature',\n",
              "   'full',\n",
              "   'set',\n",
              "   'conclude',\n",
              "   'annual',\n",
              "   '2010',\n",
              "   'lengthy',\n",
              "   'croweology'], ['multi',\n",
              "   'maker',\n",
              "   'black',\n",
              "   'band',\n",
              "   'produced',\n",
              "   'crowes',\n",
              "   'hour',\n",
              "   'presently',\n",
              "   'croweology'], ['release',\n",
              "   '2009',\n",
              "   'sold',\n",
              "   'paul',\n",
              "   'followed',\n",
              "   'croweology'], ['multi',\n",
              "   'late',\n",
              "   'kick',\n",
              "   'three',\n",
              "   'sets',\n",
              "   'croweology'], ['multi', 'history', 'black', 'three', 'performance', 'annual', 'multi', 'lengthy', 'croweology'], ['double',\n",
              "   'bad',\n",
              "   'full',\n",
              "   'night',\n",
              "   'stand',\n",
              "   'san',\n",
              "   'hiatus',\n",
              "   'croweology']],\n",
              " (11, 1787): [['change', 'seasons'],\n",
              "  ['change', 'seasons'],\n",
              "  ['change', 'seasons'],\n",
              "  ['change', 'seasons'],\n",
              "  ['change', 'seasons'],\n",
              "  ['change', 'seasons'],\n",
              "  ['theater', 'change', 'seasons'],\n",
              "  ['change', 'seasons'],\n",
              "  ['change', 'seasons'],\n",
              "  ['dream', 'theater', 'seasons']],\n",
              " (11, 1918): [['dream', 'falling', 'infinity'],\n",
              "  ['dream', 'theater', 'infinity'],\n",
              "  ['dream', 'falling', 'infinity'],\n",
              "  ['dream', 'falling', 'infinity'],\n",
              "  ['dream', 'falling', 'infinity'],\n",
              "  ['dream', 'falling', 'infinity'],\n",
              "  ['dream', 'falling', 'infinity'],\n",
              "  ['dream', 'falling', 'infinity'],\n",
              "  ['dream', 'falling', 'infinity'],\n",
              "  ['dream', 'falling', 'infinity']],\n",
              " (11, 4520): [['rattle'],\n",
              "  ['rattle'],\n",
              "  ['rattle'],\n",
              "  ['rattle'],\n",
              "  ['rattle'],\n",
              "  ['rattle'],\n",
              "  ['rattle'],\n",
              "  ['rattle'],\n",
              "  ['rattle'],\n",
              "  ['rattle']],\n",
              " (11, 2091): [['groups', 'keeps', 'solid', 'awake'],\n",
              "  ['term', 'soul', 'rush', 'theater', 'thunderous', 'awake'],\n",
              "  ['term', 'like', 'though', 'human', 'riff', 'solid', 'awake'],\n",
              "  ['feeling', 'vocals', 'true', 'real', 'guitar', 'rocking', 'awake'],\n",
              "  ['awake', 'prog', 'flourishes', 'passionate', 'volley', 'mind', 'awake'],\n",
              "  ['rock', 'lake', 'palmer', 'emotion', 'depth', 'awake'],\n",
              "  ['suck', 'something', 'crusher', 'petrucci', 'guitar', 'awake'],\n",
              "  ['supposed', 'intellectual', 'rush', 'emotion', 'guitar', 'dream', 'awake'],\n",
              "  ['anti',\n",
              "   'degree',\n",
              "   'overorchestration',\n",
              "   'many',\n",
              "   'manage',\n",
              "   'emotion',\n",
              "   'awake'],\n",
              "  ['soul', 'wailing', 'petrucci', 'awake']],\n",
              " (11, 1840): [['album', 'zeppelin', 'personal', 'live', 'led', 'zeppelin'],\n",
              "  ['roll', 'complete', 'stairway', 'led', 'zeppelin'],\n",
              "  ['personal', 'disc', 'features', 'soundtrack', 'zeppelin'],\n",
              "  ['1976', 'greatest', 'love', 'led', 'zeppelin'],\n",
              "  ['acknowledged', 'career', 'soundtrack', 'zeppelin'],\n",
              "  ['remains', 'member', 'whole', 'led', 'zeppelin'],\n",
              "  ['zeppelin', 'formidable', 'sort', 'good', 'heaven', 'led', 'zeppelin'],\n",
              "  ['song', 'versions', 'love', 'led', 'zeppelin'],\n",
              "  ['length', 'feature', 'led', 'zeppelin'],\n",
              "  ['released', 'footage', 'oblique', 'led', 'zeppelin']],\n",
              " (12, 3714): [['iron', 'maiden', 'powerslave'],\n",
              "  ['iron', 'maiden', 'powerslave'],\n",
              "  ['iron', 'maiden', 'powerslave'],\n",
              "  ['iron', 'powerslave', 'powerslave'],\n",
              "  ['iron', 'powerslave', 'powerslave'],\n",
              "  ['iron', 'maiden', 'powerslave'],\n",
              "  ['iron', 'maiden', 'powerslave'],\n",
              "  ['iron', 'maiden', 'powerslave'],\n",
              "  ['iron', 'maiden', 'powerslave'],\n",
              "  ['iron', 'maiden', 'powerslave']],\n",
              " (12, 6988): [['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny'],\n",
              "  ['soul', 'souls', 'tyranny']],\n",
              " (12,\n",
              "  7198): [['system',\n",
              "   'stone',\n",
              "   'push',\n",
              "   'boundaries',\n",
              "   'acoustic',\n",
              "   'primal',\n",
              "   'abrasive',\n",
              "   'roadrunner',\n",
              "   'becoming',\n",
              "   'expanding',\n",
              "   'overall',\n",
              "   'pulls',\n",
              "   'deliverance',\n",
              "   'reveries',\n",
              "   'ante',\n",
              "   'instrumentation',\n",
              "   'rhythms',\n",
              "   'changes',\n",
              "   'simplified',\n",
              "   'bit',\n",
              "   'reveries',\n",
              "   'well',\n",
              "   'atonement',\n",
              "   'integration',\n",
              "   'styles',\n",
              "   'album',\n",
              "   'meat',\n",
              "   'tranquility',\n",
              "   'conjuration',\n",
              "   'rest',\n",
              "   'epic',\n",
              "   'graffiti',\n",
              "   'led',\n",
              "   'metal',\n",
              "   'ghost',\n",
              "   'reveries'], ['2005',\n",
              "   'personalities',\n",
              "   'upped',\n",
              "   'album',\n",
              "   'opens',\n",
              "   'tranquility',\n",
              "   'smaller',\n",
              "   'ghost',\n",
              "   'reveries'], ['stand',\n",
              "   'system',\n",
              "   'masterstroke',\n",
              "   'metal',\n",
              "   'band',\n",
              "   'ghost',\n",
              "   'band',\n",
              "   'doesnt',\n",
              "   'ghost',\n",
              "   'reveries'], ['system',\n",
              "   'continues',\n",
              "   'changes',\n",
              "   'conjuration',\n",
              "   'ghost',\n",
              "   'ghost',\n",
              "   'reveries'], ['like',\n",
              "   'volta',\n",
              "   'band',\n",
              "   'simplistic',\n",
              "   'time',\n",
              "   'simplified',\n",
              "   'necessarily',\n",
              "   'seconds',\n",
              "   'ghost',\n",
              "   'reveries'], ['places',\n",
              "   'together',\n",
              "   'band',\n",
              "   'opeth',\n",
              "   'rousing',\n",
              "   'polyrhythms',\n",
              "   'ghost',\n",
              "   'arambel',\n",
              "   'ghost',\n",
              "   'reveries'], ['soothing',\n",
              "   'roar',\n",
              "   'abrasive',\n",
              "   'overall',\n",
              "   'simplistic',\n",
              "   'band',\n",
              "   'instrumentation',\n",
              "   'reveries',\n",
              "   'ghost',\n",
              "   'reveries'], ['ghost',\n",
              "   'shoulder',\n",
              "   'rhythms',\n",
              "   'changes',\n",
              "   'hounds',\n",
              "   'eastern',\n",
              "   'floyd',\n",
              "   'beyond',\n",
              "   'ghost',\n",
              "   'reveries'], ['abrasive',\n",
              "   'becoming',\n",
              "   'giving',\n",
              "   'organ',\n",
              "   'album',\n",
              "   'anything',\n",
              "   'graffiti',\n",
              "   'zeppelin',\n",
              "   'endeavors',\n",
              "   'ghost',\n",
              "   'reveries'], ['ghost',\n",
              "   'system',\n",
              "   'incorporating',\n",
              "   'band',\n",
              "   'instrumentation',\n",
              "   'impact',\n",
              "   'conjuration',\n",
              "   'band',\n",
              "   'epic',\n",
              "   'ghost',\n",
              "   'reveries']],\n",
              " (12, 7438): [['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation'],\n",
              "  ['mindcrime', 'operation']],\n",
              " (12,\n",
              "  948): [['dylan',\n",
              "   'hits',\n",
              "   'disc',\n",
              "   'installment',\n",
              "   'dylan',\n",
              "   'greatest',\n",
              "   'much',\n",
              "   'lay',\n",
              "   'even',\n",
              "   'richer',\n",
              "   'earlier',\n",
              "   'best',\n",
              "   'set',\n",
              "   'previously',\n",
              "   'smashing',\n",
              "   'flow',\n",
              "   'masterfully',\n",
              "   'makes',\n",
              "   'stop',\n",
              "   'let',\n",
              "   'live',\n",
              "   'time',\n",
              "   'new',\n",
              "   'stunning',\n",
              "   'takes',\n",
              "   'released',\n",
              "   'fave',\n",
              "   'rickey',\n",
              "   'dylan',\n",
              "   'hits',\n",
              "   'vol'], ['time',\n",
              "   'comes',\n",
              "   'lay',\n",
              "   'hard',\n",
              "   'listen',\n",
              "   'people',\n",
              "   'stunning',\n",
              "   'rickey',\n",
              "   'bob',\n",
              "   'dylan',\n",
              "   'hits',\n",
              "   'vol'], ['vol',\n",
              "   'selected',\n",
              "   'new',\n",
              "   'flow',\n",
              "   'flood',\n",
              "   'greatest',\n",
              "   'hits',\n",
              "   'vol'], ['bob',\n",
              "   'selected',\n",
              "   'installment',\n",
              "   'comes',\n",
              "   'much',\n",
              "   'rain',\n",
              "   'notwithstanding',\n",
              "   'listen',\n",
              "   '1971',\n",
              "   'casts',\n",
              "   'adds',\n",
              "   'flow',\n",
              "   'people',\n",
              "   'everywhere',\n",
              "   'look',\n",
              "   'makes',\n",
              "   'read',\n",
              "   'bob',\n",
              "   'stunning',\n",
              "   'shall',\n",
              "   'dylan',\n",
              "   'greatest',\n",
              "   'hits'], ['bob',\n",
              "   'dylan',\n",
              "   'dylan',\n",
              "   'hits',\n",
              "   'time',\n",
              "   'selected',\n",
              "   'programmed',\n",
              "   'two',\n",
              "   'disc',\n",
              "   'second',\n",
              "   'hits',\n",
              "   'series',\n",
              "   'much',\n",
              "   'lay',\n",
              "   'lady',\n",
              "   'lay',\n",
              "   'rain',\n",
              "   'idiosyncratic',\n",
              "   'cut',\n",
              "   'listen',\n",
              "   'earlier',\n",
              "   'best',\n",
              "   '1971',\n",
              "   'set',\n",
              "   'light',\n",
              "   'adds',\n",
              "   'river',\n",
              "   'flow',\n",
              "   'amazon',\n",
              "   'people',\n",
              "   'everywhere',\n",
              "   'look',\n",
              "   'stop',\n",
              "   'read',\n",
              "   'lunch',\n",
              "   'live',\n",
              "   'takes',\n",
              "   'shall',\n",
              "   'nowhere',\n",
              "   'flood',\n",
              "   'fave',\n",
              "   'disagreein',\n",
              "   'wright',\n",
              "   'rickey',\n",
              "   'dylan',\n",
              "   'hits'], ['new',\n",
              "   'masterfully',\n",
              "   'people',\n",
              "   'look',\n",
              "   'live',\n",
              "   'cuff',\n",
              "   'bob',\n",
              "   'dylan',\n",
              "   'greatest',\n",
              "   'hits'], ['lady',\n",
              "   'notwithstanding',\n",
              "   'non',\n",
              "   'masterfully',\n",
              "   '1963',\n",
              "   'greatest',\n",
              "   'hits',\n",
              "   'vol'], ['bob',\n",
              "   'two',\n",
              "   'comes',\n",
              "   'much',\n",
              "   'famed',\n",
              "   'lady',\n",
              "   'best',\n",
              "   'light',\n",
              "   'people',\n",
              "   'look',\n",
              "   'stop',\n",
              "   'lunch',\n",
              "   'live',\n",
              "   'dylan',\n",
              "   'greatest',\n",
              "   'hits',\n",
              "   'vol'], ['bob',\n",
              "   'programmed',\n",
              "   'richer',\n",
              "   'set',\n",
              "   'adds',\n",
              "   'previously',\n",
              "   'flow',\n",
              "   'takes',\n",
              "   'goin',\n",
              "   'flood',\n",
              "   'dylan',\n",
              "   'greatest',\n",
              "   'hits',\n",
              "   'vol'], ['dylan',\n",
              "   'listen',\n",
              "   'previously',\n",
              "   'rickey',\n",
              "   'greatest',\n",
              "   'hits',\n",
              "   'vol']],\n",
              " (13,\n",
              "  6435): [['edition',\n",
              "   'book',\n",
              "   'full',\n",
              "   'color',\n",
              "   'photos',\n",
              "   'quotes',\n",
              "   'invisible',\n",
              "   'circles'], ['available',\n",
              "   'eco',\n",
              "   'full',\n",
              "   'color',\n",
              "   'photos',\n",
              "   'lyrics',\n",
              "   'quotes'], ['first',\n",
              "   'available',\n",
              "   'book',\n",
              "   'full',\n",
              "   'color',\n",
              "   'photos',\n",
              "   'quotes'], ['available',\n",
              "   'book',\n",
              "   'color',\n",
              "   'photos',\n",
              "   'diary',\n",
              "   'quotes'], ['book', 'full', 'color', 'diary', 'quotes', 'invisible', 'circles'], ['edition',\n",
              "   'full',\n",
              "   'color',\n",
              "   'diary',\n",
              "   'quotes',\n",
              "   'invisible',\n",
              "   'circles'], ['available', 'color', 'photos', 'diary', 'quotes'], ['edition',\n",
              "   'available',\n",
              "   'eco',\n",
              "   'color',\n",
              "   'photos',\n",
              "   'quotes',\n",
              "   'invisible',\n",
              "   'circles'], ['available',\n",
              "   'full',\n",
              "   'color',\n",
              "   'photos',\n",
              "   'lyrics',\n",
              "   'diary'], ['available', 'full', 'color', 'photos', 'quotes']],\n",
              " (13,\n",
              "  5984): [['showcases',\n",
              "   'disc',\n",
              "   'also',\n",
              "   'footage',\n",
              "   'multi',\n",
              "   'feature',\n",
              "   'songs',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['live',\n",
              "   'set',\n",
              "   'london',\n",
              "   'perfor',\n",
              "   'mances',\n",
              "   'contains',\n",
              "   'footage',\n",
              "   'every',\n",
              "   'video',\n",
              "   'band',\n",
              "   'multi',\n",
              "   'songs',\n",
              "   'view',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['disc',\n",
              "   'concert',\n",
              "   'london',\n",
              "   'footage',\n",
              "   'angle',\n",
              "   'songs',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['also',\n",
              "   'video',\n",
              "   'multi',\n",
              "   'songs',\n",
              "   'show',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['showcases',\n",
              "   'tremendous',\n",
              "   'set',\n",
              "   'footage',\n",
              "   'band',\n",
              "   'time',\n",
              "   'angle',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['london',\n",
              "   'footage',\n",
              "   'band',\n",
              "   'time',\n",
              "   'songs',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['disc',\n",
              "   'mances',\n",
              "   'also',\n",
              "   'footage',\n",
              "   'multi',\n",
              "   'songs',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['iowa',\n",
              "   'also',\n",
              "   'video',\n",
              "   'band',\n",
              "   'released',\n",
              "   'multi',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['nine',\n",
              "   'tremendous',\n",
              "   'london',\n",
              "   'every',\n",
              "   'multi',\n",
              "   'songs',\n",
              "   'bac',\n",
              "   'show',\n",
              "   'slipknot',\n",
              "   'disasterpieces'], ['also',\n",
              "   'contains',\n",
              "   'band',\n",
              "   'released',\n",
              "   'time',\n",
              "   'songs',\n",
              "   'show',\n",
              "   'slipknot',\n",
              "   'disasterpieces']],\n",
              " (13, 8883): [['menacing', 'devil', 'thunder', 'godspeed'],\n",
              "  ['return',\n",
              "   'world',\n",
              "   'serial',\n",
              "   'rais',\n",
              "   'cradle',\n",
              "   'fueled',\n",
              "   'guitars',\n",
              "   'haunting',\n",
              "   'aggressive',\n",
              "   'devil',\n",
              "   'thunder',\n",
              "   'godspeed'],\n",
              "  ['kings',\n",
              "   'black',\n",
              "   'metal',\n",
              "   'story',\n",
              "   'serial',\n",
              "   'killer',\n",
              "   'french',\n",
              "   'joan',\n",
              "   'arc',\n",
              "   'rais',\n",
              "   'like',\n",
              "   'story',\n",
              "   'sinister',\n",
              "   'deed',\n",
              "   'filth',\n",
              "   'fueled',\n",
              "   'guitars',\n",
              "   'aggressive',\n",
              "   'piece',\n",
              "   'breakneck',\n",
              "   'devil'],\n",
              "  ['dastardly', 'piece', 'devil', 'thunder', 'godspeed'],\n",
              "  ['sinister', 'devil', 'thunder', 'godspeed'],\n",
              "  ['like', 'murder', 'devil', 'thunder', 'godspeed'],\n",
              "  ['black',\n",
              "   'arc',\n",
              "   'trademark',\n",
              "   'speed',\n",
              "   'guitars',\n",
              "   'haunting',\n",
              "   'chilling',\n",
              "   'devil',\n",
              "   'thunder',\n",
              "   'godspeed'],\n",
              "  ['albums',\n",
              "   'metal',\n",
              "   'tale',\n",
              "   'yet',\n",
              "   'first',\n",
              "   'joan',\n",
              "   'story',\n",
              "   'occult',\n",
              "   'bathory',\n",
              "   'fueled',\n",
              "   'crushing',\n",
              "   'guitars',\n",
              "   'band',\n",
              "   'aggressive',\n",
              "   'yet',\n",
              "   'breakneck',\n",
              "   'devil',\n",
              "   'godspeed'],\n",
              "  ['french',\n",
              "   'nobleman',\n",
              "   'like',\n",
              "   'bathory',\n",
              "   'aggressive',\n",
              "   'chilling',\n",
              "   'breakneck',\n",
              "   'devil',\n",
              "   'thunder',\n",
              "   'godspeed'],\n",
              "  ['return',\n",
              "   'yet',\n",
              "   'world',\n",
              "   'first',\n",
              "   'serial',\n",
              "   'killer',\n",
              "   'like',\n",
              "   'elizabeth',\n",
              "   'cradle',\n",
              "   'sinister',\n",
              "   'deed',\n",
              "   'filth',\n",
              "   'speed',\n",
              "   'crushing',\n",
              "   'guitars',\n",
              "   'vocals',\n",
              "   'hardest',\n",
              "   'aggressive',\n",
              "   'piece',\n",
              "   'yet',\n",
              "   'devil']],\n",
              " (13,\n",
              "  7130): [['past',\n",
              "   'single',\n",
              "   'skeleton',\n",
              "   'century',\n",
              "   'enemy',\n",
              "   'triumphant',\n",
              "   'machine',\n",
              "   'doomsday'], ['rebellion',\n",
              "   'throughout',\n",
              "   'nemesis',\n",
              "   'doomsday',\n",
              "   'enemy',\n",
              "   'machine',\n",
              "   'doomsday'], ['love',\n",
              "   'guitars',\n",
              "   'anything',\n",
              "   'arch',\n",
              "   'radio',\n",
              "   'skeleton',\n",
              "   'carry',\n",
              "   'scene',\n",
              "   'machine',\n",
              "   'doomsday'], ['vocalist',\n",
              "   'technically',\n",
              "   'anything',\n",
              "   'arch',\n",
              "   'metal',\n",
              "   'features',\n",
              "   'songs',\n",
              "   'apocalypse',\n",
              "   'machine',\n",
              "   'doomsday'], ['anything',\n",
              "   'couple',\n",
              "   'carry',\n",
              "   'legend',\n",
              "   'machine',\n",
              "   'doomsday'], ['superior',\n",
              "   'sound',\n",
              "   'couple',\n",
              "   'features',\n",
              "   'nemesis',\n",
              "   'skeleton',\n",
              "   'machine',\n",
              "   'doomsday'], ['vocalist',\n",
              "   'guitars',\n",
              "   'drumwork',\n",
              "   'nemesis',\n",
              "   'machine',\n",
              "   'bengtsson',\n",
              "   'machine',\n",
              "   'doomsday'], ['vocalist',\n",
              "   'anything',\n",
              "   'rebellion',\n",
              "   'come',\n",
              "   'carry',\n",
              "   'bengtsson',\n",
              "   'machine',\n",
              "   'doomsday'], ['rebellion',\n",
              "   'returned',\n",
              "   'nemesis',\n",
              "   'legend',\n",
              "   '2005',\n",
              "   'scene',\n",
              "   'machine',\n",
              "   'doomsday'], ['angela',\n",
              "   'anything',\n",
              "   'ever',\n",
              "   'got',\n",
              "   'single',\n",
              "   'skeleton',\n",
              "   'enemy',\n",
              "   'machine',\n",
              "   'doomsday']],\n",
              " (13,\n",
              "  7507): [['hot',\n",
              "   'album',\n",
              "   'arcadium',\n",
              "   'energy',\n",
              "   'one',\n",
              "   'restless',\n",
              "   'rock',\n",
              "   'hour',\n",
              "   'muscular',\n",
              "   'service',\n",
              "   'potent',\n",
              "   'tracks',\n",
              "   'top',\n",
              "   'wafting',\n",
              "   'mid',\n",
              "   'kick'], ['red',\n",
              "   'peppers',\n",
              "   'history',\n",
              "   'albums',\n",
              "   'devoid',\n",
              "   'hour',\n",
              "   'collection',\n",
              "   'characterized',\n",
              "   'fully',\n",
              "   'stark',\n",
              "   'band',\n",
              "   'psychedelia',\n",
              "   'perhaps',\n",
              "   'simmering',\n",
              "   'stadium'], ['album',\n",
              "   'sublime',\n",
              "   'passion',\n",
              "   'albums',\n",
              "   'chili',\n",
              "   'introspective',\n",
              "   'animal',\n",
              "   'tasteful',\n",
              "   'albums',\n",
              "   'focused',\n",
              "   'hooks',\n",
              "   'middle',\n",
              "   'groove'], ['chili',\n",
              "   'album',\n",
              "   'kind',\n",
              "   'funked',\n",
              "   'albums',\n",
              "   'enough',\n",
              "   'restless',\n",
              "   'rock',\n",
              "   'hiatuses',\n",
              "   'often',\n",
              "   'snow',\n",
              "   'venturing',\n",
              "   'paucity',\n",
              "   'undeniably'], ['spring',\n",
              "   'suggestive',\n",
              "   'bands',\n",
              "   'socks',\n",
              "   'followed',\n",
              "   'spell',\n",
              "   'potent',\n",
              "   '80s',\n",
              "   'rhythms',\n",
              "   'ballad',\n",
              "   'nature',\n",
              "   'challenges',\n",
              "   'arcadium'], ['stature',\n",
              "   'three',\n",
              "   'discs',\n",
              "   'mainstream',\n",
              "   'considerably',\n",
              "   'smile',\n",
              "   'back',\n",
              "   'tricky',\n",
              "   'rhythms',\n",
              "   'venturing',\n",
              "   'teacup',\n",
              "   'like',\n",
              "   'simmering',\n",
              "   'stadium'], ['band',\n",
              "   'sublime',\n",
              "   'one',\n",
              "   'socks',\n",
              "   'alternative',\n",
              "   'punk',\n",
              "   'hiatuses',\n",
              "   'peppers',\n",
              "   'reinvention',\n",
              "   'back',\n",
              "   'joe',\n",
              "   'delivered',\n",
              "   'paucity',\n",
              "   'mid',\n",
              "   'one',\n",
              "   'band',\n",
              "   'eases'], ['years',\n",
              "   'kiedis',\n",
              "   'bands',\n",
              "   'socks',\n",
              "   'funked',\n",
              "   'band',\n",
              "   'outfit',\n",
              "   'disc',\n",
              "   'sand',\n",
              "   'back',\n",
              "   'teacup',\n",
              "   'challenges',\n",
              "   'histrionics'], ['chili',\n",
              "   'passion',\n",
              "   'hot',\n",
              "   'history',\n",
              "   'simply',\n",
              "   'completely',\n",
              "   'devoid',\n",
              "   'tracks',\n",
              "   'smile',\n",
              "   'looks',\n",
              "   'back',\n",
              "   'joe',\n",
              "   'much',\n",
              "   'stadium',\n",
              "   'arcadium'], ['arcadium',\n",
              "   'subliminal',\n",
              "   'exuding',\n",
              "   'career',\n",
              "   'albums',\n",
              "   'stature',\n",
              "   'alternative',\n",
              "   'two',\n",
              "   'collection',\n",
              "   'snow',\n",
              "   'mellower',\n",
              "   'baby',\n",
              "   'double',\n",
              "   'seems']],\n",
              " (14, 6110): [['john', 'motley', 'crue'],\n",
              "  ['motley', 'crue'],\n",
              "  ['band', 'motley', 'crue'],\n",
              "  ['motley', 'crue'],\n",
              "  ['hard', 'motley', 'crue'],\n",
              "  ['band', 'motley', 'crue'],\n",
              "  ['motley', 'crue'],\n",
              "  ['cre', 'motley', 'crue'],\n",
              "  ['first', 'motley', 'crue'],\n",
              "  ['released', 'album', 'motley', 'crue']],\n",
              " (14, 0): [['first', 'case', 'intertwined', 'musicianship', 'christmas'],\n",
              "  ['orchestra', 'rock', 'keyboards', 'christmas'],\n",
              "  ['came', 'new', 'orchestra', 'christmas'],\n",
              "  ['way', 'disc', 'album', 'plenty', 'christmas', 'stories'],\n",
              "  ['christmas', 'whatever', 'keyboards', 'holiday', 'christmas'],\n",
              "  ['plus', 'new', 'christmas'],\n",
              "  ['eve', 'album', 'prog', 'occasionally', 'christmas'],\n",
              "  ['city', 'rock', 'plenty', 'synthesized', 'christmas'],\n",
              "  ['peace', 'instrumentals', 'verlinde', 'christmas'],\n",
              "  ['holiday', 'eve', 'holiday', 'verlinde', 'christmas']],\n",
              " (14,\n",
              "  5394): [['taking',\n",
              "   'middlebrow',\n",
              "   'whether',\n",
              "   'music',\n",
              "   'ahead',\n",
              "   'hardly',\n",
              "   'cause',\n",
              "   'distinctly',\n",
              "   'bon',\n",
              "   'jovi',\n",
              "   'rock',\n",
              "   'star'], ['80s',\n",
              "   'genre',\n",
              "   'feature',\n",
              "   'jason',\n",
              "   'bon',\n",
              "   'scott',\n",
              "   'soto',\n",
              "   'zakk',\n",
              "   'pollution',\n",
              "   'rock',\n",
              "   'star'], ['ohio',\n",
              "   'day',\n",
              "   'middlebrow',\n",
              "   '80s',\n",
              "   'defining',\n",
              "   'begs',\n",
              "   'suffering',\n",
              "   'thou',\n",
              "   'tap',\n",
              "   'rock',\n",
              "   'star'], ['surprisingly',\n",
              "   'virtual',\n",
              "   'title',\n",
              "   'headliners',\n",
              "   'retire',\n",
              "   'rock',\n",
              "   'star'], ['born',\n",
              "   'rock',\n",
              "   'theorists',\n",
              "   'distinctly',\n",
              "   'modern',\n",
              "   'star',\n",
              "   'rockers',\n",
              "   'everclear',\n",
              "   'vets',\n",
              "   'wylde',\n",
              "   'rock',\n",
              "   'star'], ['salesman',\n",
              "   'matter',\n",
              "   'nights',\n",
              "   'bon',\n",
              "   'steel',\n",
              "   'bon',\n",
              "   'jeff',\n",
              "   'wahlberg',\n",
              "   'heavy',\n",
              "   'rock',\n",
              "   'star'], ['star',\n",
              "   'verve',\n",
              "   'steel',\n",
              "   'film',\n",
              "   'honed',\n",
              "   'pop',\n",
              "   'performance',\n",
              "   'fans',\n",
              "   'forsaken',\n",
              "   'rock',\n",
              "   'star'], ['star',\n",
              "   'rock',\n",
              "   'merely',\n",
              "   'verve',\n",
              "   'lip',\n",
              "   'wylde',\n",
              "   'roll',\n",
              "   'rock',\n",
              "   'star'], ['story',\n",
              "   'middlebrow',\n",
              "   'music',\n",
              "   'surprisingly',\n",
              "   'something',\n",
              "   'ranges',\n",
              "   'pollution',\n",
              "   'live',\n",
              "   'please',\n",
              "   'thou',\n",
              "   'rock',\n",
              "   'star'], ['found',\n",
              "   'middlebrow',\n",
              "   'throwback',\n",
              "   'genre',\n",
              "   'giving',\n",
              "   'ranges',\n",
              "   'live',\n",
              "   'rock',\n",
              "   'star']],\n",
              " (14,\n",
              "  6777): [['orchestra',\n",
              "   'tso',\n",
              "   'standards',\n",
              "   'earth',\n",
              "   'continue',\n",
              "   'capra',\n",
              "   'neill',\n",
              "   'diverse',\n",
              "   'last',\n",
              "   'epics',\n",
              "   'christmas'], ['distinctive',\n",
              "   'lost',\n",
              "   'continue',\n",
              "   'jesus',\n",
              "   'time',\n",
              "   'touch',\n",
              "   'organ',\n",
              "   'savatage',\n",
              "   'christmas'], ['trans',\n",
              "   'entry',\n",
              "   'youngest',\n",
              "   'earth',\n",
              "   'song',\n",
              "   'features',\n",
              "   '15th',\n",
              "   'soaring',\n",
              "   'christmas'], ['long',\n",
              "   'platinum',\n",
              "   'siberian',\n",
              "   'like',\n",
              "   'dreamt',\n",
              "   'diverse',\n",
              "   'record',\n",
              "   'wisdom',\n",
              "   'christmas'], ['trans',\n",
              "   'unfinished',\n",
              "   'producer',\n",
              "   'song',\n",
              "   'touch',\n",
              "   '15th',\n",
              "   'arching',\n",
              "   'christmas'], ['trans',\n",
              "   'lost',\n",
              "   'christmas',\n",
              "   'broadway',\n",
              "   'lands',\n",
              "   'line',\n",
              "   'neill',\n",
              "   'arching',\n",
              "   'christmas'], ['christmas',\n",
              "   'complex',\n",
              "   'like',\n",
              "   'called',\n",
              "   'york',\n",
              "   'conceived',\n",
              "   'ferdinand',\n",
              "   'grandiose',\n",
              "   'alchemy',\n",
              "   'christmas'], ['unfinished',\n",
              "   'city',\n",
              "   'robert',\n",
              "   'savatage',\n",
              "   'fourteen',\n",
              "   'almost',\n",
              "   'faithful',\n",
              "   'christmas'], ['features',\n",
              "   'hard',\n",
              "   'ambitious',\n",
              "   'new',\n",
              "   'neill',\n",
              "   'oliva',\n",
              "   'swells',\n",
              "   'alchemy',\n",
              "   'soaring',\n",
              "   'litz',\n",
              "   'christmas'], ['symphony',\n",
              "   'holiday',\n",
              "   'rock',\n",
              "   'perhaps',\n",
              "   'savatage',\n",
              "   'swells',\n",
              "   'christmas',\n",
              "   'instrumentals',\n",
              "   'eve']],\n",
              " (14, 2880): [['dream'],\n",
              "  ['dream'],\n",
              "  ['dream'],\n",
              "  ['dream'],\n",
              "  ['dream'],\n",
              "  ['dream'],\n",
              "  ['dream'],\n",
              "  ['dream'],\n",
              "  ['dream'],\n",
              "  ['mind', 'dream']],\n",
              " (15,\n",
              "  485): [['biggest',\n",
              "   'epic',\n",
              "   'stride',\n",
              "   'followed',\n",
              "   'whatnot',\n",
              "   'intermittent',\n",
              "   'easy',\n",
              "   'brit',\n",
              "   'also',\n",
              "   'reflective',\n",
              "   'tracks',\n",
              "   'like',\n",
              "   'hands',\n",
              "   'ard',\n",
              "   'amp'], ['rockers',\n",
              "   'look',\n",
              "   'later',\n",
              "   'penned',\n",
              "   'life',\n",
              "   'hensley',\n",
              "   'amp',\n",
              "   'wizards'], ['album',\n",
              "   'heavy',\n",
              "   'penned',\n",
              "   'rockers',\n",
              "   'billy',\n",
              "   'amp',\n",
              "   'wizards'], ['hard', 'chorus', 'heavy', 'easy', 'amp', 'wizards'], ['easy',\n",
              "   'stride',\n",
              "   'year',\n",
              "   'david',\n",
              "   'sensitive',\n",
              "   'eavy',\n",
              "   'amp',\n",
              "   'wizards'], ['acoustic',\n",
              "   '1971',\n",
              "   'heep',\n",
              "   'brit',\n",
              "   'ken',\n",
              "   'billy',\n",
              "   'amp',\n",
              "   'wizards'], ['hitting',\n",
              "   'rainbow',\n",
              "   'first',\n",
              "   'byron',\n",
              "   'intermittent',\n",
              "   'heavy',\n",
              "   'sensitive',\n",
              "   'penned',\n",
              "   'demons',\n",
              "   'amp',\n",
              "   'wizards'], ['leaps',\n",
              "   'musical',\n",
              "   'uriah',\n",
              "   'hits',\n",
              "   'easy',\n",
              "   'amp',\n",
              "   'wizards'], ['states',\n",
              "   'stride',\n",
              "   'heep',\n",
              "   'sensitive',\n",
              "   'penned',\n",
              "   'amp',\n",
              "   'wizards'], ['hard',\n",
              "   'david',\n",
              "   'effect',\n",
              "   'penned',\n",
              "   'rockers',\n",
              "   'amp',\n",
              "   'wizards']],\n",
              " (15, 1640): [['hit', 'pie', 'fillmore'],\n",
              "  ['pie', 'fillmore'],\n",
              "  ['pie', 'fillmore'],\n",
              "  ['pie', 'fillmore'],\n",
              "  ['top', 'pie', 'fillmore'],\n",
              "  ['pie', 'fillmore'],\n",
              "  ['pie', 'fillmore'],\n",
              "  ['pie', 'fillmore'],\n",
              "  ['joining', 'pie', 'fillmore'],\n",
              "  ['humble', 'pie', 'fillmore']],\n",
              " (15, 3095): [['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['total', 'hits'],\n",
              "  ['livin', 'hits'],\n",
              "  ['strange', 'hits'],\n",
              "  ['hits'],\n",
              "  ['hits'],\n",
              "  ['hits']],\n",
              " (15, 11171): [['clark'],\n",
              "  ['clark'],\n",
              "  ['clark'],\n",
              "  ['clark'],\n",
              "  ['clark'],\n",
              "  ['clark'],\n",
              "  ['clark'],\n",
              "  ['clark'],\n",
              "  ['clark'],\n",
              "  ['clark']],\n",
              " (15, 953): [['netherlands'],\n",
              "  ['netherlands'],\n",
              "  ['netherlands'],\n",
              "  ['netherlands'],\n",
              "  ['netherlands'],\n",
              "  ['netherlands'],\n",
              "  ['netherlands'],\n",
              "  ['netherlands'],\n",
              "  ['netherlands'],\n",
              "  ['netherlands']],\n",
              " (16, 6431): [['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand'],\n",
              "  ['ferdinand', 'franz', 'ferdinand']],\n",
              " (16,\n",
              "  1349): [['important',\n",
              "   'chains',\n",
              "   'equal',\n",
              "   'band',\n",
              "   'arrangements',\n",
              "   'whole',\n",
              "   'jar',\n",
              "   'flies'], ['represents',\n",
              "   'fans',\n",
              "   'associate',\n",
              "   'alice',\n",
              "   'elegance',\n",
              "   'jar',\n",
              "   'flies'], ['1992',\n",
              "   'alice',\n",
              "   'portions',\n",
              "   'clearly',\n",
              "   'tunes',\n",
              "   'wasp',\n",
              "   'jar',\n",
              "   'flies'], ['alice',\n",
              "   'clearly',\n",
              "   'listen',\n",
              "   'tunes',\n",
              "   'jar',\n",
              "   'flies'], ['chains', 'guitar', 'riffs', 'listen', 'elegance', 'jar', 'flies'], ['alice',\n",
              "   'riffs',\n",
              "   'song',\n",
              "   'jerry',\n",
              "   'staley',\n",
              "   'jar',\n",
              "   'flies'], ['flies',\n",
              "   'step',\n",
              "   'chains',\n",
              "   'song',\n",
              "   'clearly',\n",
              "   'jar',\n",
              "   'flies'], ['definitive', 'dirt', 'fans', 'alice', 'chains', 'jar', 'flies'], ['witness',\n",
              "   'chains',\n",
              "   'sound',\n",
              "   'gorgeous',\n",
              "   'growling',\n",
              "   'bit',\n",
              "   'jar',\n",
              "   'flies'], ['alice',\n",
              "   'alice',\n",
              "   'vocals',\n",
              "   'bit',\n",
              "   'flat',\n",
              "   'metal',\n",
              "   'jar',\n",
              "   'flies']],\n",
              " (16, 2111): [['audio'],\n",
              "  ['audio'],\n",
              "  ['audio'],\n",
              "  ['audio'],\n",
              "  ['audio'],\n",
              "  ['audio'],\n",
              "  ['audio'],\n",
              "  ['audio'],\n",
              "  ['audio'],\n",
              "  ['audio']],\n",
              " (16, 3520): [['arrested', 'album', 'multi', 'freak', 'drags', 'dusk'],\n",
              "  ['surviving', 'scandinavian', 'dusk', 'welkin'],\n",
              "  ['firmly', 'band', 'pernicious', 'aggressive', 'dusk', 'welkin'],\n",
              "  ['dimensional', 'thrash', 'anthems', 'record', 'dusk'],\n",
              "  ['gothic', 'even', 'dusk'],\n",
              "  ['arrested', 'dusk', 'anthems'],\n",
              "  ['committing', 'record', 'aggressive', 'structure', 'dusk'],\n",
              "  ['force', 'pernicious', 'dusk', 'anthems'],\n",
              "  ['unlike', 'anthems', 'layered', 'dusk'],\n",
              "  ['rotted', 'burning', 'meticulously', 'dusk']],\n",
              " (16, 6310): [['delivered', 'glorious'],\n",
              "  ['glorious'],\n",
              "  ['glorious'],\n",
              "  ['glorious'],\n",
              "  ['glorious'],\n",
              "  ['iced', 'glorious'],\n",
              "  ['owens', 'glorious'],\n",
              "  ['glorious'],\n",
              "  ['glorious'],\n",
              "  ['burden', 'glorious']],\n",
              " (17,\n",
              "  2014): [['final',\n",
              "   'band',\n",
              "   'part',\n",
              "   'containing',\n",
              "   'extended',\n",
              "   'steve',\n",
              "   'lamb',\n",
              "   'lies'], ['comes',\n",
              "   'magnificent',\n",
              "   'trip',\n",
              "   'steve',\n",
              "   'hackett',\n",
              "   'genesis',\n",
              "   'lamb',\n",
              "   'lies'], ['works',\n",
              "   'trip',\n",
              "   'york',\n",
              "   'showcasing',\n",
              "   'phil',\n",
              "   'gabriel',\n",
              "   'lamb',\n",
              "   'lies'], ['gabriel',\n",
              "   'albums',\n",
              "   'kid',\n",
              "   'showcasing',\n",
              "   'tony',\n",
              "   'hackett',\n",
              "   'peter',\n",
              "   'lamb',\n",
              "   'lies'], ['works',\n",
              "   'peter',\n",
              "   '1974',\n",
              "   'epic',\n",
              "   'tony',\n",
              "   'steve',\n",
              "   'lamb',\n",
              "   'lies'], ['features',\n",
              "   'band',\n",
              "   '1973',\n",
              "   'tony',\n",
              "   'disturbing',\n",
              "   'lyrics',\n",
              "   'gabriel',\n",
              "   'album',\n",
              "   'lamb',\n",
              "   'lies'], ['peter',\n",
              "   'famous',\n",
              "   'underlying',\n",
              "   'collins',\n",
              "   'vocals',\n",
              "   'often',\n",
              "   'genesis',\n",
              "   'lamb',\n",
              "   'lies'], ['production',\n",
              "   'took',\n",
              "   'extended',\n",
              "   'lyrics',\n",
              "   'peter',\n",
              "   'gabriel',\n",
              "   'lamb',\n",
              "   'lies',\n",
              "   'broadway'], ['trip',\n",
              "   'band',\n",
              "   'city',\n",
              "   'named',\n",
              "   'containing',\n",
              "   'collins',\n",
              "   'located',\n",
              "   'truly',\n",
              "   'lamb',\n",
              "   'lies'], ['band',\n",
              "   'rival',\n",
              "   'weird',\n",
              "   'sections',\n",
              "   'showcasing',\n",
              "   'extraordinary',\n",
              "   'hackett',\n",
              "   'disturbing',\n",
              "   'lamb',\n",
              "   'lies']],\n",
              " (17, 2040): [['divorcing', 'matter', 'celebrating', 'rock', 'must'],\n",
              "  ['serve', 'blistering', 'listenable', 'must'],\n",
              "  ['certified', 'riaa', 'rock', 'collins', 'love', 'must'],\n",
              "  ['managed', 'albums', 'must'],\n",
              "  ['bit', 'history', 'must'],\n",
              "  ['platinum', 'got', 'must'],\n",
              "  ['china', 'rock', 'sort', 'going'],\n",
              "  ['fair', 'mood', 'durchholz', 'must'],\n",
              "  ['actually', 'homes', 'diatribe', 'must'],\n",
              "  ['triple', 'believe', 'diatribe', 'mood', 'must']],\n",
              " (17, 8389): [['moment'],\n",
              "  ['moment'],\n",
              "  ['moment'],\n",
              "  ['moment'],\n",
              "  ['moment'],\n",
              "  ['moment'],\n",
              "  ['moment'],\n",
              "  ['moment'],\n",
              "  ['moment'],\n",
              "  ['moment']],\n",
              " (17, 9243): [['compilation', 'best'],\n",
              "  ['compilation', '1972', 'best'],\n",
              "  ['platinum', 'everything', 'best'],\n",
              "  ['years', 'nineteen', 'best'],\n",
              "  ['two', 'best'],\n",
              "  ['years', 'reelin', 'follow', 'best'],\n",
              "  ['toured', 'best'],\n",
              "  ['70s', 'fame', 'best'],\n",
              "  ['amongst', '2000', 'best'],\n",
              "  ['culled', 'rock', 'best']],\n",
              " (17, 5637): [['loose'],\n",
              "  ['loose'],\n",
              "  ['loose'],\n",
              "  ['loose'],\n",
              "  ['loose'],\n",
              "  ['loose'],\n",
              "  ['loose'],\n",
              "  ['loose'],\n",
              "  ['loose'],\n",
              "  ['loose']],\n",
              " (21,\n",
              "  8721): [['audience',\n",
              "   'scherazade',\n",
              "   'version',\n",
              "   'russia',\n",
              "   'hard',\n",
              "   'live',\n",
              "   'carnegie',\n",
              "   'hall'], ['recordings',\n",
              "   'favorites',\n",
              "   'russia',\n",
              "   'well',\n",
              "   'sun',\n",
              "   'live',\n",
              "   'carnegie',\n",
              "   'hall'], ['actually',\n",
              "   'album',\n",
              "   'unfamiliar',\n",
              "   'features',\n",
              "   'well',\n",
              "   'understand',\n",
              "   'live',\n",
              "   'carnegie',\n",
              "   'hall'], ['audience', 'running', 'live', 'carnegie', 'hall'], ['audience',\n",
              "   'released',\n",
              "   'version',\n",
              "   'favorites',\n",
              "   'russia',\n",
              "   'ocean',\n",
              "   'gypsy',\n",
              "   'sun',\n",
              "   'hard',\n",
              "   'ashes',\n",
              "   'burning',\n",
              "   'hall'], ['ocean', 'running', 'ashes', 'live', 'carnegie', 'hall'], ['1976',\n",
              "   'gypsy',\n",
              "   'ashes',\n",
              "   'burning',\n",
              "   'live',\n",
              "   'carnegie',\n",
              "   'hall'], ['recordings',\n",
              "   'album',\n",
              "   'audience',\n",
              "   'scherazade',\n",
              "   'released',\n",
              "   '1976',\n",
              "   'concert',\n",
              "   'ashes',\n",
              "   'carnegie',\n",
              "   'hall'], ['favorites',\n",
              "   'sun',\n",
              "   'running',\n",
              "   'burning',\n",
              "   'live',\n",
              "   'carnegie',\n",
              "   'hall'], ['version', 'running', 'live', 'carnegie', 'hall']],\n",
              " (21, 1574): [['sahara', 'final', 'police', 'synchronicity'],\n",
              "  ['finger', 'suitably', 'chart', 'finger', 'synchronicity'],\n",
              "  ['synchronicity', 'murder', 'every', 'synchronicity'],\n",
              "  ['suitably', 'title', 'strokes', 'synchronicity'],\n",
              "  ['mother', 'studio', 'top', 'synchronicity'],\n",
              "  ['listings', 'synchronicity', '1983', 'pop', 'synchronicity'],\n",
              "  ['gradenko', 'wave', 'sarig', 'synchronicity'],\n",
              "  ['footsteps', 'numbers', 'police', 'new', 'top', 'synchronicity'],\n",
              "  ['synchronicity', 'breath', 'take', 'pretentious', 'synchronicity'],\n",
              "  ['breath', '1983', 'final', 'overblown', 'roni', 'synchronicity']],\n",
              " (21, 2184): [['try', 'line', 'almost', 'worth', 'deeds'],\n",
              "  ['title', 'every', 'standards', 'waiting', 'deeds'],\n",
              "  ['lyrical', 'socialite', 'winding', 'effect', 'deeds', 'cheap'],\n",
              "  ['squealer', 'however', 'deeds'],\n",
              "  ['big', 'feel', 'australians', 'album', 'deeds'],\n",
              "  ['released', 'big', 'lyrical', 'love', 'sex', 'deeds'],\n",
              "  ['balls', 'ostensibly', 'degree', 'australians', 'andrew', 'deeds'],\n",
              "  ['classic', 'plus', 'first', 'waiting', 'album', 'deeds'],\n",
              "  ['til', 'child', 'socialite', 'playing', 'deeds'],\n",
              "  ['1976', 'like', 'millionaire', 'deeds']],\n",
              " (21,\n",
              "  8156): [['feels',\n",
              "   'album',\n",
              "   'reflections',\n",
              "   'serving',\n",
              "   'album',\n",
              "   'listener',\n",
              "   'imagination',\n",
              "   'muddled',\n",
              "   'owens',\n",
              "   'bands',\n",
              "   'earth',\n",
              "   'iced',\n",
              "   'earth',\n",
              "   'barbaric',\n",
              "   'framing',\n",
              "   'fine',\n",
              "   'well',\n",
              "   'armageddon',\n",
              "   'lukewarm',\n",
              "   'served',\n",
              "   'feel',\n",
              "   'jedd',\n",
              "   'something',\n",
              "   'armageddon'], ['available',\n",
              "   'synopsis',\n",
              "   'jon',\n",
              "   'feels',\n",
              "   'album',\n",
              "   'element',\n",
              "   'sonic',\n",
              "   'reflections',\n",
              "   'execution',\n",
              "   'serving',\n",
              "   'way',\n",
              "   'album',\n",
              "   'musical',\n",
              "   'thoughtfully',\n",
              "   'like',\n",
              "   'deep',\n",
              "   'part',\n",
              "   'vocalist',\n",
              "   'bands',\n",
              "   'blind',\n",
              "   'iced',\n",
              "   'rather',\n",
              "   'well',\n",
              "   'overall',\n",
              "   'band',\n",
              "   'feel',\n",
              "   'faultlessness'], ['jon',\n",
              "   'wicked',\n",
              "   'schaffer',\n",
              "   'retread',\n",
              "   'problem',\n",
              "   'owens',\n",
              "   'something',\n",
              "   'wicked',\n",
              "   'armageddon'], ['jon',\n",
              "   'tackle',\n",
              "   'traditional',\n",
              "   'rock',\n",
              "   'album',\n",
              "   'sonic',\n",
              "   'serving',\n",
              "   'intriguing',\n",
              "   'vignettes',\n",
              "   'album',\n",
              "   'epic',\n",
              "   'schaffer',\n",
              "   'tim',\n",
              "   'fine',\n",
              "   'well',\n",
              "   'perspective',\n",
              "   'little',\n",
              "   'feel',\n",
              "   'faultlessness',\n",
              "   'something',\n",
              "   'armageddon'], ['jon',\n",
              "   'wicked',\n",
              "   'reflections',\n",
              "   'intriguing',\n",
              "   'moments',\n",
              "   'fails',\n",
              "   'executed',\n",
              "   'exciting',\n",
              "   'schaffer',\n",
              "   'power',\n",
              "   'bands',\n",
              "   'kamelot',\n",
              "   'fine',\n",
              "   'well',\n",
              "   'might',\n",
              "   'framing',\n",
              "   'something',\n",
              "   'wicked',\n",
              "   'armageddon'], ['jon',\n",
              "   'collection',\n",
              "   'symphony',\n",
              "   'traditional',\n",
              "   'element',\n",
              "   'execution',\n",
              "   'one',\n",
              "   'intriguing',\n",
              "   'atmosphere',\n",
              "   'songwriting',\n",
              "   'numerous',\n",
              "   'conceived',\n",
              "   'thoughtfully',\n",
              "   'domino',\n",
              "   'comes',\n",
              "   'deep',\n",
              "   'purple',\n",
              "   'although',\n",
              "   'fades',\n",
              "   'clouding',\n",
              "   'tim',\n",
              "   'occasional',\n",
              "   'music',\n",
              "   'sounds',\n",
              "   'power',\n",
              "   'metal',\n",
              "   'iced',\n",
              "   'reined',\n",
              "   'fine',\n",
              "   'moments',\n",
              "   'armageddon',\n",
              "   'lukewarm',\n",
              "   'band',\n",
              "   'best',\n",
              "   'served',\n",
              "   'fresh',\n",
              "   'perspective'], ['jon',\n",
              "   'wicked',\n",
              "   'story',\n",
              "   'execution',\n",
              "   'intriguing',\n",
              "   'vignettes',\n",
              "   'atmosphere',\n",
              "   'fails',\n",
              "   'guitar',\n",
              "   'domino',\n",
              "   'retread',\n",
              "   'vocalist',\n",
              "   'owens',\n",
              "   'sounds',\n",
              "   'power',\n",
              "   'bands',\n",
              "   'guardian',\n",
              "   'iced',\n",
              "   'kamelot',\n",
              "   'framing',\n",
              "   'fine',\n",
              "   'well',\n",
              "   'something',\n",
              "   'armageddon'], ['wicked',\n",
              "   'like',\n",
              "   'traditional',\n",
              "   'one',\n",
              "   'bridge',\n",
              "   'listener',\n",
              "   'despite',\n",
              "   'aside',\n",
              "   'like',\n",
              "   'blind',\n",
              "   'something',\n",
              "   'wicked',\n",
              "   'armageddon'], ['wicked',\n",
              "   'collection',\n",
              "   'feels',\n",
              "   'album',\n",
              "   'intriguing',\n",
              "   'favors',\n",
              "   'atmosphere',\n",
              "   'epic',\n",
              "   'memorable',\n",
              "   'listener',\n",
              "   'imagination',\n",
              "   'despite',\n",
              "   'although',\n",
              "   'clouding',\n",
              "   'occasional',\n",
              "   'aside',\n",
              "   'metal',\n",
              "   'earth',\n",
              "   'kamelot',\n",
              "   'fine',\n",
              "   'armageddon',\n",
              "   'something',\n",
              "   'armageddon'], ['jon',\n",
              "   'something',\n",
              "   'like',\n",
              "   'symphony',\n",
              "   'overture',\n",
              "   'one',\n",
              "   'bridge',\n",
              "   'songwriting',\n",
              "   'listener',\n",
              "   'deep',\n",
              "   'purple',\n",
              "   'exciting',\n",
              "   'retread',\n",
              "   'vocalist',\n",
              "   'sounds',\n",
              "   'like',\n",
              "   'power',\n",
              "   'reined',\n",
              "   'moments',\n",
              "   'band',\n",
              "   'might',\n",
              "   'served',\n",
              "   'beaudoin',\n",
              "   'something',\n",
              "   'armageddon']],\n",
              " (21, 7798): [['associated', 'every', 'squarely', 'guitar', 'paradise'],\n",
              "  ['said', 'designed', 'paradise'],\n",
              "  ['years', 'unrivaled', 'inhuman', 'direction', 'paradise'],\n",
              "  ['large', 'michael', 'come', 'yet', 'paradise'],\n",
              "  ['five', 'member', 'almost', 'fantastic', 'paradise'],\n",
              "  ['symphony', 'motion', 'paradise'],\n",
              "  ['release', 'pyrotechnics', 'warren', 'paradise'],\n",
              "  ['symphony', 'compositions', 'unrivaled', 'paradise'],\n",
              "  ['symphony', 'american', 'digipack', 'paradise'],\n",
              "  ['nearly', 'flanagan', 'men', 'paradise']],\n",
              " (23, 3901): [['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures'],\n",
              "  ['guilty', 'pleasures']],\n",
              " (23, 6415): [['lichtspielhaus'],\n",
              "  ['lichtspielhaus'],\n",
              "  ['lichtspielhaus'],\n",
              "  ['lichtspielhaus'],\n",
              "  ['lichtspielhaus'],\n",
              "  ['lichtspielhaus'],\n",
              "  ['lichtspielhaus'],\n",
              "  ['lichtspielhaus'],\n",
              "  ['lichtspielhaus'],\n",
              "  ['lichtspielhaus']],\n",
              " (23,\n",
              "  6374): [['baby',\n",
              "   'box',\n",
              "   'though',\n",
              "   'clearly',\n",
              "   'rock',\n",
              "   'pop',\n",
              "   'style',\n",
              "   'metal',\n",
              "   'remix',\n",
              "   'simple',\n",
              "   '1992',\n",
              "   '2003'], ['girl',\n",
              "   'boom',\n",
              "   'rarities',\n",
              "   'entirely',\n",
              "   'excellent',\n",
              "   'urgent',\n",
              "   'see',\n",
              "   'dance',\n",
              "   'featuring',\n",
              "   'singles',\n",
              "   '1992'], ['stefani',\n",
              "   'cover',\n",
              "   'joins',\n",
              "   'girlfriend',\n",
              "   'dvd',\n",
              "   'fraught',\n",
              "   'colourful',\n",
              "   'gwen',\n",
              "   'singles',\n",
              "   '1992'], ['wave',\n",
              "   'bathwater',\n",
              "   'dvds',\n",
              "   'anaheim',\n",
              "   'melodious',\n",
              "   'life',\n",
              "   'live',\n",
              "   'repeat',\n",
              "   'vocal',\n",
              "   'simple',\n",
              "   'singles',\n",
              "   '1992'], ['girl',\n",
              "   'singles',\n",
              "   'troubles',\n",
              "   'three',\n",
              "   'albums',\n",
              "   'return',\n",
              "   'box',\n",
              "   'label',\n",
              "   'speak',\n",
              "   'one',\n",
              "   '1992',\n",
              "   '2003'], ['joins',\n",
              "   'dvd',\n",
              "   'departure',\n",
              "   'prolific',\n",
              "   'almost',\n",
              "   'opening',\n",
              "   'beaming',\n",
              "   'one',\n",
              "   'singles',\n",
              "   '1992'], ['top',\n",
              "   'bathwater',\n",
              "   'box',\n",
              "   'titled',\n",
              "   'fun',\n",
              "   'easy',\n",
              "   'came',\n",
              "   'miserabilism',\n",
              "   'featuring',\n",
              "   'naturally',\n",
              "   'singles',\n",
              "   '2003'], ['wave',\n",
              "   'good',\n",
              "   'cds',\n",
              "   'far',\n",
              "   'albums',\n",
              "   'debut',\n",
              "   'eponymously',\n",
              "   'bouncing',\n",
              "   '1992',\n",
              "   '2003'], ['stefani',\n",
              "   'hits',\n",
              "   'girlfriend',\n",
              "   'hella',\n",
              "   'internal',\n",
              "   'ska',\n",
              "   'represent',\n",
              "   'fun',\n",
              "   'opening',\n",
              "   'crushing',\n",
              "   'grunge',\n",
              "   'wills',\n",
              "   'singles',\n",
              "   '1992'], ['grrl',\n",
              "   'hey',\n",
              "   'suffered',\n",
              "   'doubt',\n",
              "   'hit',\n",
              "   'eponymously',\n",
              "   'ballad',\n",
              "   'baby',\n",
              "   'simple',\n",
              "   'every',\n",
              "   'singles',\n",
              "   '1992']],\n",
              " (23, 9129): [['veckatimest'],\n",
              "  ['veckatimest'],\n",
              "  ['veckatimest'],\n",
              "  ['veckatimest'],\n",
              "  ['veckatimest'],\n",
              "  ['veckatimest'],\n",
              "  ['veckatimest'],\n",
              "  ['veckatimest'],\n",
              "  ['veckatimest'],\n",
              "  ['veckatimest']],\n",
              " (23, 7438): [['mindcrime'],\n",
              "  ['mindcrime'],\n",
              "  ['mindcrime'],\n",
              "  ['mindcrime'],\n",
              "  ['mindcrime'],\n",
              "  ['mindcrime'],\n",
              "  ['mindcrime'],\n",
              "  ['mindcrime'],\n",
              "  ['mindcrime'],\n",
              "  ['mindcrime']],\n",
              " (25,\n",
              "  4027): [['1970s',\n",
              "   'punk',\n",
              "   'rebirth',\n",
              "   'featuring',\n",
              "   'paul',\n",
              "   'following',\n",
              "   'immediately',\n",
              "   'album',\n",
              "   'charts',\n",
              "   'storming',\n",
              "   'phantom',\n",
              "   'charlotte',\n",
              "   'somewhat',\n",
              "   'pedestrian',\n",
              "   'production',\n",
              "   'catchy',\n",
              "   'harlot',\n",
              "   'iron',\n",
              "   'specifically',\n",
              "   'closing',\n",
              "   'title',\n",
              "   'track',\n",
              "   'collection',\n",
              "   'ferocity',\n",
              "   'unavoidable',\n",
              "   'iron',\n",
              "   'maiden'], ['steve',\n",
              "   'harris',\n",
              "   'late',\n",
              "   'rebirth',\n",
              "   'heavy',\n",
              "   'metal',\n",
              "   'original',\n",
              "   'following',\n",
              "   'immediately',\n",
              "   'upper',\n",
              "   'album',\n",
              "   'running',\n",
              "   'debut',\n",
              "   'somewhat',\n",
              "   'pedestrian',\n",
              "   'catchy',\n",
              "   'harlot',\n",
              "   'superficially',\n",
              "   'iron',\n",
              "   'self',\n",
              "   'metal',\n",
              "   'fretboard',\n",
              "   'completely',\n",
              "   'headbanging',\n",
              "   'iron',\n",
              "   'maiden'], ['opening',\n",
              "   'steve',\n",
              "   'harris',\n",
              "   'end',\n",
              "   'defined',\n",
              "   'late',\n",
              "   'rebirth',\n",
              "   'new',\n",
              "   'roar',\n",
              "   'iron',\n",
              "   'maiden',\n",
              "   'following',\n",
              "   'upper',\n",
              "   'reaches',\n",
              "   'album',\n",
              "   'charts',\n",
              "   'cockney',\n",
              "   'capitalized',\n",
              "   'assured',\n",
              "   'albeit',\n",
              "   'pedestrian',\n",
              "   'intricately',\n",
              "   'track',\n",
              "   'self',\n",
              "   'head',\n",
              "   'ferocity',\n",
              "   'furious',\n",
              "   'iron',\n",
              "   'maiden'], ['sonic',\n",
              "   'steve',\n",
              "   'punk',\n",
              "   'rebirth',\n",
              "   'rock',\n",
              "   'wave',\n",
              "   'british',\n",
              "   'metal',\n",
              "   'lead',\n",
              "   'vocalist',\n",
              "   'iron',\n",
              "   'enormous',\n",
              "   'grassroots',\n",
              "   'reaches',\n",
              "   'anno',\n",
              "   'storming',\n",
              "   'albeit',\n",
              "   'somewhat',\n",
              "   'production',\n",
              "   'irresistibly',\n",
              "   'catchy',\n",
              "   'intricately',\n",
              "   'element',\n",
              "   'self',\n",
              "   'ian',\n",
              "   'fortnam',\n",
              "   'iron',\n",
              "   'maiden'], ['sonic',\n",
              "   'pretty',\n",
              "   'much',\n",
              "   'late',\n",
              "   'new',\n",
              "   'heavy',\n",
              "   'traditionalism',\n",
              "   'album',\n",
              "   'capitalized',\n",
              "   'storming',\n",
              "   'free',\n",
              "   'assured',\n",
              "   'marred',\n",
              "   'production',\n",
              "   'catchy',\n",
              "   'superficially',\n",
              "   'still',\n",
              "   'iron',\n",
              "   'specifically',\n",
              "   'blazing',\n",
              "   'track',\n",
              "   'utterly',\n",
              "   'ferocity',\n",
              "   'much',\n",
              "   'unavoidable',\n",
              "   'iron',\n",
              "   'maiden'], ['opening',\n",
              "   'steve',\n",
              "   'east',\n",
              "   'pretty',\n",
              "   'much',\n",
              "   'wave',\n",
              "   'salvo',\n",
              "   'featuring',\n",
              "   'iron',\n",
              "   'band',\n",
              "   'grassroots',\n",
              "   'reaches',\n",
              "   'album',\n",
              "   'strident',\n",
              "   'opera',\n",
              "   'marred',\n",
              "   'pedestrian',\n",
              "   'maiden',\n",
              "   'closing',\n",
              "   'track',\n",
              "   'element',\n",
              "   'self',\n",
              "   'completely',\n",
              "   'whiplash',\n",
              "   'fortnam',\n",
              "   'iron',\n",
              "   'maiden'], ['steve',\n",
              "   'east',\n",
              "   'rebirth',\n",
              "   'rock',\n",
              "   'known',\n",
              "   'wave',\n",
              "   'salvo',\n",
              "   'stalwarts',\n",
              "   'roar',\n",
              "   'lead',\n",
              "   'vocalist',\n",
              "   'maiden',\n",
              "   'storming',\n",
              "   'epic',\n",
              "   'charlotte',\n",
              "   'albeit',\n",
              "   'production',\n",
              "   'intricately',\n",
              "   'roguishly',\n",
              "   'still',\n",
              "   'specifically',\n",
              "   'title',\n",
              "   'utterly',\n",
              "   'essential',\n",
              "   'self',\n",
              "   'much',\n",
              "   'unavoidable',\n",
              "   'iron',\n",
              "   'maiden'], ['sonic',\n",
              "   'end',\n",
              "   'punk',\n",
              "   'new',\n",
              "   'wave',\n",
              "   'salvo',\n",
              "   'roar',\n",
              "   'original',\n",
              "   'lead',\n",
              "   'vocalist',\n",
              "   'iron',\n",
              "   'maiden',\n",
              "   'following',\n",
              "   'immediately',\n",
              "   'upper',\n",
              "   'album',\n",
              "   'capitalized',\n",
              "   'storming',\n",
              "   'perfect',\n",
              "   'opera',\n",
              "   'debut',\n",
              "   'intricately',\n",
              "   'profane',\n",
              "   'maiden',\n",
              "   'blazing',\n",
              "   'unavoidable',\n",
              "   'ian',\n",
              "   'iron',\n",
              "   'maiden'], ['opening',\n",
              "   'sonic',\n",
              "   'east',\n",
              "   'pretty',\n",
              "   'defined',\n",
              "   'rock',\n",
              "   'known',\n",
              "   'paul',\n",
              "   'iron',\n",
              "   'band',\n",
              "   'following',\n",
              "   'album',\n",
              "   'cockney',\n",
              "   'epic',\n",
              "   'albeit',\n",
              "   'harlot',\n",
              "   'superficially',\n",
              "   'iron',\n",
              "   'specifically',\n",
              "   'track',\n",
              "   'metal',\n",
              "   'fretboard',\n",
              "   'ferocity',\n",
              "   'unavoidable',\n",
              "   'fortnam',\n",
              "   'iron',\n",
              "   'maiden']],\n",
              " (25,\n",
              "  5653): [['length',\n",
              "   'links',\n",
              "   'slip',\n",
              "   'throughout',\n",
              "   'stadium',\n",
              "   'substance',\n",
              "   'maiden',\n",
              "   'effectively',\n",
              "   'emphasized',\n",
              "   'morbid',\n",
              "   'entire',\n",
              "   'hills',\n",
              "   'acknowledged',\n",
              "   'number',\n",
              "   'beast',\n",
              "   'enhanced'], ['band',\n",
              "   'run',\n",
              "   'total',\n",
              "   'metal',\n",
              "   'spectacle',\n",
              "   'hallowed',\n",
              "   'damning',\n",
              "   'emphasized',\n",
              "   'morbid',\n",
              "   'flat',\n",
              "   'number',\n",
              "   'beast',\n",
              "   'enhanced'], ['version',\n",
              "   'run',\n",
              "   'name',\n",
              "   'damning',\n",
              "   'subordinated',\n",
              "   'ranks',\n",
              "   'maiden',\n",
              "   'track',\n",
              "   'gangland',\n",
              "   'number',\n",
              "   'beast',\n",
              "   'enhanced'], ['rom',\n",
              "   'full',\n",
              "   'biographies',\n",
              "   'slip',\n",
              "   'track',\n",
              "   'listing',\n",
              "   'acacia',\n",
              "   'eclipse9',\n",
              "   'northern',\n",
              "   'pubs',\n",
              "   'typical',\n",
              "   'metal',\n",
              "   'defining',\n",
              "   'title',\n",
              "   'overshadowed',\n",
              "   'many',\n",
              "   'gangland',\n",
              "   'hallowed',\n",
              "   'apotheosis',\n",
              "   'number',\n",
              "   'beast',\n",
              "   'enhanced'], ['original',\n",
              "   'includes',\n",
              "   'galleries',\n",
              "   'slip',\n",
              "   'encased',\n",
              "   'children',\n",
              "   'acacia',\n",
              "   'number',\n",
              "   'gangland',\n",
              "   'total',\n",
              "   'eclipse9',\n",
              "   'thy',\n",
              "   'loosely',\n",
              "   'meets',\n",
              "   'states',\n",
              "   'hallowed',\n",
              "   'outgrowth',\n",
              "   'subordinated',\n",
              "   'iron',\n",
              "   'effort',\n",
              "   'effectively',\n",
              "   'emphasized',\n",
              "   'approach',\n",
              "   'morbid',\n",
              "   'among',\n",
              "   'one',\n",
              "   'moments',\n",
              "   'entire',\n",
              "   'aficionados',\n",
              "   'nine',\n",
              "   'track',\n",
              "   'immediately',\n",
              "   'gangland',\n",
              "   'hallowed',\n",
              "   'number',\n",
              "   'beast',\n",
              "   'enhanced'], ['length',\n",
              "   'number',\n",
              "   'eclipse9',\n",
              "   'spectacle',\n",
              "   'generalization',\n",
              "   'album',\n",
              "   'flat',\n",
              "   'thy',\n",
              "   'andrew',\n",
              "   'number',\n",
              "   'beast',\n",
              "   'enhanced']],\n",
              " (25, 945): [['wish'],\n",
              "  ['wish'],\n",
              "  ['wish'],\n",
              "  ['wish'],\n",
              "  ['wish'],\n",
              "  ['wish'],\n",
              "  ['wish'],\n",
              "  ['wish'],\n",
              "  ['wish'],\n",
              "  ['wish']],\n",
              " (25,\n",
              "  3993): [['bears',\n",
              "   'towers',\n",
              "   'proving',\n",
              "   'helloween',\n",
              "   'coloring',\n",
              "   'gates',\n",
              "   'chemical'], ['roy',\n",
              "   'along',\n",
              "   'many',\n",
              "   'need',\n",
              "   'william',\n",
              "   'chemical'], ['career', 'many', 'others', 'chemical'], ['bears',\n",
              "   'album',\n",
              "   'maiden',\n",
              "   'majestic',\n",
              "   'psychedelic',\n",
              "   'chemical'], ['mix',\n",
              "   'chemical',\n",
              "   'mid',\n",
              "   'late',\n",
              "   'need',\n",
              "   'chemical'], ['former', 'mid', 'priest', 'evidenced', 'chemical'], ['chemical',\n",
              "   'collaborates',\n",
              "   'smith',\n",
              "   'serving',\n",
              "   'music',\n",
              "   'chock',\n",
              "   'understand',\n",
              "   'chemical'], ['solo', 'epic', 'maiden', 'tracks', 'chemical'], ['recent',\n",
              "   'many',\n",
              "   'releases',\n",
              "   'portfolio',\n",
              "   'chemical'], ['bruce', 'axeman', 'towers', 'metal', 'reesman', 'chemical']],\n",
              " (25,\n",
              "  4003): [['good',\n",
              "   'even',\n",
              "   'however',\n",
              "   'decadent',\n",
              "   'modern',\n",
              "   'class',\n",
              "   'size'], ['sounding', 'places', 'dynamic', 'like', 'rock', 'sounds', 'size'], ['band',\n",
              "   'era',\n",
              "   'glittery',\n",
              "   'indulgent',\n",
              "   'melodramatic',\n",
              "   'jon'], ['limited',\n",
              "   'strong',\n",
              "   'mechanical',\n",
              "   'david',\n",
              "   'bowie',\n",
              "   'aladdin'], ['strong', 'grind', 'friendly', 'indulgent', 'jon'], ['metal',\n",
              "   'strong',\n",
              "   'animals',\n",
              "   'self',\n",
              "   'david',\n",
              "   'jon'], ['problem', 'glam', 'indulgent', 'class', 'medium'], ['manson',\n",
              "   'vocal',\n",
              "   'speed',\n",
              "   'user',\n",
              "   'rex',\n",
              "   'size'], ['superstar',\n",
              "   'great',\n",
              "   'pop',\n",
              "   'rock',\n",
              "   'display',\n",
              "   'boisterous',\n",
              "   'jon'], ['rom', 'question', 'tuneful', 'era', 'size']],\n",
              " (26,\n",
              "  11068): [['judas',\n",
              "   'guitarists',\n",
              "   'richie',\n",
              "   'drummer',\n",
              "   'judas',\n",
              "   'tipton',\n",
              "   'redeemer',\n",
              "   'halford',\n",
              "   'bursting',\n",
              "   'priest',\n",
              "   'breaking',\n",
              "   'another',\n",
              "   'triumph',\n",
              "   'souls',\n",
              "   'redeemer'], ['mean',\n",
              "   'priest',\n",
              "   'first',\n",
              "   'onto',\n",
              "   'judas',\n",
              "   'responsible',\n",
              "   'steel',\n",
              "   '1982',\n",
              "   'anthems',\n",
              "   'trailblazing',\n",
              "   'eager',\n",
              "   'nostradamus',\n",
              "   'combination',\n",
              "   'souls',\n",
              "   'redeemer'], ['judas',\n",
              "   'wide',\n",
              "   'richie',\n",
              "   'collection',\n",
              "   'priest',\n",
              "   'along',\n",
              "   'judas',\n",
              "   'screaming',\n",
              "   'instantly',\n",
              "   'painkiller',\n",
              "   'souls',\n",
              "   'redeemer'], ['number',\n",
              "   'evidenced',\n",
              "   'tipton',\n",
              "   'redeemer',\n",
              "   'halford',\n",
              "   'first',\n",
              "   'metal',\n",
              "   'look',\n",
              "   'studs',\n",
              "   'album',\n",
              "   'statements',\n",
              "   'souls',\n",
              "   'redeemer'], ['judas',\n",
              "   'select',\n",
              "   'judas',\n",
              "   'bands',\n",
              "   'singer',\n",
              "   'travis',\n",
              "   'metal',\n",
              "   'evidenced',\n",
              "   'redeemer',\n",
              "   'tipton',\n",
              "   'albums',\n",
              "   'law',\n",
              "   'thing',\n",
              "   'souls',\n",
              "   'redeemer'], ['priest',\n",
              "   'way',\n",
              "   'bands',\n",
              "   'priest',\n",
              "   'served',\n",
              "   'souls',\n",
              "   'epic',\n",
              "   'edition',\n",
              "   'bonus',\n",
              "   'souls',\n",
              "   'judas',\n",
              "   'priest',\n",
              "   'latest',\n",
              "   'metal',\n",
              "   'priest',\n",
              "   'damned',\n",
              "   'slamming',\n",
              "   'band',\n",
              "   'tipton',\n",
              "   'redeemer',\n",
              "   'redeemer',\n",
              "   'leadoff',\n",
              "   'redeemer',\n",
              "   'also',\n",
              "   'richie',\n",
              "   'studio',\n",
              "   'material',\n",
              "   'bursting',\n",
              "   'onto',\n",
              "   'priest',\n",
              "   '1980',\n",
              "   'living',\n",
              "   'identifiable',\n",
              "   'band',\n",
              "   'fans',\n",
              "   'world',\n",
              "   'hear',\n",
              "   'latest',\n",
              "   'love',\n",
              "   'raging',\n",
              "   'guitar',\n",
              "   'bass',\n",
              "   'souls',\n",
              "   'redeemer'], ['well',\n",
              "   'ian',\n",
              "   '2014',\n",
              "   'consistent',\n",
              "   'style',\n",
              "   'along',\n",
              "   'breaking',\n",
              "   'well',\n",
              "   'identifiable',\n",
              "   'latest',\n",
              "   'triumph',\n",
              "   'guitar',\n",
              "   'souls',\n",
              "   'redeemer'], ['judas',\n",
              "   'priest',\n",
              "   'bands',\n",
              "   'richie',\n",
              "   '15th',\n",
              "   'records',\n",
              "   'version',\n",
              "   'deluxe',\n",
              "   'judas',\n",
              "   'ground',\n",
              "   'redeemer',\n",
              "   'british',\n",
              "   '1990',\n",
              "   'released',\n",
              "   'create',\n",
              "   'souls',\n",
              "   'redeemer'], ['17th',\n",
              "   'album',\n",
              "   'judas',\n",
              "   'records',\n",
              "   'standard',\n",
              "   'bonus',\n",
              "   'album',\n",
              "   'marks',\n",
              "   'material',\n",
              "   'glenn',\n",
              "   'metal',\n",
              "   'undoubtedly',\n",
              "   'starved',\n",
              "   'souls',\n",
              "   'redeemer'], ['travis',\n",
              "   'metal',\n",
              "   'perfectly',\n",
              "   'pedal',\n",
              "   'passion',\n",
              "   'redeemer',\n",
              "   '1974',\n",
              "   'heavy',\n",
              "   'metal',\n",
              "   'great',\n",
              "   'albums',\n",
              "   'british',\n",
              "   '1990',\n",
              "   'thing',\n",
              "   'riffs',\n",
              "   'souls',\n",
              "   'redeemer']],\n",
              " (26, 5657): [['hammered'],\n",
              "  ['hammered'],\n",
              "  ['hammered'],\n",
              "  ['hammered'],\n",
              "  ['hammered'],\n",
              "  ['hammered'],\n",
              "  ['hammered'],\n",
              "  ['hammered'],\n",
              "  ['hammered'],\n",
              "  ['hammered']],\n",
              " (26,\n",
              "  7694): [['features',\n",
              "   'recorded',\n",
              "   'live',\n",
              "   'radio',\n",
              "   'disc',\n",
              "   'one',\n",
              "   'concerts',\n",
              "   '20th',\n",
              "   'tour',\n",
              "   'radio',\n",
              "   'hall',\n",
              "   'band',\n",
              "   'mike',\n",
              "   'three',\n",
              "   'one',\n",
              "   'portnoy',\n",
              "   'consider',\n",
              "   'one',\n",
              "   'floyd',\n",
              "   'charts',\n",
              "   'doubt',\n",
              "   'win',\n",
              "   'win',\n",
              "   'scenario',\n",
              "   'everyone',\n",
              "   'years',\n",
              "   'bold',\n",
              "   'power',\n",
              "   'forces',\n",
              "   'compromise',\n",
              "   'creative',\n",
              "   'vision',\n",
              "   'perseverance',\n",
              "   'score',\n",
              "   'abundance',\n",
              "   'crossover',\n",
              "   'carries',\n",
              "   'highlight',\n",
              "   'pristine',\n",
              "   'concert',\n",
              "   'little',\n",
              "   'best',\n",
              "   'remembered',\n",
              "   'six',\n",
              "   'degrees',\n",
              "   'dropping',\n",
              "   'encore',\n",
              "   'mixed',\n",
              "   'digital',\n",
              "   'equally',\n",
              "   'crisp',\n",
              "   'given',\n",
              "   'ample',\n",
              "   'synthesizer',\n",
              "   'awesome',\n",
              "   'bass',\n",
              "   'john',\n",
              "   'whose',\n",
              "   'always',\n",
              "   'transcendent',\n",
              "   'directed',\n",
              "   'score',\n",
              "   'totally',\n",
              "   'covering',\n",
              "   'angles',\n",
              "   'every',\n",
              "   'transition',\n",
              "   'advantageous',\n",
              "   'score',\n",
              "   'introduction',\n",
              "   'ever',\n",
              "   'converted',\n",
              "   'dvd',\n",
              "   'score',\n",
              "   'fully',\n",
              "   'theater',\n",
              "   'history',\n",
              "   'days',\n",
              "   'boston',\n",
              "   'college',\n",
              "   'concert',\n",
              "   '2006',\n",
              "   'myung',\n",
              "   'bands',\n",
              "   'always',\n",
              "   'film',\n",
              "   'tribute',\n",
              "   'wonder',\n",
              "   'bandmates',\n",
              "   'concert',\n",
              "   'live',\n",
              "   'performances',\n",
              "   'ability',\n",
              "   'image',\n",
              "   'times',\n",
              "   'shannon',\n",
              "   'octavarium'], ['recorded',\n",
              "   'city',\n",
              "   'music',\n",
              "   'hall',\n",
              "   '2006',\n",
              "   'disc',\n",
              "   'concert',\n",
              "   'features',\n",
              "   'concerts',\n",
              "   'ever',\n",
              "   'world',\n",
              "   'dvd',\n",
              "   'city',\n",
              "   'mike',\n",
              "   'ends',\n",
              "   'one',\n",
              "   'moment',\n",
              "   'floyd',\n",
              "   'pulse',\n",
              "   'spot',\n",
              "   'dvd',\n",
              "   'doubt',\n",
              "   'loyal',\n",
              "   'bestselling',\n",
              "   'scenario',\n",
              "   'sheer',\n",
              "   'forces',\n",
              "   'dream',\n",
              "   'unique',\n",
              "   'creative',\n",
              "   'octivarium',\n",
              "   'fickle',\n",
              "   'abundance',\n",
              "   'exclusive',\n",
              "   'minimal',\n",
              "   'crossover',\n",
              "   'theater',\n",
              "   'spirit',\n",
              "   'carries',\n",
              "   'highlight',\n",
              "   'vocals',\n",
              "   'throughout',\n",
              "   'doubt',\n",
              "   'six',\n",
              "   'accompanied',\n",
              "   'baton',\n",
              "   'arranger',\n",
              "   'conductor',\n",
              "   'epics',\n",
              "   'spectacularly',\n",
              "   'ample',\n",
              "   'several',\n",
              "   'steel',\n",
              "   'guitar',\n",
              "   'must',\n",
              "   'never',\n",
              "   'john',\n",
              "   'pcm',\n",
              "   'transcendent',\n",
              "   'score',\n",
              "   'totally',\n",
              "   'music',\n",
              "   'gracefully',\n",
              "   'covering',\n",
              "   'variety',\n",
              "   'visually',\n",
              "   'angles',\n",
              "   'chris',\n",
              "   'beat',\n",
              "   'floyd',\n",
              "   'overall',\n",
              "   'fans',\n",
              "   'introduction',\n",
              "   'ever',\n",
              "   'growing',\n",
              "   'converted',\n",
              "   'fans',\n",
              "   'score',\n",
              "   'theater',\n",
              "   'met',\n",
              "   'concert',\n",
              "   'april',\n",
              "   'myung',\n",
              "   'personnel',\n",
              "   'addressed',\n",
              "   'bands',\n",
              "   'past',\n",
              "   'given',\n",
              "   'band',\n",
              "   'bow',\n",
              "   'commercial',\n",
              "   'pressures',\n",
              "   'mike',\n",
              "   'feels',\n",
              "   'also',\n",
              "   'concert',\n",
              "   'three',\n",
              "   'performances',\n",
              "   '1993',\n",
              "   '2002',\n",
              "   '2005',\n",
              "   'illustrate',\n",
              "   'refine',\n",
              "   'redefine',\n",
              "   'jeff'], ['features',\n",
              "   'grand',\n",
              "   'concert',\n",
              "   'live',\n",
              "   'city',\n",
              "   'footage',\n",
              "   'concert',\n",
              "   'bonus',\n",
              "   'one',\n",
              "   'finest',\n",
              "   'concerts',\n",
              "   'dvd',\n",
              "   'tenacious',\n",
              "   'metal',\n",
              "   'new',\n",
              "   'hall',\n",
              "   'april',\n",
              "   'drummer',\n",
              "   'founder',\n",
              "   'show',\n",
              "   'nights',\n",
              "   'prog',\n",
              "   'consider',\n",
              "   'brief',\n",
              "   'mid',\n",
              "   'spot',\n",
              "   'music',\n",
              "   'proving',\n",
              "   'base',\n",
              "   'willing',\n",
              "   'band',\n",
              "   'everyone',\n",
              "   'obvious',\n",
              "   'performance',\n",
              "   'represents',\n",
              "   'sheer',\n",
              "   'talents',\n",
              "   'repeatedly',\n",
              "   'failed',\n",
              "   'theater',\n",
              "   'perseverance',\n",
              "   'material',\n",
              "   'exclusive',\n",
              "   'minimal',\n",
              "   'dvds',\n",
              "   'concert',\n",
              "   'labrie',\n",
              "   'gig',\n",
              "   'best',\n",
              "   'metropolis',\n",
              "   'accompanied',\n",
              "   'arranger',\n",
              "   'epics',\n",
              "   'octivarium',\n",
              "   'jamshied',\n",
              "   'sharifi',\n",
              "   'including',\n",
              "   'john',\n",
              "   'proving',\n",
              "   'lap',\n",
              "   'steel',\n",
              "   'amazing',\n",
              "   'one',\n",
              "   'never',\n",
              "   'always',\n",
              "   'apparent',\n",
              "   'however',\n",
              "   'focused',\n",
              "   'music',\n",
              "   'angles',\n",
              "   'chris',\n",
              "   'unobtrusively',\n",
              "   'breathtaking',\n",
              "   'score',\n",
              "   'par',\n",
              "   'pulse',\n",
              "   'floyd',\n",
              "   'dvd',\n",
              "   'introduction',\n",
              "   'converted',\n",
              "   'dvd',\n",
              "   'fully',\n",
              "   'boston',\n",
              "   'music',\n",
              "   'city',\n",
              "   'petrucci',\n",
              "   'personnel',\n",
              "   'hide',\n",
              "   'difficulties',\n",
              "   'tribute',\n",
              "   'sense',\n",
              "   'vindication',\n",
              "   'bandmates',\n",
              "   'shown',\n",
              "   'performances',\n",
              "   '2005',\n",
              "   'sound',\n",
              "   'octivarium',\n",
              "   'jeff'], ['concert',\n",
              "   '20th',\n",
              "   'recorded',\n",
              "   'live',\n",
              "   'city',\n",
              "   'features',\n",
              "   'odyssey',\n",
              "   'dream',\n",
              "   'theater',\n",
              "   'finest',\n",
              "   'magnificent',\n",
              "   'score',\n",
              "   'anniversary',\n",
              "   'tour',\n",
              "   'performance',\n",
              "   'recorded',\n",
              "   'music',\n",
              "   'believe',\n",
              "   'drummer',\n",
              "   'nearly',\n",
              "   'one',\n",
              "   'nights',\n",
              "   'portnoy',\n",
              "   'brief',\n",
              "   'september',\n",
              "   'pulse',\n",
              "   'dvd',\n",
              "   'proving',\n",
              "   'doubt',\n",
              "   'base',\n",
              "   'band',\n",
              "   'commercial',\n",
              "   'elevate',\n",
              "   'scenario',\n",
              "   'obvious',\n",
              "   'score',\n",
              "   'combined',\n",
              "   'industry',\n",
              "   'octivarium',\n",
              "   'perseverance',\n",
              "   'longtime',\n",
              "   'offers',\n",
              "   'dream',\n",
              "   'natural',\n",
              "   'highlight',\n",
              "   'vocals',\n",
              "   'concert',\n",
              "   'jaw',\n",
              "   'dropping',\n",
              "   'accompanied',\n",
              "   'octivarium',\n",
              "   'arrangements',\n",
              "   'crisp',\n",
              "   'opportunity',\n",
              "   'demonstrate',\n",
              "   'solos',\n",
              "   'guitarist',\n",
              "   'jordan',\n",
              "   'mastery',\n",
              "   'lap',\n",
              "   'guitar',\n",
              "   'continuum',\n",
              "   'pcm',\n",
              "   'directed',\n",
              "   'however',\n",
              "   'music',\n",
              "   'visually',\n",
              "   'angles',\n",
              "   'beat',\n",
              "   'precision',\n",
              "   'osterhus',\n",
              "   'score',\n",
              "   'pulse',\n",
              "   'fans',\n",
              "   'dream',\n",
              "   'days',\n",
              "   'met',\n",
              "   'concert',\n",
              "   'band',\n",
              "   'refusal',\n",
              "   'pressures',\n",
              "   'mike',\n",
              "   'feels',\n",
              "   'sense',\n",
              "   '1993',\n",
              "   '2002',\n",
              "   '2005',\n",
              "   'sound',\n",
              "   'keeping',\n",
              "   'jeff',\n",
              "   'octavarium'], ['grand',\n",
              "   'radio',\n",
              "   'disc',\n",
              "   'footage',\n",
              "   'concert',\n",
              "   'dream',\n",
              "   'finest',\n",
              "   'anniversary',\n",
              "   'world',\n",
              "   'culminates',\n",
              "   'blazing',\n",
              "   'new',\n",
              "   '2006',\n",
              "   'easy',\n",
              "   'drummer',\n",
              "   'ends',\n",
              "   'nights',\n",
              "   'prog',\n",
              "   'consider',\n",
              "   'one',\n",
              "   'shining',\n",
              "   'floyd',\n",
              "   'pulse',\n",
              "   'dvd',\n",
              "   'proving',\n",
              "   'doubt',\n",
              "   'fan',\n",
              "   'base',\n",
              "   'willing',\n",
              "   'level',\n",
              "   'critical',\n",
              "   'elevate',\n",
              "   'everyone',\n",
              "   'represents',\n",
              "   'combined',\n",
              "   'forces',\n",
              "   'repeatedly',\n",
              "   'vision',\n",
              "   'longtime',\n",
              "   'release',\n",
              "   'carries',\n",
              "   'labrie',\n",
              "   'flawless',\n",
              "   'performances',\n",
              "   'metropolis',\n",
              "   'orchestra',\n",
              "   'arranger',\n",
              "   'octivarium',\n",
              "   'jamshied',\n",
              "   'beautifully',\n",
              "   'equally',\n",
              "   'dream',\n",
              "   'ample',\n",
              "   'demonstrate',\n",
              "   'mastery',\n",
              "   'steel',\n",
              "   'synthesizer',\n",
              "   'curiously',\n",
              "   'continuum',\n",
              "   'must',\n",
              "   'bass',\n",
              "   'john',\n",
              "   'contribution',\n",
              "   'apparent',\n",
              "   'pcm',\n",
              "   'virtuoso',\n",
              "   'transcendent',\n",
              "   'underestimate',\n",
              "   'directed',\n",
              "   'score',\n",
              "   'gracefully',\n",
              "   'transition',\n",
              "   'solo',\n",
              "   'precision',\n",
              "   'advantageous',\n",
              "   'pulse',\n",
              "   'floyd',\n",
              "   'dvd',\n",
              "   'surpasses',\n",
              "   'ever',\n",
              "   'audience',\n",
              "   'new',\n",
              "   'converted',\n",
              "   'fans',\n",
              "   'godsend',\n",
              "   'jeff',\n",
              "   'dvd',\n",
              "   'documentary',\n",
              "   'fully',\n",
              "   'chronicles',\n",
              "   'dream',\n",
              "   'met',\n",
              "   'april',\n",
              "   'portnoy',\n",
              "   'berklee',\n",
              "   'difficulties',\n",
              "   'given',\n",
              "   'band',\n",
              "   'refusal',\n",
              "   'wonder',\n",
              "   'sense',\n",
              "   'fully',\n",
              "   'portnoy',\n",
              "   'also',\n",
              "   'shown',\n",
              "   '2002',\n",
              "   '2005',\n",
              "   'dream',\n",
              "   'times',\n",
              "   'refine',\n",
              "   'jeff',\n",
              "   'octavarium']],\n",
              " (26,\n",
              "  4004): [['gene',\n",
              "   'members',\n",
              "   'title',\n",
              "   'cooper',\n",
              "   'one',\n",
              "   'mere',\n",
              "   '158',\n",
              "   'circus'], ['circus', 'stanley', 'tar', 'circus'], ['original',\n",
              "   'reunites',\n",
              "   'tour',\n",
              "   'paul',\n",
              "   'harmony',\n",
              "   'noise',\n",
              "   'state',\n",
              "   'rock',\n",
              "   'booklet',\n",
              "   'circus'], ['band',\n",
              "   'band',\n",
              "   'original',\n",
              "   'bon',\n",
              "   'aerosmith',\n",
              "   'open',\n",
              "   'beast',\n",
              "   'size',\n",
              "   'circus'], ['studio',\n",
              "   'criss',\n",
              "   'album',\n",
              "   'roar',\n",
              "   'unkillable',\n",
              "   'noise',\n",
              "   'green',\n",
              "   'circus'], ['stanley',\n",
              "   'offers',\n",
              "   'aerosmith',\n",
              "   'tour',\n",
              "   'roar',\n",
              "   'since',\n",
              "   'acoustic',\n",
              "   'like',\n",
              "   'circus'], ['original',\n",
              "   'produced',\n",
              "   'additions',\n",
              "   'alice',\n",
              "   'hawks',\n",
              "   'billy',\n",
              "   'circus'], ['gene',\n",
              "   'created',\n",
              "   'internet',\n",
              "   'tour',\n",
              "   'talk',\n",
              "   'studio',\n",
              "   'alice',\n",
              "   'circus'], ['stanley',\n",
              "   'created',\n",
              "   'roar',\n",
              "   'simmons',\n",
              "   'back',\n",
              "   'youth',\n",
              "   'dressed',\n",
              "   'state',\n",
              "   'booklet',\n",
              "   'circus'], ['original',\n",
              "   'peter',\n",
              "   'frehley',\n",
              "   'album',\n",
              "   'aerosmith',\n",
              "   'open',\n",
              "   'little',\n",
              "   'surprise',\n",
              "   'say',\n",
              "   'altman',\n",
              "   'circus']],\n",
              " (4,\n",
              "  8040): [['along',\n",
              "   'england',\n",
              "   'usual',\n",
              "   'patrick',\n",
              "   'williams',\n",
              "   'guitar',\n",
              "   'sticks',\n",
              "   'kiddies',\n",
              "   'tension',\n",
              "   'sound',\n",
              "   'deep',\n",
              "   'track',\n",
              "   'williams',\n",
              "   'gets',\n",
              "   'listening',\n",
              "   'elisabeth',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'order'], ['nicholas',\n",
              "   'soundtrack',\n",
              "   'phoenix',\n",
              "   'along',\n",
              "   'potter',\n",
              "   'low',\n",
              "   'manner',\n",
              "   'another',\n",
              "   'gets',\n",
              "   'rather',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'order'], ['featuring',\n",
              "   'colorful',\n",
              "   'soundtrack',\n",
              "   'movie',\n",
              "   'gets',\n",
              "   'new',\n",
              "   'succeeds',\n",
              "   'true',\n",
              "   'feels',\n",
              "   'elsewhere',\n",
              "   'feels',\n",
              "   'belongs',\n",
              "   'becomes',\n",
              "   'listening',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'order'], ['harry',\n",
              "   'takes',\n",
              "   'friends',\n",
              "   'potter',\n",
              "   'composer',\n",
              "   'collaborator',\n",
              "   'new',\n",
              "   'david',\n",
              "   'modern',\n",
              "   'effects',\n",
              "   'score',\n",
              "   'goes',\n",
              "   'use',\n",
              "   'menacing',\n",
              "   'manner',\n",
              "   'feels',\n",
              "   'becomes',\n",
              "   'harry',\n",
              "   'order',\n",
              "   'phoenix'], ['featuring',\n",
              "   'new',\n",
              "   'director',\n",
              "   'evil',\n",
              "   'school',\n",
              "   'format',\n",
              "   'unease',\n",
              "   'atmosphere',\n",
              "   'recognize',\n",
              "   'another',\n",
              "   'hedwig',\n",
              "   'series',\n",
              "   'beautiful',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'order'], ['order',\n",
              "   'doyle',\n",
              "   'listener',\n",
              "   'hooper',\n",
              "   'unease',\n",
              "   'dread',\n",
              "   'hits',\n",
              "   'hooper',\n",
              "   'harry',\n",
              "   'order',\n",
              "   'phoenix'], ['nicholas',\n",
              "   'hooper',\n",
              "   'soundtrack',\n",
              "   'order',\n",
              "   'next',\n",
              "   'along',\n",
              "   'yates',\n",
              "   'know',\n",
              "   'growing',\n",
              "   'spreading',\n",
              "   'works',\n",
              "   'within',\n",
              "   'old',\n",
              "   'sticks',\n",
              "   'effects',\n",
              "   'kiddies',\n",
              "   'goes',\n",
              "   'unease',\n",
              "   'low',\n",
              "   'feels',\n",
              "   'feels',\n",
              "   'storm',\n",
              "   'hooper',\n",
              "   'series',\n",
              "   'darker',\n",
              "   'goes',\n",
              "   'listening',\n",
              "   'beautiful',\n",
              "   'spoils',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'phoenix'], ['along',\n",
              "   'cinematic',\n",
              "   'doyle',\n",
              "   'john',\n",
              "   'track',\n",
              "   'growing',\n",
              "   'format',\n",
              "   'humming',\n",
              "   'feels',\n",
              "   'effective',\n",
              "   'dementors',\n",
              "   'deceptively',\n",
              "   'harry',\n",
              "   'order',\n",
              "   'phoenix'], ['harry',\n",
              "   'order',\n",
              "   'fifth',\n",
              "   'composer',\n",
              "   'track',\n",
              "   'modern',\n",
              "   'use',\n",
              "   'deceptively',\n",
              "   'recognize',\n",
              "   'like',\n",
              "   'overall',\n",
              "   'provide',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'phoenix']],\n",
              " (26,\n",
              "  8398): [['featuring',\n",
              "   'extensive',\n",
              "   'interviews',\n",
              "   'page',\n",
              "   'plant',\n",
              "   'films',\n",
              "   'two',\n",
              "   'book',\n",
              "   'definitive',\n",
              "   'music',\n",
              "   'zeppelin',\n",
              "   'bonham',\n",
              "   'newly',\n",
              "   'released',\n",
              "   'interviews',\n",
              "   'recorded',\n",
              "   'complete',\n",
              "   'led',\n",
              "   'zeppelin',\n",
              "   'band',\n",
              "   'critics',\n",
              "   'features',\n",
              "   'track',\n",
              "   'every',\n",
              "   'film',\n",
              "   'ever',\n",
              "   'led',\n",
              "   'zeppelin',\n",
              "   'celebration',\n",
              "   'inside',\n",
              "   'story'], ['page',\n",
              "   'alongside',\n",
              "   'recordings',\n",
              "   'zeppelin',\n",
              "   'dvd',\n",
              "   'set',\n",
              "   'definitive',\n",
              "   'along',\n",
              "   '1977',\n",
              "   'history',\n",
              "   'led',\n",
              "   'words',\n",
              "   'band',\n",
              "   'features',\n",
              "   'track',\n",
              "   'analysis',\n",
              "   'every',\n",
              "   'recording',\n",
              "   'film',\n",
              "   'ever',\n",
              "   'led',\n",
              "   'zeppelin',\n",
              "   'celebration',\n",
              "   'day',\n",
              "   'inside',\n",
              "   'story'], ['jones',\n",
              "   'performance',\n",
              "   'two',\n",
              "   'book',\n",
              "   'critical',\n",
              "   'led',\n",
              "   'zeppelin',\n",
              "   'archive',\n",
              "   '1977',\n",
              "   'tour',\n",
              "   'book',\n",
              "   'complete',\n",
              "   'words',\n",
              "   'band',\n",
              "   'features',\n",
              "   'track',\n",
              "   'led',\n",
              "   'zeppelin',\n",
              "   'celebration',\n",
              "   'day',\n",
              "   'inside',\n",
              "   'story'], ['extensive',\n",
              "   'page',\n",
              "   'plant',\n",
              "   'jones',\n",
              "   'performance',\n",
              "   'review',\n",
              "   'music',\n",
              "   'led',\n",
              "   'bonham',\n",
              "   'include',\n",
              "   'newly',\n",
              "   'band',\n",
              "   'tour',\n",
              "   'history',\n",
              "   'led',\n",
              "   'band',\n",
              "   'track',\n",
              "   'zeppelin',\n",
              "   'recording',\n",
              "   'led',\n",
              "   'zeppelin',\n",
              "   'celebration',\n",
              "   'inside',\n",
              "   'story']],\n",
              " (4,\n",
              "  5860): [['warner',\n",
              "   'records',\n",
              "   'harry',\n",
              "   'chamber',\n",
              "   'follow',\n",
              "   'musical',\n",
              "   'addition',\n",
              "   'william',\n",
              "   'themes',\n",
              "   'orchestrator',\n",
              "   'motion',\n",
              "   'picture',\n",
              "   'harry',\n",
              "   'feature',\n",
              "   'john',\n",
              "   'though',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'williams',\n",
              "   'themes',\n",
              "   'restless',\n",
              "   'light',\n",
              "   'since',\n",
              "   'lacking',\n",
              "   'veteran',\n",
              "   'mastery',\n",
              "   'orchestral',\n",
              "   'harry',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'chamber',\n",
              "   'secrets'], ['warner',\n",
              "   'records',\n",
              "   'original',\n",
              "   'potter',\n",
              "   'chamber',\n",
              "   'year',\n",
              "   'original',\n",
              "   'sorcerer',\n",
              "   'film',\n",
              "   'john',\n",
              "   'williams',\n",
              "   'jaws',\n",
              "   'star',\n",
              "   'nomination',\n",
              "   'first',\n",
              "   'film',\n",
              "   'score',\n",
              "   'new',\n",
              "   'musical',\n",
              "   'william',\n",
              "   'ross',\n",
              "   'number',\n",
              "   'williams',\n",
              "   'picture',\n",
              "   'secrets',\n",
              "   'five',\n",
              "   'poster',\n",
              "   'another',\n",
              "   'franchise',\n",
              "   'another',\n",
              "   'williams',\n",
              "   'gazillion',\n",
              "   'already',\n",
              "   'harry',\n",
              "   'proves',\n",
              "   'held',\n",
              "   'high',\n",
              "   'scoring',\n",
              "   'established',\n",
              "   'sorcerer',\n",
              "   'manages',\n",
              "   'chapter',\n",
              "   'many',\n",
              "   'cues',\n",
              "   'thus',\n",
              "   'hooks',\n",
              "   'mastery',\n",
              "   'stylistic',\n",
              "   'proceedings',\n",
              "   'maturity',\n",
              "   'film',\n",
              "   'crucial',\n",
              "   'potter',\n",
              "   'fictional',\n",
              "   'wizard',\n",
              "   'proves',\n",
              "   'real',\n",
              "   'thing',\n",
              "   'harry',\n",
              "   'potter',\n",
              "   'chamber',\n",
              "   'secrets']]}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stability=0\n",
        "# count_all=0\n",
        "for ui in dict_features.keys():\n",
        "  features=dict_features[ui]\n",
        "  stabs=0\n",
        "  count=0\n",
        "  if(len(features)>1):\n",
        "    # count_all+=1\n",
        "    for i in range(len(features)):\n",
        "      for j in range(len(features)):\n",
        "        if i != j:\n",
        "          intersection = list(set(features[i]) & set(features[j]))\n",
        "          union = list(set(features[i]) | set(features[j]))\n",
        "          # print(features[i],features[j])\n",
        "          # print(intersection)\n",
        "          # print(union)\n",
        "          count+=1\n",
        "          stabs+=(len(intersection)/len(union))\n",
        "    # print(stabs)\n",
        "    # print(len(features)*(len(features)-1))\n",
        "    # print((stabs/(len(features)*(len(features)-1))))\n",
        "    stability+=(stabs/(9.0*10.0))\n",
        "\n",
        "stability=stability/len( dict_features)\n",
        "print(stability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzjzCNTA5Nxu",
        "outputId": "1d8cecc7-5821-4ed2-bda2-d747f12dd5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4726154221190375\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}