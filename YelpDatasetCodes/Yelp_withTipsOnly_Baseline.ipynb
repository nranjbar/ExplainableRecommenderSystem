{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Links to Yelp dataset files : https://drive.google.com/drive/folders/15_pQBF8zIOnxXSaDx19DN8eU-lXrvIeW?usp=share_link https://drive.google.com/drive/folders/1-9SOeLW8g97fiX2On7_7Wm1ePt2pSnDU?usp=share_link"
      ],
      "metadata": {
        "id": "Kcx3W8YUMkSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing..."
      ],
      "metadata": {
        "id": "6kCoOziYMs81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehDCtL8ZppEb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tqdm\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import ndcg_score\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTIqI8mN1WOb",
        "outputId": "b11af8de-4adf-4da1-ec0d-baf0fa6a0e57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klGqwqNWJfP5"
      },
      "source": [
        "# Preprocessing Amazon ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slNP3spRNbLd"
      },
      "outputs": [],
      "source": [
        "user_thresh=20\n",
        "feature_thresh=2000\n",
        "review_dir='/content/drive/Shareddrives/Unlimited Drive | @LicenseMarket/Recommender/Yelp/Kaggle/yelp_academic_dataset_review.json'\n",
        "sentires_dir='/content/drive/Shareddrives/Unlimited Drive | @LicenseMarket/Recommender/Yelp/Yelp'\n",
        "test_length=5\n",
        "sample_ratio=2\n",
        "val_length=1\n",
        "neg_length=100\n",
        "dataset='yelp'\n",
        "save_path='/content/drive/MyDrive/Yelp/'\n",
        "save_path2='/content/drive/MyDrive/Yelp/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jD2qCBTqKLKe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ztz2ycBKc7O"
      },
      "outputs": [],
      "source": [
        "def get_feature_list(sentiment_data):\n",
        "    \"\"\"\n",
        "    from user sentiment data, get all the features [F1, F2, ..., Fk] mentioned in the reviews\n",
        "    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]\n",
        "    :return: feature set F\n",
        "    \"\"\"\n",
        "    feature_list = []\n",
        "    for row in sentiment_data:\n",
        "        for fos in row[2:]:\n",
        "            feature = fos[0]\n",
        "            if feature not in feature_list:\n",
        "                feature_list.append(feature)\n",
        "    feature_list = np.array(feature_list)\n",
        "    return feature_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF6TuALLKfx9"
      },
      "outputs": [],
      "source": [
        "def get_user_attention_matrix(sentiment_data, user_num, feature_list, max_range=5):\n",
        "    \"\"\"\n",
        "    build user attention matrix\n",
        "    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]\n",
        "    :param user_num: number of users\n",
        "    :param feature_list: [F1, F2, ..., Fk]\n",
        "    :param max_range: normalize the attention value to [1, max_range]\n",
        "    :return: the user attention matrix, Xij is user i's attention on feature j\n",
        "    \"\"\"\n",
        "    user_counting_matrix = np.zeros((user_num, len(feature_list)))  # tij = x if user i mention feature j x times\n",
        "    for row in sentiment_data:\n",
        "        user = row[0]\n",
        "        for fos in row[2:]:\n",
        "            feature = fos[0]\n",
        "            user_counting_matrix[user, feature] += 1\n",
        "    user_attention_matrix = np.zeros((user_num, len(feature_list)))  # xij = [1-N], normalized attention matrix\n",
        "    for i in range(len(user_counting_matrix)):\n",
        "        for j in range(len(user_counting_matrix[i])):\n",
        "            if user_counting_matrix[i, j] == 0:\n",
        "                norm_v = 0  # if nor mentioned: 0\n",
        "            else:\n",
        "                norm_v = 1 + (max_range - 1) * ((2 / (1 + np.exp(-user_counting_matrix[i, j]))) - 1)  # norm score\n",
        "            user_attention_matrix[i, j] = norm_v\n",
        "    user_attention_matrix = np.array(user_attention_matrix, dtype='float32')\n",
        "    return user_attention_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY76Dr2iKi9q"
      },
      "outputs": [],
      "source": [
        "def get_item_quality_matrix(sentiment_data, item_num, feature_list, max_range=5):\n",
        "    \"\"\"\n",
        "    build item quality matrix\n",
        "    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]\n",
        "    :param item_num: number of items\n",
        "    :param feature_list: [F1, F2, ..., Fk]\n",
        "    :param max_range: normalize the quality value to [1, max_range]\n",
        "    :return: the item quality matrix, Yij is item i's quality on feature j\n",
        "    \"\"\"\n",
        "    item_counting_matrix = np.zeros((item_num, len(feature_list)))  # kij = x if item i's feature j is mentioned x times\n",
        "    item_sentiment_matrix = np.zeros((item_num, len(feature_list)))  # sij = x if the overall rating is x (sum up)\n",
        "    for row in sentiment_data:\n",
        "        item = row[1]\n",
        "        for fos in row[2:]:\n",
        "            feature = fos[0]\n",
        "            sentiment = fos[2]\n",
        "            item_counting_matrix[item, feature] += 1\n",
        "            if sentiment == '+1':\n",
        "                item_sentiment_matrix[item, feature] += 1\n",
        "            elif sentiment == '-1':\n",
        "                item_sentiment_matrix[item, feature] -= 1\n",
        "            else:\n",
        "                print(\"sentiment data error: the sentiment value can only be +1 or -1\")\n",
        "                exit(1)\n",
        "    item_quality_matrix = np.zeros((item_num, len(feature_list)))\n",
        "    for i in range(len(item_counting_matrix)):\n",
        "        for j in range(len(item_counting_matrix[i])):\n",
        "            if item_counting_matrix[i, j] == 0:\n",
        "                norm_v = 0  # if not mentioned: 0\n",
        "            else:\n",
        "                norm_v = 1 + ((max_range - 1) / (1 + np.exp(-item_sentiment_matrix[i, j])))  # norm score\n",
        "            item_quality_matrix[i, j] = norm_v\n",
        "    item_quality_matrix = np.array(item_quality_matrix, dtype='float32')\n",
        "    return item_quality_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7N5LDenKmmu"
      },
      "outputs": [],
      "source": [
        "def get_user_item_dict(sentiment_data):\n",
        "    \"\"\"\n",
        "    build user & item dictionary\n",
        "    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]\n",
        "    :return: user dictionary {u1:[i, i, i...], u2:[i, i, i...]}, similarly, item dictionary\n",
        "    \"\"\"\n",
        "    user_dict = {}\n",
        "    item_dict = {}\n",
        "    for row in sentiment_data:\n",
        "        user = row[0]\n",
        "        item = row[1]\n",
        "        if user not in user_dict:\n",
        "            user_dict[user] = [item]\n",
        "        else:\n",
        "            user_dict[user].append(item)\n",
        "        if item not in item_dict:\n",
        "            item_dict[item] = [user]\n",
        "        else:\n",
        "            item_dict[item].append(user)\n",
        "    return user_dict, item_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIIOepRtKpWq"
      },
      "outputs": [],
      "source": [
        "def get_user_item_set(sentiment_data):\n",
        "    \"\"\"\n",
        "    get user item set\n",
        "    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]\n",
        "    :return: user_set = set(u1, u2, ..., um); item_set = (i1, i2, ..., in)\n",
        "    \"\"\"\n",
        "    user_set = set()\n",
        "    item_set = set()\n",
        "    for row in sentiment_data:\n",
        "        user = row[0]\n",
        "        item = row[1]\n",
        "        user_set.add(user)\n",
        "        item_set.add(item)\n",
        "    return user_set, item_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE1QvOctKr0v"
      },
      "outputs": [],
      "source": [
        "def sample_training_pairs(user, training_items, item_set, sample_ratio=10):\n",
        "    positive_items = set(training_items)\n",
        "    negative_items = set()\n",
        "    for item in item_set:\n",
        "        if item not in positive_items:\n",
        "            negative_items.add(item)\n",
        "    neg_length = len(positive_items) * sample_ratio\n",
        "    negative_items = np.random.choice(np.array(list(negative_items)), neg_length, replace=False)\n",
        "    train_pairs = []\n",
        "    for p_item in positive_items:\n",
        "        train_pairs.append([user, p_item, 1])\n",
        "    for n_item in negative_items:\n",
        "        train_pairs.append([user, n_item, 0])\n",
        "    return train_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2IuW30GKuO_"
      },
      "outputs": [],
      "source": [
        "def check_string(string):\n",
        "    # if the string contains letters\n",
        "    string_lowercase = string.lower()\n",
        "    contains_letters = string_lowercase.islower()\n",
        "    return contains_letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoTLr-DlKxm8"
      },
      "outputs": [],
      "source": [
        "def visualization(train_losses, val_losses, path):\n",
        "    plt.plot(np.arange(len(train_losses)), train_losses, label='training loss')\n",
        "    plt.plot(np.array(len(val_losses)), val_losses, label='validation loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(path)\n",
        "    plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7VIlHxeKzkn"
      },
      "outputs": [],
      "source": [
        "def get_mask_vec(user_attantion, k):\n",
        "    \"\"\"\n",
        "    get the top-k mask for features. The counterfactual explanations can only be chosen from this space\n",
        "    :param user_attantion: user's attantion vector on all the features\n",
        "    :param k: the k from mask\n",
        "    :return: a mask vector with 1's on the top-k features that the user cares about and 0's for others.\n",
        "    \"\"\"\n",
        "    top_indices = np.argsort(user_attantion)[::-1][:k]\n",
        "    mask = [0 for i in range(len(user_attantion))]\n",
        "    for index in top_indices:\n",
        "        if user_attantion[index] > 0:  # only consider the user mentioned features\n",
        "            mask[index] = 1\n",
        "    return np.array(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JA18uyUyK2d8"
      },
      "outputs": [],
      "source": [
        "def feature_filtering(sentiment_data, valid_features):\n",
        "    \"\"\"\n",
        "    filter the sentiment data, remove the invalid features\n",
        "    :param sentiment_data: [userID, itemID, [fos triplet 1], [fos triplet 2], ...]\n",
        "    :param valid_features: set of valid features\n",
        "    :return: the filtered sentiment data\n",
        "    \"\"\"\n",
        "    cleaned_sentiment_data = []\n",
        "    for row in sentiment_data:\n",
        "        user = row[0]\n",
        "        item = row[1]\n",
        "        cleaned_sentiment_data.append([user, item])\n",
        "        for fos in row[2:]:\n",
        "            if fos[0] in valid_features:\n",
        "                cleaned_sentiment_data[-1].append(fos)\n",
        "        if len(cleaned_sentiment_data[-1]) == 2:\n",
        "            del cleaned_sentiment_data[-1]\n",
        "    return np.array(cleaned_sentiment_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij_rZj82KQg7"
      },
      "outputs": [],
      "source": [
        "def sentiment_data_filtering(sentiment_data, user_thresh, feature_thresh):\n",
        "    \"\"\"\n",
        "    filter the sentiment data, remove the users with less review number less than \"user_thresh\" and remove the features\n",
        "    mentioned less than \"feature_thresh\" or don't contain letters.\n",
        "    :param sentiment_data: [userID, itemID, [fos triplet 1], [fos triplet 2], ...]\n",
        "    :param user_thresh: the threshold for user reviews\n",
        "    :param feature_thresh: the threshold features\n",
        "    :return: the filtered sentiment data\n",
        "    \"\"\"\n",
        "    print('======================= filtering sentiment data =======================')\n",
        "    sentiment_data = np.array(sentiment_data)\n",
        "    last_length = len(sentiment_data)\n",
        "    un_change_count = 0  # iteratively filtering users and features, if the data stay unchanged twice, stop\n",
        "    user_dict, item_dict = get_user_item_dict(sentiment_data)\n",
        "    features = get_feature_list(sentiment_data)\n",
        "    print(\"original review length: \", len(sentiment_data))\n",
        "    print(\"original user length: \", len(user_dict))\n",
        "    print(\"original item length: \", len(item_dict))\n",
        "    print(\"original feature length: \", len(features))\n",
        "    while True:\n",
        "        # feature filtering\n",
        "        feature_count_dict = {}\n",
        "        for row in sentiment_data:\n",
        "            for fos in row[2:]:\n",
        "                feature = fos[0]\n",
        "                if feature not in feature_count_dict:\n",
        "                    feature_count_dict[feature] = 1\n",
        "                else:\n",
        "                    feature_count_dict[feature] += 1\n",
        "        valid_features = set()\n",
        "        for key, value in feature_count_dict.items():\n",
        "            if check_string(key) and value > feature_thresh:\n",
        "                valid_features.add(key)\n",
        "        sentiment_data = [row for row in sentiment_data if row[2][0] in valid_features]\n",
        "        sentiment_data = feature_filtering(sentiment_data, valid_features)\n",
        "        length = len(sentiment_data)\n",
        "        if length != last_length:\n",
        "            last_length = length\n",
        "            un_change_count = 0\n",
        "        else:\n",
        "            un_change_count += 1\n",
        "            if un_change_count == 2:\n",
        "                break\n",
        "        # user filtering\n",
        "        user_dict, item_dict = get_user_item_dict(sentiment_data)\n",
        "        valid_user = set()  # the valid users\n",
        "        for key, value in user_dict.items():\n",
        "            if len(value) > (user_thresh - 1):\n",
        "              valid_user.add(key)\n",
        "        sentiment_data = [x for x in sentiment_data if x[0] in valid_user]  # remove user with small interactions\n",
        "        length = len(sentiment_data)\n",
        "        if length != last_length:\n",
        "            last_length = length\n",
        "            un_change_count = 0\n",
        "        else:\n",
        "            un_change_count += 1\n",
        "            if un_change_count == 2:\n",
        "                break\n",
        "    user_dict, item_dict = get_user_item_dict(sentiment_data)\n",
        "    features = get_feature_list(sentiment_data)\n",
        "    print('valid review length: ', len(sentiment_data))\n",
        "    print(\"valid user: \", len(user_dict))\n",
        "    print('valid item : ', len(item_dict))\n",
        "    print(\"valid feature length: \", len(features))\n",
        "    print('user dense is:', len(sentiment_data) / len(user_dict))\n",
        "    sentiment_data = np.array(sentiment_data)\n",
        "    return sentiment_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YelpDataset():\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # self.args = preprocessing_args\n",
        "        self.sentiment_data = None  # [userID, itemID, [fos triplet 1], [fos triplet 2], ...]\n",
        "\n",
        "        self.user_name_dict = {}  # rename users to integer names\n",
        "        self.item_name_dict = {}\n",
        "        self.feature_name_dict = {}\n",
        "\n",
        "        self.features = []  # feature list\n",
        "        self.users = []\n",
        "        self.items = []\n",
        "\n",
        "        # the interacted items for each user, sorted with date {user:[i1, i2, i3, ...], user:[i1, i2, i3, ...]}\n",
        "        self.user_hist_inter_dict = {}\n",
        "        # the interacted users for each item\n",
        "        self.item_hist_inter_dict = {}  \n",
        "\n",
        "        self.user_num = None\n",
        "        self.item_num = None\n",
        "        self.feature_num = None  # number of features\n",
        "\n",
        "        self.user_feature_matrix = None  # user aspect attention matrix\n",
        "        self.item_feature_matrix = None  # item aspect quality matrix\n",
        "\n",
        "        self.training_data = None\n",
        "        self.test_data = None\n",
        "        self.pre_processing()\n",
        "        self.get_user_item_feature_matrix()\n",
        "        self.sample_training()  # sample training data, for traning BPR loss\n",
        "        self.sample_test()  # sample test data\n",
        "    def pre_processing(self,):\n",
        "       sentiment_data = []  # [userID, itemID, [fos triplet 1], [fos triplet 2], ...]\n",
        "       with open(sentires_dir, 'r') as f:\n",
        "            line = f.readline().strip()\n",
        "            while line:\n",
        "                # print(count)\n",
        "                # print('line', line)\n",
        "                user = line.split('@')[0]\n",
        "                item = line.split('@')[1]\n",
        "                sentiment_data.append([user, item])\n",
        "                l = len(user) + len(item)\n",
        "                fosr_data = line[l+3:]\n",
        "                for seg in fosr_data.split('||'):\n",
        "                    fos = seg.split(':')[0].strip('|')\n",
        "                    if len(fos.split('|')) > 1:\n",
        "                        feature = fos.split('|')[0]\n",
        "                        opinion = fos.split('|')[1]\n",
        "                        sentiment = fos.split('|')[2]\n",
        "                        sentiment_data[-1].append([feature, opinion, sentiment])\n",
        "                line = f.readline().strip()\n",
        "       sentiment_data = np.array(sentiment_data)\n",
        "       sentiment_data = sentiment_data_filtering(\n",
        "          sentiment_data, \n",
        "          user_thresh, \n",
        "          feature_thresh)\n",
        "       user_dict, item_dict = get_user_item_dict(sentiment_data)  # not sorted with time\n",
        "       user_item_date_dict = {}   # {(user, item): date, (user, item): date ...}  # used to remove duplicate\n",
        "       for i, line in enumerate(open(review_dir, \"r\")):\n",
        "            record = json.loads(line)\n",
        "            user = record['user_id']\n",
        "            item = record['business_id']\n",
        "            date = record['date']\n",
        "            if user in user_dict and item in user_dict[user] and (user, item) not in user_item_date_dict:\n",
        "                user_item_date_dict[(user, item)] = date\n",
        "       sentiment_data = [row for row in sentiment_data if (row[0], row[1]) in user_item_date_dict]\n",
        "       sentiment_data = sentiment_data_filtering(sentiment_data, user_thresh, feature_thresh)\n",
        "       user_dict, item_dict = get_user_item_dict(sentiment_data)\n",
        "       for key in list(user_item_date_dict.keys()):\n",
        "            if key[0] not in user_dict or key[1] not in user_dict[key[0]]:\n",
        "                del user_item_date_dict[key]\n",
        "        \n",
        "        # rename users, items, and features to integer names\n",
        "       user_name_dict = {}\n",
        "       item_name_dict = {}\n",
        "       feature_name_dict = {}\n",
        "       features = get_feature_list(sentiment_data)\n",
        "        \n",
        "       count = 0\n",
        "       for user in user_dict:\n",
        "            if user not in user_name_dict:\n",
        "                user_name_dict[user] = count\n",
        "                count += 1\n",
        "       count = 0\n",
        "       for item in item_dict:\n",
        "            if item not in item_name_dict:\n",
        "                item_name_dict[item] = count\n",
        "                count += 1\n",
        "       count = 0\n",
        "       for feature in features:\n",
        "            if feature not in feature_name_dict:\n",
        "                feature_name_dict[feature] = count\n",
        "                count += 1\n",
        "        \n",
        "       for i in range(len(sentiment_data)):\n",
        "            sentiment_data[i][0] = user_name_dict[sentiment_data[i][0]]\n",
        "            sentiment_data[i][1] = item_name_dict[sentiment_data[i][1]]\n",
        "            for j in range(len(sentiment_data[i]) - 2):\n",
        "                sentiment_data[i][j+2][0] = feature_name_dict[sentiment_data[i][j + 2][0]]\n",
        "\n",
        "       renamed_user_item_date_dict = {}\n",
        "       for key, value in user_item_date_dict.items():\n",
        "            renamed_user_item_date_dict[user_name_dict[key[0]], item_name_dict[key[1]]] = value\n",
        "       user_item_date_dict = renamed_user_item_date_dict\n",
        "\n",
        "        # sort with date\n",
        "       user_item_date_dict = dict(sorted(user_item_date_dict.items(), key=lambda item: item[1]))\n",
        "\n",
        "       user_hist_inter_dict = {}  # {\"u1\": [i1, i2, i3, ...], \"u2\": [i1, i2, i3, ...]}, sort with time\n",
        "       item_hist_inter_dict = {}\n",
        "        # ranked_user_item_dict = {}  # {\"u1\": [i1, i2, i3, ...], \"u2\": [i1, i2, i3, ...]}\n",
        "       for key, value in user_item_date_dict.items():\n",
        "            user = key[0]\n",
        "            item = key[1]\n",
        "            if user not in user_hist_inter_dict:\n",
        "                user_hist_inter_dict[user] = [item]\n",
        "            else:\n",
        "                user_hist_inter_dict[user].append(item)\n",
        "            if item not in item_hist_inter_dict:\n",
        "                item_hist_inter_dict[item] = [user]\n",
        "            else:\n",
        "                item_hist_inter_dict[item].append(user)\n",
        "\n",
        "       user_hist_inter_dict = dict(sorted(user_hist_inter_dict.items()))\n",
        "       item_hist_inter_dict = dict(sorted(item_hist_inter_dict.items()))\n",
        "\n",
        "       users = list(user_hist_inter_dict.keys())\n",
        "       items = list(item_hist_inter_dict.keys())\n",
        "       self.sentiment_data = sentiment_data\n",
        "       self.user_name_dict = user_name_dict\n",
        "       self.item_name_dict = item_name_dict\n",
        "       self.feature_name_dict = feature_name_dict\n",
        "       self.user_hist_inter_dict = user_hist_inter_dict\n",
        "       self.item_hist_inter_dict = item_hist_inter_dict\n",
        "       self.users = users\n",
        "       self.items = items\n",
        "       self.features = features\n",
        "       self.user_num = len(users)\n",
        "       self.item_num = len(items)\n",
        "       self.feature_num = len(features)\n",
        "       return True\n",
        "    def get_user_item_feature_matrix(self,):\n",
        "        # exclude test data from the sentiment data to construct matrix\n",
        "        train_u_i_set = set()\n",
        "        for user, items in self.user_hist_inter_dict.items():\n",
        "            items = items\n",
        "            for item in items:\n",
        "                train_u_i_set.add((user, item))\n",
        "        train_sentiment_data = []\n",
        "        for row in self.sentiment_data:\n",
        "            user = row[0]\n",
        "            item = row[1]\n",
        "            if (user, item) in train_u_i_set:\n",
        "                train_sentiment_data.append(row)\n",
        "        self.user_feature_matrix = get_user_attention_matrix(\n",
        "            train_sentiment_data, \n",
        "            self.user_num, \n",
        "            self.features, \n",
        "            max_range=5)\n",
        "        self.item_feature_matrix = get_item_quality_matrix(\n",
        "            train_sentiment_data, \n",
        "            self.item_num, \n",
        "            self.features, \n",
        "            max_range=5)\n",
        "        return True\n",
        "    def sample_training(self):\n",
        "        print('======================= sample training data =======================')\n",
        "        # print(self.user_feature_matrix.shape, self.item_feature_matrix.shape)\n",
        "        training_data = []\n",
        "        training_pairs = np.loadtxt(save_path2+'training_data_2.txt',dtype=str)\n",
        "        for pair in training_pairs:\n",
        "          if pair[0] in self.user_name_dict.keys() and pair[1] in self.item_name_dict.keys():\n",
        "            training_data.append([self.user_name_dict[pair[0]],self.item_name_dict[pair[1]],int(pair[2])])\n",
        "\n",
        "        # item_set = set(self.items)\n",
        "        # for user, items in self.user_hist_inter_dict.items():\n",
        "        #     items = items[:-(test_length+val_length)]\n",
        "        #     training_pairs = sample_training_pairs(\n",
        "        #         user, \n",
        "        #         items, \n",
        "        #         item_set, \n",
        "        #         sample_ratio)\n",
        "        #     for pair in training_pairs:\n",
        "        #         training_data.append(pair)\n",
        "        print('# training samples :', len(training_data))\n",
        "        self.training_data = np.array(training_data)\n",
        "        return True\n",
        "    \n",
        "    def sample_test(self):\n",
        "        print('======================= sample test data =======================')\n",
        "        user_item_label_list = []  # [[u, [item1, item2, ...], [l1, l2, ...]], ...]\n",
        "        with open(save_path2+'test_data_2.pickle', 'rb') as f:\n",
        "            test_pairs= pickle.load(f)\n",
        "        for user_id in test_pairs.keys():\n",
        "          if user_id in self.user_name_dict.keys():\n",
        "            user=self.user_name_dict[user_id]\n",
        "            items_ids=test_pairs[user_id][0]\n",
        "            labels=test_pairs[user_id][1]\n",
        "            items=np.array([self.item_name_dict[item] for item in items_ids if item in self.item_name_dict.keys()])\n",
        "            labels=np.array([float(labels[i]) for i in range(len(labels)) if items_ids[i] in self.item_name_dict.keys()])\n",
        "            user_item_label_list.append([user,items,labels])\n",
        "        \n",
        "        # for user, items in self.user_hist_inter_dict.items():\n",
        "        #     items = items[-(test_length+val_length):]\n",
        "        #     user_item_label_list.append([user, items, np.ones(len(items))])  # add the test items\n",
        "        #     negative_items = [item for item in self.items if \n",
        "        #         item not in self.user_hist_inter_dict[user]]  # the not interacted items\n",
        "        #     negative_items = np.random.choice(np.array(negative_items), neg_length, replace=False)\n",
        "        #     user_item_label_list[-1][1] = np.concatenate((user_item_label_list[-1][1], negative_items), axis=0)\n",
        "        #     user_item_label_list[-1][2] = np.concatenate((user_item_label_list[-1][2], np.zeros(neg_length)), axis=0)\n",
        "        print('# test samples :', len(user_item_label_list))\n",
        "        self.test_data = np.array(user_item_label_list)\n",
        "        user_item_label_list2 = []  # [[u, [item1, item2, ...], [l1, l2, ...]], ...]\n",
        "        with open(save_path2+'validation_data_2.pickle', 'rb') as f:\n",
        "            validation_pairs= pickle.load(f)\n",
        "        for user_id in validation_pairs.keys():\n",
        "          if user_id in self.user_name_dict.keys():\n",
        "            user=self.user_name_dict[user_id]\n",
        "            items_ids=validation_pairs[user_id][0]\n",
        "            labels=validation_pairs[user_id][1]\n",
        "            items=np.array([self.item_name_dict[item] for item in items_ids if item in self.item_name_dict.keys()])\n",
        "            labels=np.array([float(labels[i]) for i in range(len(labels)) if items_ids[i] in self.item_name_dict.keys()])\n",
        "            user_item_label_list2.append([user,items,labels])\n",
        "        \n",
        "        # for user, items in self.user_hist_inter_dict.items():\n",
        "        #     items = items[-(test_length+val_length):]\n",
        "        #     user_item_label_list.append([user, items, np.ones(len(items))])  # add the test items\n",
        "        #     negative_items = [item for item in self.items if \n",
        "        #         item not in self.user_hist_inter_dict[user]]  # the not interacted items\n",
        "        #     negative_items = np.random.choice(np.array(negative_items), neg_length, replace=False)\n",
        "        #     user_item_label_list[-1][1] = np.concatenate((user_item_label_list[-1][1], negative_items), axis=0)\n",
        "        #     user_item_label_list[-1][2] = np.concatenate((user_item_label_list[-1][2], np.zeros(neg_length)), axis=0)\n",
        "        print('# validation samples :', len(user_item_label_list2))\n",
        "        self.validation_data = np.array(user_item_label_list2)\n",
        "        return True\n",
        "    \n",
        "    def save(self, save_path):\n",
        "        return True\n",
        "    \n",
        "    def load(self):\n",
        "        return False"
      ],
      "metadata": {
        "id": "aX1TBUQx08Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJU1IgY8LF07"
      },
      "outputs": [],
      "source": [
        "def yelp_preprocessing():\n",
        "    rec_dataset = YelpDataset()\n",
        "    return rec_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ECM0Lw3NFbC"
      },
      "outputs": [],
      "source": [
        "def dataset_init():\n",
        "\tif dataset == \"yelp\":\n",
        "\t\trec_dataset = yelp_preprocessing()\n",
        "\telif dataset == \"cell_phones\" or \"kindle_store\" or \"electronic\" or \"cds_and_vinyl\":\n",
        "\t\trec_dataset = amazon_preprocessing()\n",
        "\treturn rec_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9uxsi7iMPtv"
      },
      "source": [
        "# Training base Model..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAb2lKMWb1Zo"
      },
      "outputs": [],
      "source": [
        "dataset=\"yelp\"\n",
        "gpu=True\n",
        "cuda='0'\n",
        "weight_decay=0.000001\n",
        "lr=0.005\n",
        "epochs=36\n",
        "batch_size=128\n",
        "rec_k=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4Zo-5mANolR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "class UserItemInterDataset(Dataset):\n",
        "    def __init__(self, data, user_feature_matrix, item_feature_matrix):\n",
        "        self.data = data\n",
        "        self.user_feature_matrix = user_feature_matrix\n",
        "        self.item_feature_matrix = item_feature_matrix\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        user = self.data[index][0]\n",
        "        item = self.data[index][1]\n",
        "        label = self.data[index][2]\n",
        "        user_feature = self.user_feature_matrix[user]\n",
        "        item_feature = self.item_feature_matrix[item]\n",
        "        return user_feature, item_feature, label\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Black-box model..."
      ],
      "metadata": {
        "id": "0dqHL9pMM1Sz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lma7BKCoNySG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import ndcg_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep5Pi9VbNtvi"
      },
      "outputs": [],
      "source": [
        "def compute_ndcg(test_data, user_feature_matrix, item_feature_matrix, k, model, device):\n",
        "    model.eval()\n",
        "    ndcgs = []\n",
        "    with torch.no_grad():\n",
        "        for row in test_data:\n",
        "            user = row[0]\n",
        "            items = row[1]\n",
        "            gt_labels = row[2]\n",
        "            # print(len(items))\n",
        "            # print(len(gt_labels))\n",
        "            user_features = np.array([user_feature_matrix[user] for i in range(len(items))])\n",
        "            item_features = np.array([item_feature_matrix[item] for item in items])\n",
        "            scores = model(torch.from_numpy(user_features).to(device),\n",
        "                                    torch.from_numpy(item_features).to(device)).squeeze()\n",
        "        \n",
        "            scores = np.array(scores.to('cpu'))\n",
        "            # print(user_features.shape)\n",
        "            # print(len(scores))\n",
        "            # print(len(gt_labels))\n",
        "            ndcg = ndcg_score([gt_labels], [scores], k=k)\n",
        "            ndcgs.append(ndcg)\n",
        "    ave_ndcg = np.mean(ndcgs)\n",
        "    return ave_ndcg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnvfmuGXM-7F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import tqdm\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-szQN49N-l0"
      },
      "outputs": [],
      "source": [
        "from numpy import core\n",
        "\n",
        "class BaseRecModel(torch.nn.Module):\n",
        "    def __init__(self, feature_length):\n",
        "        super(BaseRecModel, self).__init__()\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(feature_length * 2, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 1),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, user_feature, item_feature):\n",
        "        fusion = torch.cat((user_feature, item_feature), 1)\n",
        "        out = self.fc(fusion)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05LMQ0K63i30",
        "outputId": "800ec8c4-6a58-4aba-dc37-80c62015ec7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if gpu:\n",
        "  device = torch.device('cuda:%s' % cuda)\n",
        "else:\n",
        "  device = 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZvHSb8CgLB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6217db8f-dfdf-4053-ce6c-fc01c21e5489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-04712ae582c0>:53: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sentiment_data = np.array(sentiment_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================= filtering sentiment data =======================\n",
            "original review length:  1366842\n",
            "original user length:  84842\n",
            "original item length:  52594\n",
            "original feature length:  11135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8047dbd5a64f>:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(cleaned_sentiment_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid review length:  696465\n",
            "valid user:  13034\n",
            "valid item :  47031\n",
            "valid feature length:  255\n",
            "user dense is: 53.43447905477981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-394a3508aa51>:66: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sentiment_data = np.array(sentiment_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================= filtering sentiment data =======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-394a3508aa51>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  sentiment_data = np.array(sentiment_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original review length:  696465\n",
            "original user length:  13034\n",
            "original item length:  47031\n",
            "original feature length:  255\n",
            "valid review length:  696465\n",
            "valid user:  13034\n",
            "valid item :  47031\n",
            "valid feature length:  255\n",
            "user dense is: 53.43447905477981\n",
            "======================= sample training data =======================\n",
            "# training samples : 317566\n",
            "======================= sample test data =======================\n",
            "# test samples : 2049\n",
            "# validation samples : 2049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-04712ae582c0>:213: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.test_data = np.array(user_item_label_list)\n",
            "<ipython-input-57-04712ae582c0>:235: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.validation_data = np.array(user_item_label_list2)\n"
          ]
        }
      ],
      "source": [
        "rec_dataset = dataset_init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
        "with open(os.path.join(save_path2,dataset + \"_dataset_obj_main.pickle\"), 'wb') as outp:\n",
        "    pickle.dump(rec_dataset, outp, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "7dM2ptrUpZ_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(save_path2, dataset + \"_dataset_obj_main.pickle\"), 'rb') as inp:\n",
        "  rec_dataset = pickle.load(inp)"
      ],
      "metadata": {
        "id": "lDouhixgwHdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(list(set(rec_dataset.features))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKGsVwNZlLlC",
        "outputId": "95e4714e-15fe-48a3-8c52-5902049f7b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rec_dataset.test_data"
      ],
      "metadata": {
        "id": "zLHFz8NOpbbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2G66iinM9Os",
        "outputId": "56e92898-299f-4bc0-cc88-454dfd00875c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA 0\n",
            "init ndcg: 0.3028848819387577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/36 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:  training loss:  0.42214802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 1/36 [00:14<08:13, 14.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0:  training loss:  0.42214802 NDCG:  0.49525104293724864\n",
            "epoch 1:  training loss:  0.4099281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 2/36 [00:27<07:43, 13.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1:  training loss:  0.4099281 NDCG:  0.49776260005681483\n",
            "epoch 2:  training loss:  0.4070018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 3/36 [00:36<06:15, 11.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 2:  training loss:  0.4070018 NDCG:  0.5010354940528069\n",
            "epoch 3:  training loss:  0.40491983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█         | 4/36 [00:44<05:29, 10.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 3:  training loss:  0.40491983 NDCG:  0.5015879617146849\n",
            "epoch 4:  training loss:  0.40295365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 5/36 [00:53<05:02,  9.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 4:  training loss:  0.40295365 NDCG:  0.5043302146494103\n",
            "epoch 5:  training loss:  0.4010969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 6/36 [01:02<04:42,  9.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 5:  training loss:  0.4010969 NDCG:  0.5058430631958454\n",
            "epoch 6:  training loss:  0.39911625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 7/36 [01:10<04:25,  9.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 6:  training loss:  0.39911625 NDCG:  0.5092635389903876\n",
            "epoch 7:  training loss:  0.39701122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 22%|██▏       | 8/36 [01:22<04:35,  9.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 7:  training loss:  0.39701122 NDCG:  0.5124873579843109\n",
            "epoch 8:  training loss:  0.39475402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 9/36 [01:30<04:16,  9.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 8:  training loss:  0.39475402 NDCG:  0.5141606906744036\n",
            "epoch 9:  training loss:  0.39230338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 10/36 [01:39<04:00,  9.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 9:  training loss:  0.39230338 NDCG:  0.5170387598685293\n",
            "epoch 10:  training loss:  0.38951093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 11/36 [01:48<03:46,  9.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 10:  training loss:  0.38951093 NDCG:  0.5195052455355846\n",
            "epoch 11:  training loss:  0.38673386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 12/36 [01:57<03:35,  8.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 11:  training loss:  0.38673386 NDCG:  0.5216298494998618\n",
            "epoch 12:  training loss:  0.38356283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▌      | 13/36 [02:05<03:24,  8.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 12:  training loss:  0.38356283 NDCG:  0.5243422055308086\n",
            "epoch 13:  training loss:  0.38027066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 39%|███▉      | 14/36 [02:14<03:14,  8.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 13:  training loss:  0.38027066 NDCG:  0.5262228976733442\n",
            "epoch 14:  training loss:  0.3768152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 15/36 [02:23<03:03,  8.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 14:  training loss:  0.3768152 NDCG:  0.5288621182928653\n",
            "epoch 15:  training loss:  0.37330744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 16/36 [02:31<02:54,  8.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 15:  training loss:  0.37330744 NDCG:  0.5274238295696199\n",
            "epoch 16:  training loss:  0.36949182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 17/36 [02:40<02:46,  8.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 16:  training loss:  0.36949182 NDCG:  0.5273928065110595\n",
            "epoch 17:  training loss:  0.36555558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 18/36 [02:49<02:37,  8.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 17:  training loss:  0.36555558 NDCG:  0.5331497015542664\n",
            "epoch 18:  training loss:  0.36185828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 19/36 [02:58<02:29,  8.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 18:  training loss:  0.36185828 NDCG:  0.5338643556874878\n",
            "epoch 19:  training loss:  0.35747004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 20/36 [03:06<02:20,  8.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 19:  training loss:  0.35747004 NDCG:  0.5343018714268231\n",
            "epoch 20:  training loss:  0.35310096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 21/36 [03:15<02:11,  8.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 20:  training loss:  0.35310096 NDCG:  0.5321863916992327\n",
            "epoch 21:  training loss:  0.34910184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 61%|██████    | 22/36 [03:24<02:02,  8.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 21:  training loss:  0.34910184 NDCG:  0.531845670765252\n",
            "epoch 22:  training loss:  0.34460253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 23/36 [03:33<01:53,  8.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 22:  training loss:  0.34460253 NDCG:  0.535292786327059\n",
            "epoch 23:  training loss:  0.33965316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 24/36 [03:41<01:45,  8.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 23:  training loss:  0.33965316 NDCG:  0.5326213875394561\n",
            "epoch 24:  training loss:  0.33507854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 25/36 [03:50<01:36,  8.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 24:  training loss:  0.33507854 NDCG:  0.5218103444711512\n",
            "epoch 25:  training loss:  0.3304872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 72%|███████▏  | 26/36 [03:59<01:27,  8.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 25:  training loss:  0.3304872 NDCG:  0.5321986038609967\n",
            "epoch 26:  training loss:  0.32609797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 27/36 [04:08<01:18,  8.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 26:  training loss:  0.32609797 NDCG:  0.5320458501197967\n",
            "epoch 27:  training loss:  0.32097372\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 28/36 [04:16<01:09,  8.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 27:  training loss:  0.32097372 NDCG:  0.5263879501133832\n",
            "epoch 28:  training loss:  0.31620866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 29/36 [04:25<01:01,  8.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 28:  training loss:  0.31620866 NDCG:  0.529194145280863\n",
            "epoch 29:  training loss:  0.31169385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 30/36 [04:34<00:52,  8.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 29:  training loss:  0.31169385 NDCG:  0.5160448224590467\n",
            "epoch 30:  training loss:  0.30634132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 31/36 [04:44<00:45,  9.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 30:  training loss:  0.30634132 NDCG:  0.5234662112192809\n",
            "epoch 31:  training loss:  0.30182943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 89%|████████▉ | 32/36 [04:53<00:36,  9.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 31:  training loss:  0.30182943 NDCG:  0.5250482344308258\n",
            "epoch 32:  training loss:  0.2970131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 33/36 [05:01<00:26,  8.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 32:  training loss:  0.2970131 NDCG:  0.5249306454479016\n",
            "epoch 33:  training loss:  0.2925485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 34/36 [05:10<00:17,  8.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 33:  training loss:  0.2925485 NDCG:  0.5089526265453299\n",
            "epoch 34:  training loss:  0.2879885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 35/36 [05:19<00:08,  8.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 34:  training loss:  0.2879885 NDCG:  0.5192458777452177\n",
            "epoch 35:  training loss:  0.2832115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 36/36 [05:28<00:00,  9.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 35:  training loss:  0.2832115 NDCG:  0.5170412818019532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NDCG Test:  0.5001901568632098\n"
          ]
        }
      ],
      "source": [
        "def train_base_recommendation():\n",
        "    if gpu:\n",
        "        device = torch.device('cuda:%s' % cuda)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "\n",
        "    # rec_dataset = dataset_init()\n",
        "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
        "    with open(os.path.join(save_path2,dataset + \"_dataset_obj_main.pickle\"), 'wb') as outp:\n",
        "        pickle.dump(rec_dataset, outp, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    train_loader = DataLoader(dataset=UserItemInterDataset(rec_dataset.training_data, \n",
        "                                rec_dataset.user_feature_matrix, \n",
        "                                rec_dataset.item_feature_matrix),\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "    model = BaseRecModel(rec_dataset.feature_num).to(device)\n",
        "    loss_fn = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    out_path = os.path.join(\"./logs\", dataset + \"_logs\")\n",
        "    Path(out_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ndcg = compute_ndcg(rec_dataset.test_data, \n",
        "            rec_dataset.user_feature_matrix, \n",
        "            rec_dataset.item_feature_matrix, \n",
        "            rec_k, \n",
        "            model, \n",
        "            device)\n",
        "    print('init ndcg:', ndcg)\n",
        "    for epoch in tqdm.trange(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        losses = []\n",
        "        for user_behaviour_feature, item_aspect_feature, label in train_loader:\n",
        "            user_behaviour_feature = user_behaviour_feature.to(device)\n",
        "            item_aspect_feature = item_aspect_feature.to(device)\n",
        "            label = label.float().to(device)\n",
        "            out = model(user_behaviour_feature, item_aspect_feature).squeeze()\n",
        "            loss = loss_fn(out, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            losses.append(loss.to('cpu').detach().numpy())\n",
        "            ave_train = np.mean(np.array(losses))\n",
        "            # print(out)\n",
        "            # print(label)\n",
        "            # print('----------')\n",
        "        print('epoch %d: ' % epoch, 'training loss: ', ave_train)\n",
        "        # compute necg\n",
        "        if epoch % 1 == 0:\n",
        "            ndcg = compute_ndcg(rec_dataset.validation_data, \n",
        "            rec_dataset.user_feature_matrix, \n",
        "            rec_dataset.item_feature_matrix, \n",
        "            rec_k, \n",
        "            model, \n",
        "            device)\n",
        "            print('epoch %d: ' % epoch, 'training loss: ', ave_train, 'NDCG: ', ndcg)\n",
        "    torch.save(model.state_dict(), os.path.join(out_path, \"model_main.model\"))\n",
        "    ndcg_test = compute_ndcg(rec_dataset.test_data, \n",
        "            rec_dataset.user_feature_matrix, \n",
        "            rec_dataset.item_feature_matrix, \n",
        "            rec_k, \n",
        "            model, \n",
        "            device)\n",
        "    print('NDCG Test: ',ndcg_test)\n",
        "    return 0\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    if gpu:\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] =cuda\n",
        "        print(\"Using CUDA\",cuda)\n",
        "    else:\n",
        "        print(\"Using CPU\")\n",
        "    train_base_recommendation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjCEe5Nq342r"
      },
      "source": [
        "# Training ExpOptimization Model..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ucRyXRT4mzb"
      },
      "outputs": [],
      "source": [
        "dataset=\"yelp\"\n",
        "base_model_path='/content/drive/MyDrive/Yelp/'\n",
        "gpu=True\n",
        "cuda='0'\n",
        "data_obj_path='/content/drive/MyDrive/Yelp/'\n",
        "rec_k=5\n",
        "lam=100\n",
        "gam=1.0\n",
        "alp=0.01\n",
        "user_mask=False\n",
        "lr=0.01\n",
        "step=1000\n",
        "mask_thresh=0.1\n",
        "test_num=1000\n",
        "save_path='/content/drive/MyDrive/Yelp/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snwXLWLePEmC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path='/content/drive/Shareddrives/Unlimited Drive | @LicenseMarket/Recommender/Yelp/'"
      ],
      "metadata": {
        "id": "K7I1_zbmdlnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQRrvFS4cifS",
        "outputId": "de5759b7-14f7-41af-eff7-8ce797af6b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100000\n",
            "200000\n",
            "300000\n",
            "400000\n",
            "500000\n",
            "1Xw_npZXLcsWBvlLYCiW_A fKrmWy4GFsrgdOYhN9pyZA\n",
            "600000\n",
            "700000\n",
            "800000\n",
            "900000\n",
            "1000000\n",
            "1100000\n",
            "1200000\n",
            "1300000\n"
          ]
        }
      ],
      "source": [
        "items_list1=[]\n",
        "users_list=[]\n",
        "review_features={}\n",
        "f=open(save_path+'Yelp')\n",
        "lines=f.readlines()\n",
        "i=0\n",
        "for line in lines:\n",
        "  if i%100000==0:\n",
        "    print(i)\n",
        "  i+=1\n",
        "  user_id = line.split('@')[0]\n",
        "  item_id = line.split('@')[1]\n",
        "  # if item_id in items_list:\n",
        "  users_list.append(user_id)\n",
        "  items_list1.append(item_id)\n",
        "  l = len(user_id) + len(item_id)\n",
        "  fosr_data = line[l+3:]\n",
        "  for seg in fosr_data.split('||'):\n",
        "    if (user_id,item_id) not in review_features.keys():\n",
        "      review_features[(user_id,item_id)]=[]\n",
        "    fos = seg.split(':')[0].strip('|')\n",
        "    if len(fos.split('|')) > 1:\n",
        "          feature = fos.split('|')[0]\n",
        "          opinion = fos.split('|')[1]\n",
        "          sentiment = fos.split('|')[2]\n",
        "          sentence= seg.split(':')[1]\n",
        "          if sentiment=='+1':\n",
        "            senti=1\n",
        "          else:\n",
        "            senti=-1\n",
        "          review_features[(user_id,item_id)].append([feature,opinion,senti,sentence])\n",
        "    else:\n",
        "      print(user_id,item_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del review_features"
      ],
      "metadata": {
        "id": "XnVLcRAKh7Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPUU-jJ-dwpl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from nltk import FreqDist\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize,pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o93FnpHX60cX",
        "outputId": "f6012d6e-abcd-4e80-efc6-e73c134f6b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def lemmatization(text):\n",
        "    result=''\n",
        "    wordnet = WordNetLemmatizer()\n",
        "    for token,tag in pos_tag(text):\n",
        "        pos=tag[0].lower()\n",
        "        if pos not in ['a', 'r', 'n', 'v']:\n",
        "            pos='n'\n",
        "        # if pos in ['n','a']:   \n",
        "        result+=wordnet.lemmatize(token,pos)+' '\n",
        "    return result\n",
        "def remove_stopwords(text):\n",
        "    en_stopwords = stopwords.words('english')\n",
        "    en_stopwords+=['may','could','that','without','iii','with','and','This','That','Those','These','the','The','brbr','so','it','such']\n",
        "    result = []\n",
        "    for token in text:\n",
        "        if token not in en_stopwords:\n",
        "            result.append(token)\n",
        "            \n",
        "    return result\n",
        "def remove_punct(text):\n",
        "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "    lst=tokenizer.tokenize(' '.join(text))\n",
        "    return lst\n",
        "\n",
        "def remove_tag(text):\n",
        "    text=' '.join(text)\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "\n",
        "def preprocess(text):\n",
        "  chars=['&','%','#','@','^','>','<','\\n','\\\\','\\t',';','\"','/']\n",
        "  stwords=stopwords.words('english')\n",
        "  for ch in chars:\n",
        "    text=text.replace(ch,' ')\n",
        "  text=\" \".join(text.split())\n",
        "  # text=text.lower()\n",
        "  text_tokenized=word_tokenize(text)\n",
        "  cleaned_text= remove_stopwords(text_tokenized)\n",
        "  cleaned_text= remove_punct(cleaned_text)\n",
        "  # cleaned_text=lemmatization(cleaned_text)\n",
        "  cleaned_text=remove_tag(cleaned_text)\n",
        "  cleaned_text=remove_urls(cleaned_text)\n",
        "  cleaned_text=''.join([i for i in cleaned_text ])\n",
        "  cleaned_text=[word for word in cleaned_text.split(' ') if len(word)>1]\n",
        "  # print(cleaned_text)\n",
        "  return ' '.join(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtlEHpkN-lCe"
      },
      "outputs": [],
      "source": [
        "def preprocess_text_first(text):\n",
        "  while '<' in text and '>' in text and text.index('<')<text.index('>'):\n",
        "    toRemove=text[int(text.index('<')):int(text.index('>'))]+'>'\n",
        "    text=text.replace(toRemove,' ')\n",
        "  list_to_replace=['mso','gte','xml','false','#',',','!','-','\\'','\\\"','[',']','/','\\\\n','\\\\','span','a-size-base','a-color-secondary','input type','header name','value','=','<a href= javascript:void(0) class= ','{','}','class=','header','<a href= javascript:void(0)','<','>','href',')','(',';','quot','&',':','javascript']\n",
        "  for char in list_to_replace:\n",
        "    text=text.replace(char,' ')\n",
        "  for i in range(15):\n",
        "    text=text.replace('  ',' ')\n",
        "  # while 'if' in text and 'endif' in text and text.index('if')<text.index('endif'):\n",
        "  #   # print(int(text.index('if')),int(text.index('endif')))\n",
        "  #   toRemove=text[int(text.index('if')):int(text.index('endif'))]+'endif'\n",
        "  #   text=text.replace(toRemove,' ')\n",
        "  new_text=''\n",
        "  for word in text.split(' '):\n",
        "    if len(word)>1 and len(word)<35:\n",
        "      new_text+=word+' '\n",
        "  # new_text=lemmatization(new_text)\n",
        "  # print(new_text)\n",
        "  return new_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBVGW8k1aL45",
        "outputId": "0b1f7c2f-6622-4099-9aca-ab90a669d673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "50000\n",
            "100000\n",
            "150000\n",
            "200000\n",
            "250000\n",
            "300000\n",
            "350000\n",
            "400000\n",
            "450000\n",
            "500000\n",
            "550000\n",
            "600000\n",
            "650000\n",
            "700000\n",
            "750000\n",
            "800000\n",
            "850000\n",
            "900000\n",
            "950000\n",
            "1000000\n",
            "1050000\n",
            "1100000\n"
          ]
        }
      ],
      "source": [
        "user_test_perspective={}\n",
        "i=0\n",
        "for (user_id , item_id) in review_features.keys():\n",
        "  review_feature=review_features[(user_id,item_id)]\n",
        "  if i%50000==0:\n",
        "    print(i)\n",
        "  i+=1\n",
        "  for features in review_feature:\n",
        "    sentence=features[3]\n",
        "    sentence=preprocess_text_first(sentence)\n",
        "    sentence=preprocess(sentence).lower()\n",
        "    final_vect=[]\n",
        "    if (user_id , item_id) not in user_test_perspective.keys():\n",
        "      new_final_vect=[]\n",
        "    else:\n",
        "      new_final_vect=user_test_perspective[(user_id , item_id)]\n",
        "    final_vect+=sentence.split(' ')\n",
        "    for word in sentence.split(' '):\n",
        "      # tokens=list(set(df_words[df_words['word']==word]['tokenized'].values))\n",
        "      # for token in tokens:\n",
        "        final_vect+=word.split(' ')\n",
        "    final_vect=list(set(final_vect))\n",
        "    for word in final_vect:\n",
        "      if len(word)>1:\n",
        "        new_final_vect.append(word)\n",
        "    user_test_perspective[(user_id , item_id)]=list(set(new_final_vect))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_test_perspective"
      ],
      "metadata": {
        "id": "QZF5dCSfTF-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGywTyrWPsrh"
      },
      "outputs": [],
      "source": [
        "def evaluate_user_perspective(user_perspective_data, u_i_expl_dict):\n",
        "    pres = []\n",
        "    recs = []\n",
        "    f1s = []\n",
        "    # print(user_perspective_data)\n",
        "    # rint(u_i_expl_dict)\n",
        "    for u_i, gt_features in user_perspective_data.items():\n",
        "        # print(u_i)\n",
        "        # print(u_i_expl_dict)\n",
        "        if u_i in u_i_expl_dict:\n",
        "            TP = 0\n",
        "            pre_features = u_i_expl_dict[u_i]\n",
        "            # print('f: ', gt_features, pre_features)\n",
        "            for feature in pre_features:\n",
        "                if feature in gt_features:\n",
        "                    TP += 1\n",
        "            pre = TP / len(pre_features)\n",
        "            rec = TP / len(gt_features)\n",
        "            if (pre + rec) != 0:\n",
        "                f1 = (2 * pre * rec) / (pre + rec)\n",
        "            else:\n",
        "                f1 = 0\n",
        "            pres.append(pre)\n",
        "            recs.append(rec)\n",
        "            f1s.append(f1)\n",
        "    ave_pre = np.mean(pres)\n",
        "    ave_rec = np.mean(recs)\n",
        "    ave_f1 = np.mean(f1s)\n",
        "    return ave_pre, ave_rec, ave_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzO5JyluPyG-"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_perspective(\n",
        "        rec_dict,\n",
        "        u_i_exp_dict,\n",
        "        base_model,\n",
        "        user_feature_matrix,\n",
        "        item_feature_matrix,\n",
        "        rec_k,\n",
        "        device):\n",
        "    \"\"\"\n",
        "    compute PN, PS and F_NS score for the explanations\n",
        "    :param rec_dict: {u1: [i1, i2, i3, ...] , u2: [i1, i2, i3, ...]}\n",
        "    :param u_i_exp_dict: {(u, i): [f1, f2, ...], ...}\n",
        "    :param base_model: the trained base recommendation model\n",
        "    :param user_feature_matrix: |u| x |p| matrix, the attention on each feature p for each user u\n",
        "    :param item_feature_matrix: |i| x |p| matrix, the quality on each feature p for each item i\n",
        "    :param rec_k: the length of the recommendation list, only generated explanations for the items on the list\n",
        "    :param device: the device of the model\n",
        "    :return: the mean of the PN, PS and FNS scores\n",
        "    \"\"\"\n",
        "    pn_count = 0\n",
        "    ps_count = 0\n",
        "    for u_i, fs in u_i_exp_dict.items():\n",
        "        user = u_i[0]\n",
        "        target_item = u_i[1]\n",
        "        features = set(fs)\n",
        "        items = rec_dict[user]\n",
        "        target_index = items.index(target_item)\n",
        "        # compute PN\n",
        "        cf_items_features = []\n",
        "        for item in items:\n",
        "            item_ori_feature = np.array(item_feature_matrix[item])\n",
        "            item_cf_feature = np.array([0 if s in features else item_ori_feature[s]\n",
        "                                        for s in range(len(item_ori_feature))], dtype='float32')\n",
        "            cf_items_features.append(item_cf_feature)\n",
        "        cf_ranking_scores = base_model(torch.from_numpy(np.array([user_feature_matrix[user]\n",
        "                                                                      for i in range(len(cf_items_features))])\n",
        "                                                            ).to(device),\n",
        "                                           torch.from_numpy(np.array(cf_items_features)).to(device)).squeeze()\n",
        "        cf_score_list = cf_ranking_scores.to('cpu').detach().numpy()\n",
        "        sorted_index = np.argsort(cf_score_list)[::-1]\n",
        "        cf_rank = np.argwhere(sorted_index == target_index)[0, 0]  # the updated ranking of the current item\n",
        "        if cf_rank > rec_k - 1:\n",
        "            pn_count += 1\n",
        "        # compute NS\n",
        "        cf_items_features = []\n",
        "        for item in items:\n",
        "            item_ori_feature = np.array(item_feature_matrix[item])\n",
        "            item_cf_feature = np.array([item_ori_feature[s] if s in features else 0\n",
        "                                        for s in range(len(item_ori_feature))], dtype='float32')\n",
        "            cf_items_features.append(item_cf_feature)\n",
        "        cf_ranking_scores = base_model(torch.from_numpy(np.array([user_feature_matrix[user]\n",
        "                                                                      for i in range(len(cf_items_features))])\n",
        "                                                            ).to(device),\n",
        "                                           torch.from_numpy(np.array(cf_items_features)).to(device)).squeeze()\n",
        "        cf_score_list = cf_ranking_scores.to('cpu').detach().numpy()\n",
        "        sorted_index = np.argsort(cf_score_list)[::-1]\n",
        "        cf_rank = np.argwhere(sorted_index == target_index)[0, 0]  # the updated ranking of the current item\n",
        "        if cf_rank < rec_k:\n",
        "            ps_count += 1\n",
        "    if len(u_i_exp_dict) != 0:\n",
        "        pn = pn_count / len(u_i_exp_dict)\n",
        "        ps = ps_count / len(u_i_exp_dict)\n",
        "        if (pn + ps) != 0:\n",
        "            fns = (2 * pn * ps) / (pn + ps)\n",
        "        else:\n",
        "            fns = 0\n",
        "    else:\n",
        "        pn = 0\n",
        "        ps = 0\n",
        "        fns = 0\n",
        "    return pn, ps, fns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j03L6H5PA4bK"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp4cMNYcPIFm"
      },
      "outputs": [],
      "source": [
        "class ExpOptimizationModel(torch.nn.Module):\n",
        "    def __init__(self, base_model, rec_dataset, device):\n",
        "        super(ExpOptimizationModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.rec_dataset = rec_dataset\n",
        "        self.device = device\n",
        "        self.u_i_exp_dict = {}  # {(user, item): [f1, f2, f3 ...], ...}\n",
        "        self.user_feature_matrix = torch.from_numpy(self.rec_dataset.user_feature_matrix).to(self.device)\n",
        "        self.item_feature_matrix = torch.from_numpy(self.rec_dataset.item_feature_matrix).to(self.device)\n",
        "        self.rec_dict, self.user_perspective_test_data = self.generate_rec_dict()\n",
        "\n",
        "    def generate_rec_dict(self):\n",
        "        rec_dict = {}\n",
        "        correct_rec_dict = {}  # used for user-side evaluation\n",
        "        for row in self.rec_dataset.test_data:\n",
        "            user = row[0]\n",
        "            items = row[1]\n",
        "            labels = row[2]\n",
        "            correct_rec_dict[user] = []\n",
        "            user_features = self.user_feature_matrix[user].repeat(len(items), 1)\n",
        "            scores = self.base_model(user_features,\n",
        "                        self.item_feature_matrix[items]).squeeze()\n",
        "            scores = np.array(scores.to('cpu'))\n",
        "            sort_index = sorted(range(len(scores)), key=lambda k: scores[k], reverse=True)\n",
        "            sorted_items = [items[i] for i in sort_index]\n",
        "            rec_dict[user] = sorted_items\n",
        "            for i in range(rec_k):  # find the correct items and add to the user side test data\n",
        "                if labels[sort_index[i]] == 1:\n",
        "                    correct_rec_dict[user].append(items[sort_index[i]])\n",
        "        user_item_feature_dict = {}  # {(u, i): f, (u, i): f]\n",
        "\n",
        "        for row in self.rec_dataset.sentiment_data:\n",
        "            user = row[0]\n",
        "            item = row[1]\n",
        "            user_item_feature_dict[(user, item)] = []\n",
        "            for fos in row[2:]:\n",
        "                feature = fos[0]\n",
        "                user_item_feature_dict[(user, item)].append(feature)\n",
        "        user_perspective_test_data = {}  # {(u, i):f, (u, i): f]}\n",
        "        # for user, tiems in correct_rec_dict.items():\n",
        "        #     for item in tiems:\n",
        "        #       if (user, item) in user_item_feature_dict.keys():\n",
        "        #           feature = user_item_feature_dict[(user, item)]\n",
        "        #           user_perspective_test_data[(user, item)] = feature\n",
        "        # # print(user_perspective_test_data)\n",
        "        return rec_dict, user_perspective_test_data\n",
        "\n",
        "    def generate_explanation(self):\n",
        "        # u_i_exps_dict = {}  # {(user, item): [f1, f2, f3 ...], ...}\n",
        "        exp_nums = []\n",
        "        exp_complexities = []\n",
        "        self.no_exp_count = 0\n",
        "        self.all_count=0\n",
        "        if test_num == -1:\n",
        "            test_num1 = len(list(self.rec_dict.items()))\n",
        "        else:\n",
        "            test_num1 = test_num\n",
        "        # print(test_num1)\n",
        "        count=0\n",
        "        for user, items in tqdm.tqdm(list(self.rec_dict.items())[:10]):\n",
        "            count+=1\n",
        "            items = self.rec_dict[user]\n",
        "            margin_item = items[rec_k]\n",
        "            margin_score = self.base_model(self.user_feature_matrix[user].unsqueeze(0), \n",
        "                            self.item_feature_matrix[margin_item].unsqueeze(0)).squeeze()\n",
        "            if user_mask:\n",
        "                # mask_vec = self.generate_mask(user)\n",
        "                mask_vec = torch.where(self.user_feature_matrix[user]>0, 1., 0.).unsqueeze(0)  # only choose exps from the user cared aspects\n",
        "            else:\n",
        "                mask_vec = torch.ones(self.rec_dataset.feature_num, device=self.device).unsqueeze(0)\n",
        "            for item in items[: rec_k]:\n",
        "                explanation_features, exp_num, exp_complexity = self.explain(\n",
        "                    self.user_feature_matrix[user], \n",
        "                    self.item_feature_matrix[item], \n",
        "                    margin_score,\n",
        "                    mask_vec)\n",
        "                self.all_count+=1\n",
        "                if explanation_features is None:\n",
        "                    # print('no explanation for user %d and item %d' % (user, item))\n",
        "                    self.no_exp_count += 1\n",
        "                else:\n",
        "                    self.u_i_exp_dict[(user, item)] = explanation_features\n",
        "                    exp_nums.append(exp_num)\n",
        "                    exp_complexities.append(exp_complexity)\n",
        "            # if count%500==0:\n",
        "            #   json1 = json.dumps(self.u_i_exp_dict)\n",
        "            #   f = open(\"drive/MyDrive/ranjbar/dict{}.json\".format(count),\"w\")\n",
        "            #   f.write(json1)\n",
        "            #   f.close()\n",
        "\n",
        "        print('all_count',self.all_count,'ave num: ', np.mean(exp_nums), 'ave complexity: ', np.mean(exp_complexities) , 'no_exp_count: ', self.no_exp_count)\n",
        "        # print()\n",
        "        return True\n",
        "    \n",
        "    def explain(self, user_feature, item_feature, margin_score, mask_vec):\n",
        "        exp_generator = EXPGenerator(\n",
        "            self.rec_dataset, \n",
        "            self.base_model, \n",
        "            user_feature, \n",
        "            item_feature, \n",
        "            margin_score, \n",
        "            mask_vec,\n",
        "            self.device).to(self.device)\n",
        "\n",
        "        # optimization\n",
        "        optimizer = torch.optim.SGD(exp_generator.parameters(), lr=lr, weight_decay=0)\n",
        "        exp_generator.train()\n",
        "        lowest_loss = None\n",
        "        lowest_bpr = None\n",
        "        lowest_l2 = 0\n",
        "        optimize_delta = None\n",
        "        score = exp_generator()\n",
        "        bpr, l2, l1, loss = exp_generator.loss(score)\n",
        "        # print('init: ', 0, '  train loss: ', loss, '  bpr: ', bpr, '  l2: ', l2, '  l1: ', l1)\n",
        "        lowest_loss = loss\n",
        "        optimize_delta = exp_generator.delta.detach().to('cpu').numpy()\n",
        "        lowest_l2 = l2\n",
        "        for epoch in range(step):\n",
        "            exp_generator.zero_grad()\n",
        "            score = exp_generator()\n",
        "            bpr, l2, l1, loss = exp_generator.loss(score)\n",
        "            # if epoch % 100 == 0:\n",
        "            #     print(\n",
        "            #         'epoch', epoch,\n",
        "            #         'bpr: ', bpr,\n",
        "            #         'l2: ', l2,\n",
        "            #         'l1', l1,\n",
        "            #         'loss', loss)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if loss < lowest_loss:\n",
        "                lowest_loss = loss\n",
        "                lowest_l2 = l2\n",
        "                lowest_bpr = bpr\n",
        "                optimize_delta = exp_generator.delta.detach().to('cpu').numpy()\n",
        "        if lowest_bpr >= lam * alp:\n",
        "            explanation_features = None \n",
        "            exp_num = None\n",
        "            exp_complexity = None\n",
        "        else:\n",
        "            # optimize_delta = exp_generator.delta.detach().to('cpu').numpy()\n",
        "            explanation_features = np.argwhere(optimize_delta < - mask_thresh).squeeze(axis=1)\n",
        "            if len(explanation_features) == 0:\n",
        "                explanation_features = np.array([np.argmin(optimize_delta)])\n",
        "            exp_num = len(explanation_features)\n",
        "            exp_complexity = lowest_l2.to('cpu').detach().numpy() + gam * exp_num\n",
        "        return explanation_features, exp_num, exp_complexity\n",
        "    \n",
        "    def user_side_evaluation(self):\n",
        "        ave_pre, ave_rec, ave_f1 = evaluate_user_perspective(self.user_perspective_test_data, self.u_i_exp_dict)\n",
        "        print('user\\'s perspective:')\n",
        "        print('ave pre: ', ave_pre, '  ave rec: ', ave_rec, '  ave f1: ', ave_f1)\n",
        "    \n",
        "    def model_side_evaluation(self):\n",
        "        ave_pn, ave_ps, ave_fns = evaluate_model_perspective(\n",
        "            self.rec_dict,\n",
        "            self.u_i_exp_dict,\n",
        "            self.base_model,\n",
        "            self.rec_dataset.user_feature_matrix,\n",
        "            self.rec_dataset.item_feature_matrix,\n",
        "            rec_k,\n",
        "            self.device)\n",
        "        print('model\\'s perspective:')\n",
        "        print('ave PN: ', ave_pn, '  ave PS: ', ave_ps, '  ave F_{NS}: ', ave_fns)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ-IFon4P5O4"
      },
      "outputs": [],
      "source": [
        "class EXPGenerator(torch.nn.Module):\n",
        "    def __init__(self, rec_dataset, base_model, user_feature, item_feature, margin_score, mask_vec, device):\n",
        "        super(EXPGenerator, self).__init__()\n",
        "        self.rec_dataset = rec_dataset\n",
        "        self.base_model = base_model\n",
        "        self.user_feature = user_feature\n",
        "        self.item_feature = item_feature\n",
        "        self.margin_score = margin_score\n",
        "        self.mask_vec = mask_vec\n",
        "        self.device = device\n",
        "        self.feature_range = [0, 5]  # hard coded, should be improved later\n",
        "        self.delta_range = self.feature_range[1] - self.feature_range[0]  # the maximum feature value.\n",
        "        self.delta = torch.nn.Parameter(\n",
        "            torch.FloatTensor(len(self.user_feature)).uniform_(-self.delta_range, 0))\n",
        "\n",
        "    def get_masked_item_feature(self):\n",
        "        item_feature_star = torch.clamp(\n",
        "            (self.item_feature + torch.clamp((self.delta * self.mask_vec), -self.delta_range, 0)),\n",
        "            self.feature_range[0], self.feature_range[1])\n",
        "        # print(self.item_feature)\n",
        "        # print(self.delta)\n",
        "        return item_feature_star\n",
        "    \n",
        "    def forward(self):\n",
        "        item_feature_star = self.get_masked_item_feature()\n",
        "        # print(item_feature_star)\n",
        "        score = self.base_model(self.user_feature.unsqueeze(0), item_feature_star)\n",
        "        return score\n",
        "    \n",
        "    def loss(self, score):\n",
        "        bpr = torch.nn.functional.relu(alp + score - self.margin_score) * lam\n",
        "        # print(score)\n",
        "        # print(self.margin_score)\n",
        "        l2 = torch.linalg.norm(self.delta)\n",
        "        l1 = torch.linalg.norm(self.delta, ord=1) * gam\n",
        "        loss = l2 + bpr + l1\n",
        "        return bpr, l2, l1, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol5Ikjv8PAkK",
        "outputId": "6e5d003d-bf43-4c0f-8599-8efde70f0549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [22:44<00:00,  6.82s/it]\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_count 1000 ave num:  6.567421790722761 ave complexity:  10.501755286880167 no_exp_count:  73\n",
            "user's perspective:\n",
            "ave pre:  nan   ave rec:  nan   ave f1:  nan\n",
            "model's perspective:\n",
            "ave PN:  0.9719525350593312   ave PS:  0.9320388349514563   ave F_{NS}:  0.9515773261090789\n"
          ]
        }
      ],
      "source": [
        "def generate_explanation():\n",
        "    if gpu:\n",
        "        device = torch.device('cuda:%s' %cuda)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    print(device)\n",
        "    # import dataset\n",
        "    # with open(os.path.join(data_obj_path, dataset + \"_dataset_obj_main.pickle\"), 'rb') as inp:\n",
        "    #     rec_dataset = pickle.load(inp)\n",
        "    \n",
        "    base_model = BaseRecModel(rec_dataset.feature_num).to(device)\n",
        "    base_model.load_state_dict(torch.load(os.path.join(base_model_path,\"model_main.model\"),map_location=torch.device(device)))\n",
        "    base_model.eval()\n",
        "    #  fix the rec model\n",
        "    for param in base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    # Create optimization model\n",
        "    opt_model = ExpOptimizationModel(\n",
        "        base_model=base_model,\n",
        "        rec_dataset=rec_dataset,\n",
        "        device = device,\n",
        "        \n",
        "    )\n",
        "\n",
        "    opt_model.generate_explanation()\n",
        "    opt_model.user_side_evaluation()\n",
        "    opt_model.model_side_evaluation()\n",
        "    # print(opt_model.u_i_exp_dict)\n",
        "    # Path(save_path).mkdir(parents=True, exist_ok=True)\n",
        "    # with open(os.path.join(save_path, dataset + \"_explanation_obj_main.pickle\"), 'wb') as outp:\n",
        "    #     pickle.dump(opt_model, outp, pickle.HIGHEST_PROTOCOL)\n",
        "    return opt_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    opt_model=generate_explanation()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cuda:0\n",
        "\n",
        "100%|██████████| 1000/1000 [56:18<00:00,  3.38s/it]\n",
        "\n",
        "all_count 5000\n",
        "user's perspective:\n",
        "ave pre:  0.055013481725532325   ave rec:  0.24509021707792805   ave f1:  0.07864064144286358\n",
        "model's perspective:\n",
        "ave PN:  0.9956788719581533   ave PS:  0.9563338639981805   ave F_{NS}:  0.9756098465767294\n"
      ],
      "metadata": {
        "id": "n_pQxcgPnyMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVckcA6GyHJB",
        "outputId": "3e78ca27-b410-41bb-8867-dbd3689530f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(save_path2, dataset + \"_dataset_obj_onlytips.pickle\"), 'rb') as inp:\n",
        "  rec_dataset2 = pickle.load(inp)"
      ],
      "metadata": {
        "id": "rmvlTC9oy3kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec_dataset2.test_data"
      ],
      "metadata": {
        "id": "EnhNRta_y_xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_list=[(90027, 1379),(50, 4398),(7063, 13200),(1120, 1321),(1097, 13448),(6708, 12469),(6708, 2261),(8694, 6988),(1717, 3766)]"
      ],
      "metadata": {
        "id": "IdpKTITe6jmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_item_ids=[]\n",
        "for (user,item) in chosen_list:\n",
        "  user_id=rec_dataset2.inv_user_name_dict[user]\n",
        "  item_id=rec_dataset2.inv_item_name_dict[item]\n",
        "  user_item_ids.append((user_id,item_id))"
      ],
      "metadata": {
        "id": "3tBpyJsr7RPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inv_user_name_dict = {v: k for k, v in rec_dataset.user_name_dict.items()}\n",
        "inv_item_name_dict = {v: k for k, v in rec_dataset.item_name_dict.items()}\n",
        "inv_feature_name_dict = {v: k for k, v in rec_dataset.feature_name_dict.items()}"
      ],
      "metadata": {
        "id": "uJCL2oHP5oJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mip-N2s9YMMv",
        "outputId": "4e532948-9d3d-46d0-ccbd-7891c566ff16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pCpZ6KF0vCOF3fyj9W8iMg 7JhQXpiMml41sxN34CUcKQ\n",
            "store\n",
            "beans\n",
            "sugar\n",
            "neighborhood\n",
            "bathroom\n",
            "stores\n",
            "----------\n",
            "nlReKgQoRz6uPfVaEG93mw tU692E8N0xBQ7Ogc78gN2g\n",
            "staff\n",
            "taste\n",
            "inside\n",
            "----------\n",
            "0du93EkEwKuxRG_x6hqVUg KnsY8rh5tigp5t6WpilGdA\n",
            "coffee\n",
            "favorite\n",
            "spot\n",
            "cheese\n",
            "tasting\n",
            "price\n",
            "eating\n",
            "hour\n",
            "ingredients\n",
            "tea\n",
            "chocolate\n",
            "neighborhood\n",
            "shop\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "for (user,item) in opt_model.u_i_exp_dict.keys():\n",
        "  user_id=inv_user_name_dict[user]\n",
        "  item_id=inv_item_name_dict [item]\n",
        "  if(user_id,item_id) in user_item_ids:\n",
        "    print(user_id,item_id)\n",
        "    feas= opt_model.u_i_exp_dict[(user,item)]\n",
        "    for fea in feas:\n",
        "      print(inv_feature_name_dict[fea])\n",
        "    print('----------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEsDdcYFWx6v",
        "outputId": "00b80724-6fec-487f-dd9e-fe1927a5e6b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================= filtering sentiment data =======================\n",
            "original review length:  52178\n",
            "original user length:  21615\n",
            "original item length:  9292\n",
            "original feature length:  325\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid review length:  4454\n",
            "valid user:  251\n",
            "valid item :  1918\n",
            "valid feature length:  88\n",
            "user dense is: 17.745019920318725\n",
            "======================= filtering sentiment data =======================\n",
            "original review length:  1072\n",
            "original user length:  201\n",
            "original item length:  634\n",
            "original feature length:  84\n",
            "valid review length:  180\n",
            "valid user:  14\n",
            "valid item :  134\n",
            "valid feature length:  15\n",
            "user dense is: 12.857142857142858\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ],
      "source": [
        "sentiment_data = []  # [userID, itemID, [fos triplet 1], [fos triplet 2], ...]\n",
        "with open(sentires_dir, 'r') as f:\n",
        "    line = f.readline().strip()\n",
        "    while line:\n",
        "        # print(count)\n",
        "        # print('line', line)\n",
        "        user = line.split('@')[0]\n",
        "        item = line.split('@')[1]\n",
        "        sentiment_data.append([user, item])\n",
        "        l = len(user) + len(item)\n",
        "        fosr_data = line[l+3:]\n",
        "        for seg in fosr_data.split('||'):\n",
        "            fos = seg.split(':')[0].strip('|')\n",
        "            if len(fos.split('|')) > 1:\n",
        "                feature = fos.split('|')[0]\n",
        "                opinion = fos.split('|')[1]\n",
        "                sentiment = fos.split('|')[2]\n",
        "                sentiment_data[-1].append([feature, opinion, sentiment])\n",
        "        line = f.readline().strip()\n",
        "sentiment_data = np.array(sentiment_data)\n",
        "sentiment_data = sentiment_data_filtering(\n",
        "    sentiment_data, \n",
        "    user_thresh, \n",
        "    feature_thresh)\n",
        "user_dict, item_dict = get_user_item_dict(sentiment_data)  # not sorted with time\n",
        "user_item_date_dict = {}   # {(user, item): date, (user, item): date ...}  # used to remove duplicate\n",
        "\n",
        "for i, line in enumerate(open(review_dir, \"r\")):\n",
        "    record = json.loads(line)\n",
        "    user = record['reviewerID']\n",
        "    item = record['asin']\n",
        "    date = record['unixReviewTime']\n",
        "    if user in user_dict and item in user_dict[user] and (user, item) not in user_item_date_dict:\n",
        "        user_item_date_dict[(user, item)] = date\n",
        "\n",
        "# remove the (user, item) not exist in the official dataset, possibly due to update?\n",
        "sentiment_data = [row for row in sentiment_data if (row[0], row[1]) in user_item_date_dict]\n",
        "sentiment_data = sentiment_data_filtering(sentiment_data, user_thresh, feature_thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBniImSFWTFa"
      },
      "outputs": [],
      "source": [
        "user_name_dict = {}\n",
        "item_name_dict = {}\n",
        "feature_name_dict = {}\n",
        "features = get_feature_list(sentiment_data)\n",
        "count = 0\n",
        "for feature in features:\n",
        "    if feature not in feature_name_dict:\n",
        "        feature_name_dict[feature] = count\n",
        "        count += 1\n",
        "count = 0\n",
        "for user in user_dict:\n",
        "    if user not in user_name_dict:\n",
        "        user_name_dict[user] = count\n",
        "        count += 1\n",
        "count = 0\n",
        "for item in item_dict:\n",
        "    if item not in item_name_dict:\n",
        "        item_name_dict[item] = count\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIlTTgrIxe_N",
        "outputId": "92bbbaa2-1a6c-4a46-944b-5387cf8ea7c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "251"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(user_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZqrZ_3sZTFW"
      },
      "outputs": [],
      "source": [
        "inv_user_name_dict = {v: k for k, v in user_name_dict.items()}\n",
        "inv_item_name_dict = {v: k for k, v in item_name_dict.items()}\n",
        "inv_features_dict = {v: k for k, v in feature_name_dict.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrII6Jdxa9w_",
        "outputId": "ace98f36-10c2-40d0-9a41-8d83c9a62fe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-06 15:23:24--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Cell_Phones_and_Accessories.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1232323281 (1.1G) [application/octet-stream]\n",
            "Saving to: ‘Cell_Phones_and_Accessories.json.gz’\n",
            "\n",
            "Cell_Phones_and_Acc 100%[===================>]   1.15G  21.4MB/s    in 56s     \n",
            "\n",
            "2022-03-06 15:24:21 (20.9 MB/s) - ‘Cell_Phones_and_Accessories.json.gz’ saved [1232323281/1232323281]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFiles/Cell_Phones_and_Accessories.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG_zjVEftc7S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    name=b'\"verified\": \\\"true\\\",'\n",
        "    l=l.replace(b'\"verified\": true,',bytes(name))\n",
        "    name1=b'\"verified\": \\\"false\\\",'\n",
        "    l=l.replace(b'\"verified\": false,',bytes(name))\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    rem_list={'verified','reviewTime','style','image'}\n",
        "    [d.pop(key) for key in rem_list if key in d.keys()]\n",
        "    if d['reviewerID'] in user_dict:\n",
        "      df[i] = d\n",
        "      i += 1\n",
        "      if i%1000==0:\n",
        "        print(i)\n",
        "  return pd.DataFrame.from_dict(df, orient='index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD1a56K_VFnv",
        "outputId": "893f0ea9-0170-4347-99d7-29a385486862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n"
          ]
        }
      ],
      "source": [
        "df_reviews = getDF('Cell_Phones_and_Accessories.json.gz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RgxE9mT2skM"
      },
      "outputs": [],
      "source": [
        "df_reviews['asin']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt3ofHE8YzIu",
        "outputId": "24d1aaa4-91d0-4b7d-94da-e917ab37df59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AL4YOB7KRVQ9W\n",
            "B0013URK04\n",
            "battery\n",
            "size\n",
            "----------------\n",
            "AL4YOB7KRVQ9W\n",
            "B00170KUM0\n",
            "case\n",
            "----------------\n",
            "AL4YOB7KRVQ9W\n",
            "B000FL9QGI\n",
            "screen\n",
            "size\n",
            "----------------\n",
            "AL4YOB7KRVQ9W\n",
            "B000C1CHVC\n",
            "screen\n",
            "----------------\n",
            "AL4YOB7KRVQ9W\n",
            "B000SZ9I0K\n",
            "case\n",
            "----------------\n",
            "A5JLAU2ARJ0BO\n",
            "B00170KUM0\n",
            "phone\n",
            "----------------\n",
            "A5JLAU2ARJ0BO\n",
            "B000SZ9I0K\n",
            "phone\n",
            "----------------\n",
            "A5JLAU2ARJ0BO\n",
            "B000GAO9T2\n",
            "phone\n",
            "----------------\n",
            "A5JLAU2ARJ0BO\n",
            "B000C1CHVC\n",
            "phone\n",
            "screen\n",
            "----------------\n",
            "A22S7D0LP8GRDH\n",
            "B0009W8DKI\n",
            "phone\n",
            "product\n",
            "----------------\n",
            "A22S7D0LP8GRDH\n",
            "B000T4LFCO\n",
            "size\n",
            "----------------\n",
            "A22S7D0LP8GRDH\n",
            "B000SZ9I0K\n",
            "phone\n",
            "fit\n",
            "----------------\n",
            "A22S7D0LP8GRDH\n",
            "B0000WZWSI\n",
            "fit\n",
            "----------------\n",
            "A22S7D0LP8GRDH\n",
            "B001BZH2QI\n",
            "sound\n",
            "battery\n",
            "----------------\n",
            "A1ODOGXEYECQQ8\n",
            "B0013URK04\n",
            "battery\n",
            "----------------\n",
            "A1ODOGXEYECQQ8\n",
            "B000HJC56G\n",
            "phone\n",
            "----------------\n",
            "A1ODOGXEYECQQ8\n",
            "B000WR81ZM\n",
            "battery\n",
            "----------------\n",
            "A1ODOGXEYECQQ8\n",
            "B000XQGJUG\n",
            "phone\n",
            "----------------\n",
            "A1ODOGXEYECQQ8\n",
            "B0009W8DL2\n",
            "battery\n",
            "----------------\n",
            "A1X1CEGHTHMBL1\n",
            "B000BYPLVI\n",
            "product\n",
            "----------------\n",
            "A1X1CEGHTHMBL1\n",
            "B0014KOB6Y\n",
            "device\n",
            "----------------\n",
            "A1X1CEGHTHMBL1\n",
            "B0018MK2C0\n",
            "device\n",
            "----------------\n",
            "A3NOBH42C7UI5M\n",
            "B001630QZE\n",
            "fit\n",
            "case\n",
            "----------------\n",
            "A3NOBH42C7UI5M\n",
            "B000FHBEFS\n",
            "case\n",
            "----------------\n",
            "A3NOBH42C7UI5M\n",
            "B00170KUM0\n",
            "case\n",
            "----------------\n",
            "A3NOBH42C7UI5M\n",
            "B001J5P7E4\n",
            "fit\n",
            "case\n",
            "----------------\n",
            "A3NOBH42C7UI5M\n",
            "B000FFEH04\n",
            "case\n",
            "----------------\n",
            "A18MBO1U4DPY20\n",
            "B00081GX8O\n",
            "phone\n",
            "----------------\n",
            "A18MBO1U4DPY20\n",
            "B000HJC56G\n",
            "phone\n",
            "----------------\n",
            "A18MBO1U4DPY20\n",
            "B000QGF8OQ\n",
            "battery\n",
            "----------------\n",
            "A18MBO1U4DPY20\n",
            "B000M92GLK\n",
            "phone\n",
            "----------------\n",
            "A18MBO1U4DPY20\n",
            "B001LKZNMI\n",
            "phone\n",
            "----------------\n",
            "A3AYF9CD4PXDJR\n",
            "B000BYPLVI\n",
            "product\n",
            "size\n",
            "----------------\n",
            "A3AYF9CD4PXDJR\n",
            "B000T4LFCO\n",
            "size\n",
            "----------------\n",
            "A3AYF9CD4PXDJR\n",
            "B000S0B9ZC\n",
            "size\n",
            "----------------\n",
            "A3AYF9CD4PXDJR\n",
            "B000BBCTJ8\n",
            "size\n",
            "----------------\n",
            "A3AYF9CD4PXDJR\n",
            "B0019LSK38\n",
            "size\n",
            "----------------\n",
            "A680RUE1FDO8B\n",
            "B00170KUM0\n",
            "phone\n",
            "case\n",
            "----------------\n",
            "A680RUE1FDO8B\n",
            "B000GAO9T2\n",
            "phone\n",
            "case\n",
            "----------------\n",
            "A680RUE1FDO8B\n",
            "B000HJC56G\n",
            "phone\n",
            "----------------\n",
            "A680RUE1FDO8B\n",
            "B000SZ9I0K\n",
            "phone\n",
            "case\n",
            "----------------\n",
            "A680RUE1FDO8B\n",
            "B001713AAI\n",
            "quality\n",
            "phone\n",
            "----------------\n",
            "A360VP3RBMJFDL\n",
            "B000ROO28K\n",
            "quality\n",
            "phone\n",
            "----------------\n",
            "A360VP3RBMJFDL\n",
            "B000SZ9I0K\n",
            "phone\n",
            "----------------\n",
            "A360VP3RBMJFDL\n",
            "B000TLVMMA\n",
            "quality\n",
            "----------------\n",
            "A360VP3RBMJFDL\n",
            "B000WPDI2K\n",
            "quality\n",
            "----------------\n",
            "A360VP3RBMJFDL\n",
            "B00081GX8O\n",
            "phone\n",
            "----------------\n",
            "A1F9Z42CFF9IAY\n",
            "B001713AAI\n",
            "quality\n",
            "phone\n",
            "size\n",
            "----------------\n",
            "A1F9Z42CFF9IAY\n",
            "B000HJC56G\n",
            "phone\n",
            "size\n",
            "----------------\n",
            "A1F9Z42CFF9IAY\n",
            "B000GAO9T2\n",
            "phone\n",
            "----------------\n",
            "A1F9Z42CFF9IAY\n",
            "B000RUPEOA\n",
            "quality\n",
            "case\n",
            "----------------\n",
            "A1F9Z42CFF9IAY\n",
            "B000M92GLK\n",
            "phone\n",
            "----------------\n",
            "AVPNQUVZWMDSX\n",
            "B00177YJFM\n",
            "battery\n",
            "----------------\n",
            "AVPNQUVZWMDSX\n",
            "B0009W8DL2\n",
            "battery\n",
            "----------------\n",
            "AVPNQUVZWMDSX\n",
            "B0009W8DKI\n",
            "product\n",
            "----------------\n",
            "AVPNQUVZWMDSX\n",
            "B0000WZWSI\n",
            "fit\n",
            "----------------\n",
            "AVPNQUVZWMDSX\n",
            "B000PYJ4NK\n",
            "fit\n",
            "----------------\n",
            "A3EXWV8FNSSFL6\n",
            "B000AZ1LIK\n",
            "phone\n",
            "----------------\n",
            "A3EXWV8FNSSFL6\n",
            "B000XQGJUG\n",
            "phone\n",
            "----------------\n",
            "A3EXWV8FNSSFL6\n",
            "B00170KUM0\n",
            "phone\n",
            "----------------\n",
            "A3EXWV8FNSSFL6\n",
            "B000CORUSO\n",
            "phone\n",
            "----------------\n",
            "A3EXWV8FNSSFL6\n",
            "B000M92GLK\n",
            "phone\n",
            "----------------\n",
            "A1N5FSCYN4796F\n",
            "B001J5P7E4\n",
            "fit\n",
            "case\n",
            "----------------\n",
            "A1N5FSCYN4796F\n",
            "B000UWDU5K\n",
            "fit\n",
            "case\n",
            "----------------\n",
            "A1N5FSCYN4796F\n",
            "B0006GFARG\n",
            "device\n",
            "----------------\n",
            "A1N5FSCYN4796F\n",
            "B001IATYMQ\n",
            "device\n",
            "----------------\n"
          ]
        }
      ],
      "source": [
        "for key in opt_model.u_i_exp_dict.keys():\n",
        "  user=key[0]\n",
        "  item=key[1]\n",
        "  features=opt_model.u_i_exp_dict[key]\n",
        "  user_id=inv_user_name_dict[user]\n",
        "  item_id=inv_item_name_dict[item]\n",
        "  print(user_id)\n",
        "  print(item_id)\n",
        "  # print(df_reviews[(df_reviews['reviewerID']==user_id)])\n",
        "  for feature in features:\n",
        "    print(inv_features_dict[feature])\n",
        "  print('----------------')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_explanation_check_stability():\n",
        "    if gpu:\n",
        "        device = torch.device('cuda:%s' %cuda)\n",
        "    else:\n",
        "        device = 'cpu'\n",
        "    print(device)\n",
        "    # import dataset\n",
        "    # with open(os.path.join(data_obj_path, dataset + \"_dataset_obj_main.pickle\"), 'rb') as inp:\n",
        "    #     rec_dataset = pickle.load(inp)\n",
        "    \n",
        "    base_model = BaseRecModel(rec_dataset.feature_num).to(device)\n",
        "    base_model.load_state_dict(torch.load(os.path.join(base_model_path,\"model_main.model\"),map_location=torch.device(device)))\n",
        "    base_model.eval()\n",
        "    #  fix the rec model\n",
        "    for param in base_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    \n",
        "    # Create optimization model\n",
        "    features_found=[]\n",
        "    for i in range(10):\n",
        "      opt_model = ExpOptimizationModel(\n",
        "        base_model=base_model,\n",
        "        rec_dataset=rec_dataset,\n",
        "        device = device,)\n",
        "      opt_model.generate_explanation()\n",
        "      features_found.append(opt_model.u_i_exp_dict)\n",
        "      \n",
        "    \n",
        "    return features_found\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    features_found=generate_explanation_check_stability()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw3vT7lYtMMH",
        "outputId": "65a6a4a6-811f-4cbb-ddf4-46ec1ad1712a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "cuda:0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:13<00:00,  7.32s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_count 50 ave num:  6.16 ave complexity:  10.002760995328426 no_exp_count:  0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:07<00:00,  6.72s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_count 50 ave num:  5.92 ave complexity:  9.768209390342236 no_exp_count:  0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:05<00:00,  6.50s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_count 50 ave num:  6.32 ave complexity:  10.180015035271644 no_exp_count:  0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:05<00:00,  6.50s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_count 50 ave num:  6.34 ave complexity:  10.246311855316161 no_exp_count:  0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:04<00:00,  6.50s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_count 50 ave num:  6.18 ave complexity:  10.078241356611251 no_exp_count:  0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:04<00:00,  6.50s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_count 50 ave num:  5.8 ave complexity:  9.741915336251258 no_exp_count:  0\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:04<00:00,  6.41s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_count 50 ave num:  6.081632653061225 ave complexity:  9.968037297226944 no_exp_count:  1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:04<00:00,  6.46s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "all_count 50 ave num:  6.1 ave complexity:  9.98515693962574 no_exp_count:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:03<00:00,  6.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_count 50 ave num:  6.04 ave complexity:  9.931369032263756 no_exp_count:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:04<00:00,  6.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_count 50 ave num:  5.938775510204081 ave complexity:  9.787047523929148 no_exp_count:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_features={}\n",
        "for iter_feas in features_found:\n",
        "  for u_i in iter_feas.keys():\n",
        "    # feas=list(iter_feas[u_i][0][0].columns)\n",
        "    feas=iter_feas[u_i]\n",
        "    # print(iter_feas[u_i])\n",
        "    if u_i in dict_features.keys():\n",
        "      dict_features[u_i].append(feas)\n",
        "    else:\n",
        "      dict_features[u_i]=[]\n",
        "      dict_features[u_i].append(feas)"
      ],
      "metadata": {
        "id": "nHcOCida1oXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_features"
      ],
      "metadata": {
        "id": "ZA9cH1yMebbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Stability..."
      ],
      "metadata": {
        "id": "xjN48gThNAuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stability=0\n",
        "# count_all=0\n",
        "for ui in dict_features.keys():\n",
        "  features=dict_features[ui]\n",
        "  stabs=0\n",
        "  count=0\n",
        "  if(len(features)>1):\n",
        "    # count_all+=1\n",
        "    for i in range(len(features)):\n",
        "      for j in range(len(features)):\n",
        "        # print(features[i])\n",
        "        # print(features[j])\n",
        "        if len(features[i])>0 and len(features[j])>0:\n",
        "          if i != j:\n",
        "            intersection = list(set(features[i]) & set(features[j]))\n",
        "            union = list(set(features[i]) | set(features[j]))\n",
        "            # print(features[i],features[j])\n",
        "            # print(intersection)\n",
        "            # print(union)\n",
        "            count+=1\n",
        "            stabs+=(len(intersection)/len(union))\n",
        "    # print(stabs)\n",
        "    # print(len(features)*(len(features)-1))\n",
        "    # print((stabs/(len(features)*(len(features)-1))))\n",
        "    stability+=(stabs/(9.0*10.0))\n",
        "\n",
        "stability=stability/len(dict_features)\n",
        "print(stability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzjzCNTA5Nxu",
        "outputId": "7462afa7-7688-4c3a-f6bb-49c0b471ed40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7434970227211033\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}